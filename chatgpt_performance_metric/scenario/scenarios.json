[
    {
        "question": "Please tell me about EDS mean?",
        "contexts": [
            [
                "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: ENN Framework Data Type References\\_ennBufferInfo\nContent:\n## Public Attributes\n| | Name |\n| -------------- | -------------- |\n| bool | **is_able_to_update** |\n| uint32_t | **n** |\n| uint32_t | **width** |\n| uint32_t | **height** |\n| uint32_t | **channel** |\n| uint32_t | **size** |\n| uint32_t | **buffer_type** |\n| const char * | **label** |\n## Public Attributes Documentation\n### variable is_able_to_update\n```cpp\nbool is_able_to_update;\n```\n### variable n\n```cpp\nuint32_t n;\n```\n### variable width\n```cpp\nuint32_t width;\n```\n### variable height\n```cpp\nuint32_t height;\n```\n### variable channel\n```cpp\nuint32_t channel;\n```\n### variable size\n```cpp\nuint32_t size;\n```\n### variable buffer_type\n```cpp\nuint32_t buffer_type;\n```\n### variable label\n```cpp\nconst char * label;\n```",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN Framework Data Type References\nContent:\n# _NumberOfBuffersInfo\n## Public Attributes\n| | Name |\n| -------------- | -------------- |\n| uint32_t | **n_in_buf** |\n| uint32_t | **n_out_buf** |\n## Public Attributes Documentation\n### variable n_in_buf\n```cpp\nuint32_t n_in_buf;\n```\n### variable n_out_buf\n```cpp\nuint32_t n_out_buf;\n```\n# _ennBuffer\n## Public Attributes\n| | Name |\n| -------------- | -------------- |\n| void * | **va** |\n| uint32_t | **size** |\n| uint32_t | **offset** |\n## Public Attributes Documentation\n### variable va\n```cpp\nvoid * va;\n```\n### variable size\n```cpp\nuint32_t size;\n```\n### variable offset\n```cpp\nuint32_t offset;\n```",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-2\nContent:\n| 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O |\n| 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O |\n| 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O |\n| 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O |\n| 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O |\n| 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O |\n| 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O |\n| 20 | DIV | DIV | | | O | O |\n| 21 | DYNAMIC_CONVOLUTION | - | O | | | |\n| 22 | DYNAMIC_FC | - | O | | | |\n| 23 | ELEMENTWISE_DIV | DIV | | O | O | O |\n| 24 | ELEMENTWISE_MUL | MUL | O | O | O | O |\n| 25 | ELEMENTWISE_SUB | SUB | O | O | O | O |\n| 26 | ELEMENTWISE_SUM | ADD | O | O | O | O |",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Section: API References\\API reference\nContent:\n## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references)\n- **_NumberOfBuffersInfo**\n- **_ennBuffer**\n- **_ennBufferInfo**\n## [ENN Framwork API Functions](api-reference/enn-framework-api-functions)\n- **Context initialize / deinitialize**\n- **OpenModel / CloseModel related**\n- **Memory Handling**\n- **Setters and Getters for model**\n- **Commit Buffer**\n- **Execute Models**\n- **Security, preference, get meta information..** ",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: Starting Android Project\\Starting the Android Project\\Using ERD Board\\Linking Up the ERD Board\nContent:\n#### Configuring the ERD Board\nEnabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.\nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\n\tThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**.\n#### Connecting the ERD Board\n1. Connect the ERD board to your computer using a USB cable.\n2. In the pop-up that appears, select **Allow** to enable the USB debugging.\n3. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-1\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnSetPreferencePresetId**(const uint32_t val)<br>Setting Preset ID for operation performance. |\n| EnnReturn | **EnnSetPreferencePerfConfigId**(const uint32_t val)<br>Setting PerfConfig ID for operation performance. |\n| EnnReturn | **EnnSetPreferencePerfMode**(const uint32_t val)<br>Setting Performance Mode. |\n| EnnReturn | **EnnSetPreferenceTimeOut**(const uint32_t val)<br>Setting Preset ID for time out. |\n| EnnReturn | **EnnSetPreferencePriority**(const uint32_t val)<br>Setting priority value for NPU. |\n| EnnReturn | **EnnSetPreferenceCoreAffinity**(const uint32_t val)<br>Setting affinity to set NPU core operation. |\n| EnnReturn | **EnnGetPreferencePresetId**(uint32_t * val_ptr)<br>Get current information for Preset ID. |",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnGetPreferencePresetId\n```cpp\nEnnReturn EnnGetPreferencePresetId(\n\tuint32_t * val_ptr\n)\n```\nGet current information for Preset ID.\n**Parameters**:\n- **val** [OUT] current value of Preset ID\n**Return**: EnnReturn result, 0 is success\n#### function EnnGetPreferencePerfConfigId\n```cpp\nEnnReturn EnnGetPreferencePerfConfigId(\n\tuint32_t * val_ptr\n)\n```\nGet current information for PerfConfig ID.\n**Parameters**:\n- **val** [OUT] current value of PerfConfig ID\n**Return**: EnnReturn result, 0 is success ",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-2\nContent:\n| EnnReturn | **EnnGetPreferencePerfConfigId**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. |\n| EnnReturn | **EnnGetPreferencePerfMode**(uint32_t * val_ptr)<br>Get current information for Performance Mode. |\n| EnnReturn | **EnnGetPreferenceTimeOut**(uint32_t * val_ptr)<br>Get current information for Time Out. |\n| EnnReturn | **EnnGetPreferencePriority**(uint32_t * val_ptr)<br>Get current information for NPU Priority. |\n| EnnReturn | **EnnGetPreferenceCoreAffinity**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. |\n| EnnReturn | **EnnGetMetaInfo**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. |\n| EnnReturn | **EnnSetExecMsgAlwaysOn**()<br>Set frequency of execution message print. |",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "URL: news/40ed73bb-4b81-47ed-b2ea-96181bdaa61b/detail\nCategory: News\nTitle: \uc774\uc81c\ub294 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc\u2019\ub85c! \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \ucd08\uc5f0\uacb0 \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 \uae30\uc220\nContent: \uc77c\uae4c? \uc0dd\uc0dd\ud55c \ud604\uc7a5 \uc18c\uc2dd\uc744 \ub2f4\uae30 \uc704\ud574 \uc0bc\uc131\uc804\uc790 \ubc18\ub3c4\uccb4 \ub274\uc2a4\ub8f8\uc774 \ubbf8\uad6d \uc0c8\ub108\uc81c\uc774\uc5d0 \uc704\uce58\ud55c \ubbf8\uc8fc\ucd1d\uad04(DSA) \uc0ac\uc625\uc73c\ub85c \ubc1c\uac78\uc74c\ud588\ub2e4.\n**\ub85c\ubd07\uc758 \ub300\uc911\ud654\ub97c \uc704\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4\uc758 \ub3c4\uc57d**\n<br>\n\uccad\uba85\ud55c \uac00\uc744 \ud587\uc0b4\uc774 \ub0b4\ub9ac\ucb10\ub294 DSA \ucea0\ud37c\uc2a4 \uc55e. \ud14c\ud06c \ub370\uc774 \ud589\uc0ac\ub97c \uc55e\ub450\uace0 \ud604\uc7a5\uc740 \uc774\ub978 \uc2dc\uac01\ubd80\ud130 \ub9ce\uc740 \ucc38\uad00\uac1d\ub4e4\ub85c \ubd81\uc801\uc600\ub2e4. \uc774\ub4e4\uc740 \ubcf8 \ud589\uc0ac\uac00 \uc2dc\uc791\ub418\uae30 \uc804, \uc785\uad6c\uc5d0 \ub193\uc778 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc \uc6d4(System LSI Humanoid Wall)\u2019\uc744 \ud765\ubbf8\ub86d\uac8c \uc0b4\ud3b4\ubcf4\uba70, \ud65c\ubc1c\ud55c \ub124\ud2b8\uc6cc\ud0b9\uc73c\ub85c \ud604\uc7a5\uc744 \ub354\uc6b1 \ud65c\uae30\ucc28\uac8c \ub9cc\ub4e4\uc5c8\ub2e4.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnSetPreferencePresetId\n```cpp\nEnnReturn EnnSetPreferencePresetId(\n\tconst uint32_t val\n)\n```\nSetting Preset ID for operation performance.\n**Parameters**:\n- **val** [IN] value to set preset ID\n**Return**: EnnReturn result, 0 is success\n#### function EnnSetPreferencePerfConfigId\n```cpp\nEnnReturn EnnSetPreferencePerfConfigId(\n\tconst uint32_t val\n)\n```\nSetting PerfConfig ID for operation performance.\n**Parameters**:\n- **val** [IN] value to set PerfConfig ID\n**Return**: EnnReturn result, 0 is success ",
                "URL: news/40ed73bb-4b81-47ed-b2ea-96181bdaa61b/detail\nCategory: News\nTitle: \uc774\uc81c\ub294 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc\u2019\ub85c! \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \ucd08\uc5f0\uacb0 \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 \uae30\uc220\nContent: \ud2b9\ud788, 2\uc5b5 \ud654\uc18c \uc774\ubbf8\uc9c0\uc13c\uc11c\ub97c \ud65c\uc6a9\ud574 \ucd08\uace0\ud574\uc0c1\ub3c4\uc758 \ud2b9\uc218 \uc90c\uc774 \uac00\ub2a5\ud55c \u2018\uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4(Zoom Anyplace)\u2019, \ubb3c\uccb4\uc5d0 \uad74\uc808\uc774\ub098 \ubc18\uc0ac\ub418\ub294 \ube5b\uc744 \ucd94\uc801\ud574 \uc0ac\ubb3c\uc744 \uc2e4\uac10\ub098\uac8c \ud45c\ud604\ud558\ub294 \u2018\ub808\uc774 \ud2b8\ub808\uc774\uc2f1(Ray Tracing)\u2019 \ub4f1\uc740 \ud604\uc7a5\uc5d0\uc11c \uc9c1\uc811 \uc2dc\uc5f0\uc744 \uc120\ubcf4\uc774\uba70 \ucc38\uad00\uac1d\ub4e4\uc758 \uc774\ubaa9\uc744 \ub04c\uc5c8\ub2e4.\n\n\ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc5d0\uc11c \uc131\ud669\ub9ac\uc5d0 \ub9c8\ubb34\ub9ac\ub41c \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019. \uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc\uc758 \uae38\ub85c \ub4e4\uc5b4\uc11c\uae30 \uc704\ud55c \uc0bc\uc131\uc804\uc790 \ubc18\ub3c4\uccb4\uc758 \ub178\ub825\uc744 \uc5ff\ubcfc \uc218 \uc788\uc5c8\ub358 \uc2dc\uac04\uc774\uc5c8\ub2e4. \uc774\ubc88 \ud14c\ud06c \ub370\uc774\ub97c \ud1a0\ub300\ub85c \uc77c\uc0c1\uc744 \uc0c8\ub86d\uac8c \uadf8\ub824\ub0bc \uc0bc\uc131\uc804\uc790 \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4\uc758 \ud601\uc2e0\uc801\uc778 \ubbf8\ub798\ub97c \uae30\ub300\ud574 \ubcf8\ub2e4.\n ",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Copying Data to ERD Board\nContent:\nThe following commands copy the necessary files to the ERD board:\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/model.nnc /data/local/tmp/\nadb push example/input.bin /data/local/tmp/\nadb push example/golden.bin /data/local/tmp/\n```\nThese commands perform the following actions:\n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-3\nContent:\n| 27 | EXP | EXP | | O | O | O |\n| 28 | UNSQUEEZE | EXPAND_DIMS | | | | O |\n| 29 | FLATTEN | - | O | O | O | O |\n| 30 | FLOOR | FLOOR | | O | O | O |\n| 31 | FLOOR_DIV | FLOOR_DIV | | O | | O |\n| 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O |\n| 33 | GATHER | GATHER | | | O | O |\n| 34 | GLOBAL_AVGPOOL | - | O | | O | O |\n| 35 | GLOBAL_MAXPOOL | - | O | O | O | O |\n| 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O |\n| 37 | HARD_SWISH | HARD_SWISH | O | O | O | O |\n| 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O |\n| 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O |\n| 40 | LOG | LOG | | | O | O |\n| 41 | LOGISTIC | LOGISTIC | O | O | O | O |\n| 42 | MAXIMUM | MAXIMUM | | O | O | O |",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-2\nContent:\n| DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| AVGPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\n| MAXPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\nFor `AVGPOOL` and `MAXPOOL`, when the kernel size is larger than the supported size, it is split into multiple pooling layers as the origin.",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-1\nContent:\n| Operator_Name | Kernel Size | Padding | Strides | Dilation | Boost Mode |\n| ------------------------------ | ----------- | -------- | -------- | --------- | ---------- |\n| CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | {1 ~ 27} |\n| DECONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DECONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |",
                "Question: How do I post an article in the Best Lab?\nAnswer: Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, up to six projects are featured, receiving special attention. There is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Setters and Getters for model\\Functions-2\nContent:\n| EnnReturn | **EnnSetBufferByIndex**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, EnnBufferPtr buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core. |\n| EnnReturn | **EnnSetBufferByLabel**(const EnnModelId model_id, const char * label, EnnBufferPtr buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnGetPreferencePriority\n```cpp\nEnnReturn EnnGetPreferencePriority(\n\tuint32_t * val_ptr\n)\n```\nGet current information for NPU Priority.\n**Parameters**:\n- **val** [OUT] current value of NPU Priority\n**Return**: EnnReturn result, 0 is success\n#### function EnnGetPreferenceCoreAffinity\n```cpp\nEnnReturn EnnGetPreferenceCoreAffinity(\n\tuint32_t * val_ptr\n)\n```\nGet current information for NPU Core affinity.\n**Parameters**:\n- **val** [OUT] current value of NPU Core affinity\n**Return**: EnnReturn result, 0 is success "
            ]
        ],
        "answer": "EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
        "ground_truth": "EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI."
    },
    {
        "question": "Please tell me who operates EDS Assistant?",
        "contexts": [
            [
                "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Copying Data to ERD Board\nContent:\nThe following commands copy the necessary files to the ERD board:\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/model.nnc /data/local/tmp/\nadb push example/input.bin /data/local/tmp/\nadb push example/golden.bin /data/local/tmp/\n```\nThese commands perform the following actions:\n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n```kotlin\npackage com.samsung.depthestimation.data\n\nobject ModelConstants {\n const val MODEL_NAME = \\\"MiDaS_v2.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 256\n const val INPUT_SIZE_H = 256\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 256\n const val OUTPUT_SIZE_H = 256\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n}\n```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Section: Using Adb to Execute\\Using Adb to Execute\nContent:\nThis section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.segmentation.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"deeplabv3.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_H = INPUT_SIZE_H\n const val OUTPUT_SIZE_W = INPUT_SIZE_W\n const val OUTPUT_SIZE_C = 21\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-6\nContent:\n| 75 | SELECT | SELECT | | | | O |\n| 76 | SHIFT_CLAMP | - | O | O | | |\n| 77 | SIN | SIN | | | O | O |\n| 78 | SLICE | SLICE | O | O | O | O |\n| 79 | SOFTMAX | SOFTMAX | | O | O | O |\n| 80 | SPACE_TO_BATCH_ND | SPACE_TO_BATCH_ND | | | | O |\n| 81 | SPACE_TO_DEPTH | SPACE_TO_DEPTH | O | O | O | O |\n| 82 | SPLIT | SPLIT | O | | O | O |\n| 83 | SPLIT_V | SPLIT_V | O | | | O |\n| 84 | SQRT | SQRT | | O | O | O |\n| 85 | SQUARED_DIFFERENCE | SQUARED_DIFFERENCE | | | O | O |\n| 86 | SQUEEZE | SQUEEZE | | | O | O |\n| 87 | STRIDED_SLICE | STRIDED_SLICE | O | O | O | O |\n| 88 | SUB | SUB | | | O | O |\n| 89 | TANH | TANH | O | O | O | O |\n| 90 | TILE | TILE | O | | O | O |\n| 91 | TRANSPOSE | TRANSPOSE | O | O | O | O |",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-2\nContent:\n| 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O |\n| 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O |\n| 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O |\n| 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O |\n| 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O |\n| 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O |\n| 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O |\n| 20 | DIV | DIV | | | O | O |\n| 21 | DYNAMIC_CONVOLUTION | - | O | | | |\n| 22 | DYNAMIC_FC | - | O | | | |\n| 23 | ELEMENTWISE_DIV | DIV | | O | O | O |\n| 24 | ELEMENTWISE_MUL | MUL | O | O | O | O |\n| 25 | ELEMENTWISE_SUB | SUB | O | O | O | O |\n| 26 | ELEMENTWISE_SUM | ADD | O | O | O | O |",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-2\nContent:\n| EnnReturn | **EnnGetPreferencePerfConfigId**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. |\n| EnnReturn | **EnnGetPreferencePerfMode**(uint32_t * val_ptr)<br>Get current information for Performance Mode. |\n| EnnReturn | **EnnGetPreferenceTimeOut**(uint32_t * val_ptr)<br>Get current information for Time Out. |\n| EnnReturn | **EnnGetPreferencePriority**(uint32_t * val_ptr)<br>Get current information for NPU Priority. |\n| EnnReturn | **EnnGetPreferenceCoreAffinity**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. |\n| EnnReturn | **EnnGetMetaInfo**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. |\n| EnnReturn | **EnnSetExecMsgAlwaysOn**()<br>Set frequency of execution message print. |",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38 \ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc2dc\uc2a4\ud15cLSI \uc0ac\uc5c5\ubd80 \ubc15\uc6a9\uc778 \uc0ac\uc7a5\uc774 \ubc1c\ud45c\ub97c \ud558\uace0 \uc788\ub294 \ubaa8\uc2b5**\n_* *_\n_* *_\n\n**\u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38\ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \ubc1c\ud45c\ub97c \ub4e3\ub294 \uccad\uc911\ub4e4\uc758 \ubaa8\uc2b5**\n_* *_\n_* *_\n\n**\u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38 \ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019 \ud604\uc7a5",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Starting Android Project\\Starting the Android Project\\Using ERD Board\\Linking Up the ERD Board\nContent:\n#### Configuring the ERD Board\nEnabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.\nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\n\tThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**.\n#### Connecting the ERD Board\n1. Connect the ERD board to your computer using a USB cable.\n2. In the pop-up that appears, select **Allow** to enable the USB debugging.\n3. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n>adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n>```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
                "URL: news/40ed73bb-4b81-47ed-b2ea-96181bdaa61b/detail\nCategory: News\nTitle: \uc774\uc81c\ub294 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc\u2019\ub85c! \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \ucd08\uc5f0\uacb0 \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 \uae30\uc220\nContent: \uc77c\uae4c? \uc0dd\uc0dd\ud55c \ud604\uc7a5 \uc18c\uc2dd\uc744 \ub2f4\uae30 \uc704\ud574 \uc0bc\uc131\uc804\uc790 \ubc18\ub3c4\uccb4 \ub274\uc2a4\ub8f8\uc774 \ubbf8\uad6d \uc0c8\ub108\uc81c\uc774\uc5d0 \uc704\uce58\ud55c \ubbf8\uc8fc\ucd1d\uad04(DSA) \uc0ac\uc625\uc73c\ub85c \ubc1c\uac78\uc74c\ud588\ub2e4.\n**\ub85c\ubd07\uc758 \ub300\uc911\ud654\ub97c \uc704\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4\uc758 \ub3c4\uc57d**\n<br>\n\uccad\uba85\ud55c \uac00\uc744 \ud587\uc0b4\uc774 \ub0b4\ub9ac\ucb10\ub294 DSA \ucea0\ud37c\uc2a4 \uc55e. \ud14c\ud06c \ub370\uc774 \ud589\uc0ac\ub97c \uc55e\ub450\uace0 \ud604\uc7a5\uc740 \uc774\ub978 \uc2dc\uac01\ubd80\ud130 \ub9ce\uc740 \ucc38\uad00\uac1d\ub4e4\ub85c \ubd81\uc801\uc600\ub2e4. \uc774\ub4e4\uc740 \ubcf8 \ud589\uc0ac\uac00 \uc2dc\uc791\ub418\uae30 \uc804, \uc785\uad6c\uc5d0 \ub193\uc778 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc \uc6d4(System LSI Humanoid Wall)\u2019\uc744 \ud765\ubbf8\ub86d\uac8c \uc0b4\ud3b4\ubcf4\uba70, \ud65c\ubc1c\ud55c \ub124\ud2b8\uc6cc\ud0b9\uc73c\ub85c \ud604\uc7a5\uc744 \ub354\uc6b1 \ud65c\uae30\ucc28\uac8c \ub9cc\ub4e4\uc5c8\ub2e4."
            ]
        ],
        "answer": "EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
        "ground_truth": "EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team."
    },
    {
        "question": "Please tell us about the purpose of EDS Assistant.",
        "contexts": [
            [
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
                "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Copying Data to ERD Board\nContent:\nThe following commands copy the necessary files to the ERD board:\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/model.nnc /data/local/tmp/\nadb push example/input.bin /data/local/tmp/\nadb push example/golden.bin /data/local/tmp/\n```\nThese commands perform the following actions:\n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Section: Using Adb to Execute\\Using Adb to Execute\nContent:\nThis section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-2\nContent:\n| EnnReturn | **EnnGetPreferencePerfConfigId**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. |\n| EnnReturn | **EnnGetPreferencePerfMode**(uint32_t * val_ptr)<br>Get current information for Performance Mode. |\n| EnnReturn | **EnnGetPreferenceTimeOut**(uint32_t * val_ptr)<br>Get current information for Time Out. |\n| EnnReturn | **EnnGetPreferencePriority**(uint32_t * val_ptr)<br>Get current information for NPU Priority. |\n| EnnReturn | **EnnGetPreferenceCoreAffinity**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. |\n| EnnReturn | **EnnGetMetaInfo**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. |\n| EnnReturn | **EnnSetExecMsgAlwaysOn**()<br>Set frequency of execution message print. |",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: How do I post an article in the Best Lab?\nAnswer: Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, up to six projects are featured, receiving special attention. There is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n```kotlin\npackage com.samsung.depthestimation.data\n\nobject ModelConstants {\n const val MODEL_NAME = \\\"MiDaS_v2.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 256\n const val INPUT_SIZE_H = 256\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 256\n const val OUTPUT_SIZE_H = 256\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n}\n```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: Starting Android Project\\Starting the Android Project\\Using ERD Board\\Linking Up the ERD Board\nContent:\n#### Configuring the ERD Board\nEnabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.\nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\n\tThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**.\n#### Connecting the ERD Board\n1. Connect the ERD board to your computer using a USB cable.\n2. In the pop-up that appears, select **Allow** to enable the USB debugging.\n3. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n>adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n>```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.segmentation.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"deeplabv3.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_H = INPUT_SIZE_H\n const val OUTPUT_SIZE_W = INPUT_SIZE_W\n const val OUTPUT_SIZE_C = 21\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: API References\\API reference\nContent:\n## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references)\n- **_NumberOfBuffersInfo**\n- **_ennBuffer**\n- **_ennBufferInfo**\n## [ENN Framwork API Functions](api-reference/enn-framework-api-functions)\n- **Context initialize / deinitialize**\n- **OpenModel / CloseModel related**\n- **Memory Handling**\n- **Setters and Getters for model**\n- **Commit Buffer**\n- **Execute Models**\n- **Security, preference, get meta information..** ",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-1\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnSetPreferencePresetId**(const uint32_t val)<br>Setting Preset ID for operation performance. |\n| EnnReturn | **EnnSetPreferencePerfConfigId**(const uint32_t val)<br>Setting PerfConfig ID for operation performance. |\n| EnnReturn | **EnnSetPreferencePerfMode**(const uint32_t val)<br>Setting Performance Mode. |\n| EnnReturn | **EnnSetPreferenceTimeOut**(const uint32_t val)<br>Setting Preset ID for time out. |\n| EnnReturn | **EnnSetPreferencePriority**(const uint32_t val)<br>Setting priority value for NPU. |\n| EnnReturn | **EnnSetPreferenceCoreAffinity**(const uint32_t val)<br>Setting affinity to set NPU core operation. |\n| EnnReturn | **EnnGetPreferencePresetId**(uint32_t * val_ptr)<br>Get current information for Preset ID. |",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK."
            ]
        ],
        "answer": "The main purpose of EDS Assistant is to provide support for developers utilizing the Samsung S.LSI Exynos Neural Network Software Development Kit (ENN SDK). It is designed to offer guidance, information, and assistance related to model conversion, execution, API usage, support, and troubleshooting for developers using the ENN framework on Exynos platforms.",
        "ground_truth": "The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK."
    },
    {
        "question": "Please tell us about your future goals for EDS Assistant?",
        "contexts": [
            [
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
                "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
                "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Copying Data to ERD Board\nContent:\nThe following commands copy the necessary files to the ERD board:\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/model.nnc /data/local/tmp/\nadb push example/input.bin /data/local/tmp/\nadb push example/golden.bin /data/local/tmp/\n```\nThese commands perform the following actions:\n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-2\nContent:\n| EnnReturn | **EnnGetPreferencePerfConfigId**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. |\n| EnnReturn | **EnnGetPreferencePerfMode**(uint32_t * val_ptr)<br>Get current information for Performance Mode. |\n| EnnReturn | **EnnGetPreferenceTimeOut**(uint32_t * val_ptr)<br>Get current information for Time Out. |\n| EnnReturn | **EnnGetPreferencePriority**(uint32_t * val_ptr)<br>Get current information for NPU Priority. |\n| EnnReturn | **EnnGetPreferenceCoreAffinity**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. |\n| EnnReturn | **EnnGetMetaInfo**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. |\n| EnnReturn | **EnnSetExecMsgAlwaysOn**()<br>Set frequency of execution message print. |",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.segmentation.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"deeplabv3.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_H = INPUT_SIZE_H\n const val OUTPUT_SIZE_W = INPUT_SIZE_W\n const val OUTPUT_SIZE_C = 21\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\nExecuting NN models on the ENN framework comprises of three steps such as initializing the framework, inferring the models, and deinitializing the framework.\nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.\n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\n\tA1[Initialize]\n\tA1 --> A2[Open Model]\n\tA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\n\tB1[Copy Input Layer]\n\tB1 --> B2[Execute Model]\n\tB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\n\tC1[Release Buffers]\n\tC1 --> C2[Close Model]\n\tC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```\nTo infer multiple data, repeat `Inference`.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n```kotlin\npackage com.samsung.depthestimation.data\n\nobject ModelConstants {\n const val MODEL_NAME = \\\"MiDaS_v2.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 256\n const val INPUT_SIZE_H = 256\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 256\n const val OUTPUT_SIZE_H = 256\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n}\n```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: How do I post an article in the Best Lab?\nAnswer: Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, up to six projects are featured, receiving special attention. There is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-1\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnSetPreferencePresetId**(const uint32_t val)<br>Setting Preset ID for operation performance. |\n| EnnReturn | **EnnSetPreferencePerfConfigId**(const uint32_t val)<br>Setting PerfConfig ID for operation performance. |\n| EnnReturn | **EnnSetPreferencePerfMode**(const uint32_t val)<br>Setting Performance Mode. |\n| EnnReturn | **EnnSetPreferenceTimeOut**(const uint32_t val)<br>Setting Preset ID for time out. |\n| EnnReturn | **EnnSetPreferencePriority**(const uint32_t val)<br>Setting priority value for NPU. |\n| EnnReturn | **EnnSetPreferenceCoreAffinity**(const uint32_t val)<br>Setting affinity to set NPU core operation. |\n| EnnReturn | **EnnGetPreferencePresetId**(uint32_t * val_ptr)<br>Get current information for Preset ID. |",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Context initialize / deinitialize\nContent:\n### Functions\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnInitialize**(void )<br>Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. |\n| EnnReturn | **EnnDeinitialize**(void )<br>Deinitialize Enn Framework. Framework degenerates context in a caller's process. |\n### Functions Documentation\n#### function EnnInitialize\n```cpp\nEnnReturn EnnInitialize(\n\tvoid\n)\n```\nInitialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.\n**Return**: EnnReturn result, 0 is success\n#### function EnnDeinitialize\n```cpp\nEnnReturn EnnDeinitialize(\n\tvoid\n)\n```\nDeinitialize Enn Framework. Framework degenerates context in a caller's process.\n**Return**: EnnReturn result, 0 is success ",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: Starting Android Project\\Starting the Android Project\\Using ERD Board\\Linking Up the ERD Board\nContent:\n#### Configuring the ERD Board\nEnabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.\nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\n\tThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**.\n#### Connecting the ERD Board\n1. Connect the ERD board to your computer using a USB cable.\n2. In the pop-up that appears, select **Allow** to enable the USB debugging.\n3. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device."
            ]
        ],
        "answer": "EDS Assistant will evolve to provide information on technical issues, product information, events, and announcements related to Samsung S.LSI Exynos products, including AI-related services such as the Samsung S.LSI ENN SDK. Additionally, it will offer support for users using the ENN SDK, along with helpful guidance for developers.",
        "ground_truth": "EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK."
    },
    {
        "question": "Please tell us about the company that actually developed EDS Assistant.",
        "contexts": [
            [
                "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Copying Data to ERD Board\nContent:\nThe following commands copy the necessary files to the ERD board:\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/model.nnc /data/local/tmp/\nadb push example/input.bin /data/local/tmp/\nadb push example/golden.bin /data/local/tmp/\n```\nThese commands perform the following actions:\n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: Using Adb to Execute\\Using Adb to Execute\nContent:\nThis section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "URL: news/40ed73bb-4b81-47ed-b2ea-96181bdaa61b/detail\nCategory: News\nTitle: \uc774\uc81c\ub294 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc\u2019\ub85c! \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \ucd08\uc5f0\uacb0 \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 \uae30\uc220\nContent: \uc77c\uae4c? \uc0dd\uc0dd\ud55c \ud604\uc7a5 \uc18c\uc2dd\uc744 \ub2f4\uae30 \uc704\ud574 \uc0bc\uc131\uc804\uc790 \ubc18\ub3c4\uccb4 \ub274\uc2a4\ub8f8\uc774 \ubbf8\uad6d \uc0c8\ub108\uc81c\uc774\uc5d0 \uc704\uce58\ud55c \ubbf8\uc8fc\ucd1d\uad04(DSA) \uc0ac\uc625\uc73c\ub85c \ubc1c\uac78\uc74c\ud588\ub2e4.\n**\ub85c\ubd07\uc758 \ub300\uc911\ud654\ub97c \uc704\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4\uc758 \ub3c4\uc57d**\n<br>\n\uccad\uba85\ud55c \uac00\uc744 \ud587\uc0b4\uc774 \ub0b4\ub9ac\ucb10\ub294 DSA \ucea0\ud37c\uc2a4 \uc55e. \ud14c\ud06c \ub370\uc774 \ud589\uc0ac\ub97c \uc55e\ub450\uace0 \ud604\uc7a5\uc740 \uc774\ub978 \uc2dc\uac01\ubd80\ud130 \ub9ce\uc740 \ucc38\uad00\uac1d\ub4e4\ub85c \ubd81\uc801\uc600\ub2e4. \uc774\ub4e4\uc740 \ubcf8 \ud589\uc0ac\uac00 \uc2dc\uc791\ub418\uae30 \uc804, \uc785\uad6c\uc5d0 \ub193\uc778 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc \uc6d4(System LSI Humanoid Wall)\u2019\uc744 \ud765\ubbf8\ub86d\uac8c \uc0b4\ud3b4\ubcf4\uba70, \ud65c\ubc1c\ud55c \ub124\ud2b8\uc6cc\ud0b9\uc73c\ub85c \ud604\uc7a5\uc744 \ub354\uc6b1 \ud65c\uae30\ucc28\uac8c \ub9cc\ub4e4\uc5c8\ub2e4.",
                "Section: Starting Android Project\\Starting the Android Project\\Using ERD Board\\Linking Up the ERD Board\nContent:\n#### Configuring the ERD Board\nEnabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.\nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\n\tThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**.\n#### Connecting the ERD Board\n1. Connect the ERD board to your computer using a USB cable.\n2. In the pop-up that appears, select **Allow** to enable the USB debugging.\n3. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n```kotlin\npackage com.samsung.depthestimation.data\n\nobject ModelConstants {\n const val MODEL_NAME = \\\"MiDaS_v2.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 256\n const val INPUT_SIZE_H = 256\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 256\n const val OUTPUT_SIZE_H = 256\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n}\n```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n>adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n>```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: How do I post an article in the Best Lab?\nAnswer: Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, up to six projects are featured, receiving special attention. There is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.segmentation.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"deeplabv3.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_H = INPUT_SIZE_H\n const val OUTPUT_SIZE_W = INPUT_SIZE_W\n const val OUTPUT_SIZE_C = 21\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file."
            ]
        ],
        "answer": "Samsung S.LSI supervises the development of EDS Assistant, and ThunderSoft Korea supervises the actual development.",
        "ground_truth": "Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development."
    },
    {
        "question": "Please tell us about the ENN SDK.",
        "contexts": [
            [
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Using JNI Library from Kotlin\nContent:\nFollowing is an example for using JNI function in Kotlin.\n1. Load the JNI library\n\t(example):\n\t```kotlin\n\tinit {\n\t\tSystem.loadLibrary(\"enn_jni\")\n\t\t...\n\t}\n\t```\n2. Declare the external function\n\t(example):\n\t```kotlin\n\tprivate external fun ennOpenModel(filename: String): Long\n\t...\n\t```\n3. Use the JNI function (example):\n\t```kotlin\n\t// to open a model\n\tprivate fun setupENN() {\n\t\t...\n\t\tmodelId = ennOpenModel(fileAbsoluteDirectory)\n\t\t...\n\t}\n\t```",
                "Section: API References\\API reference\nContent:\n## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references)\n- **_NumberOfBuffersInfo**\n- **_ennBuffer**\n- **_ennBufferInfo**\n## [ENN Framwork API Functions](api-reference/enn-framework-api-functions)\n- **Context initialize / deinitialize**\n- **OpenModel / CloseModel related**\n- **Memory Handling**\n- **Setters and Getters for model**\n- **Commit Buffer**\n- **Execute Models**\n- **Security, preference, get meta information..** ",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Context initialize / deinitialize\nContent:\n### Functions\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnInitialize**(void )<br>Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. |\n| EnnReturn | **EnnDeinitialize**(void )<br>Deinitialize Enn Framework. Framework degenerates context in a caller's process. |\n### Functions Documentation\n#### function EnnInitialize\n```cpp\nEnnReturn EnnInitialize(\n\tvoid\n)\n```\nInitialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.\n**Return**: EnnReturn result, 0 is success\n#### function EnnDeinitialize\n```cpp\nEnnReturn EnnDeinitialize(\n\tvoid\n)\n```\nDeinitialize Enn Framework. Framework degenerates context in a caller's process.\n**Return**: EnnReturn result, 0 is success ",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\nContent:\nThis section provides an overview of Android (Kotlin) sample applications.\nEach sample application entry provides the details of the functionality of the sample application, its location, and instructions for running it.\nFor more information on implementing the sample applications, refer to Getting Started With Android Samples guide.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Adding C++ to Module\nContent:\n1. Right click **Project** panel with **Android** option being selected.\n\t\n2. Select the **Add C++ to Module** option and click OK.\n\t",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Writing JNI Library\nContent:\nFollowing function is an example of an implemented JNI wrapper.<br>\nFor more information, refer to the Android Developer Documentation.\n(example):\n```cpp\nextern \"C\"\nJNIEXPORT jlong JNICALL\nJava_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel(\n\t\tJNIEnv *env,\n\t\tjobject thiz,\n\t\tjstring j_filename\n) {\n\tEnnModelId model_id;\n\tconst char *filename = env->GetStringUTFChars(j_filename, 0);\n\tif (enn::api::EnnOpenModel(filename, &model_id)) {\n\t\t__android_log_print(ANDROID_LOG_ERROR, LOG_TAG, \"EnnOpenModel of [%s] Failed\", filename);\n\t}\n\treturn static_cast<jlong>(model_id);\n}\n```",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application."
            ]
        ],
        "answer": "The Exynos Neural Network Software Development Kit (ENN SDK) is a comprehensive toolkit developed by Samsung S.LSI to optimize and implement AI and machine learning models on Exynos-based devices. It allows users to convert trained TensorFlow Lite neural network models to a format that can efficiently run on Samsung Exynos hardware, enabling optimal performance. The ENN SDK includes the ENN SDK service for model conversion and the ENN framework for executing converted models on Exynos platforms. The SDK supports various AI model categories such as Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, and Computer Vision for Healthcare. Additionally, it provides comprehensive documentation, guided samples, and support for developers. If you have specific questions or need assistance with any aspect of the ENN SDK, please feel free to ask!",
        "ground_truth": "ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement."
    },
    {
        "question": "I want to know about ENN SDK.",
        "contexts": [
            [
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Section: API References\\API reference\nContent:\n## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references)\n- **_NumberOfBuffersInfo**\n- **_ennBuffer**\n- **_ennBufferInfo**\n## [ENN Framwork API Functions](api-reference/enn-framework-api-functions)\n- **Context initialize / deinitialize**\n- **OpenModel / CloseModel related**\n- **Memory Handling**\n- **Setters and Getters for model**\n- **Commit Buffer**\n- **Execute Models**\n- **Security, preference, get meta information..** ",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Context initialize / deinitialize\nContent:\n### Functions\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnInitialize**(void )<br>Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. |\n| EnnReturn | **EnnDeinitialize**(void )<br>Deinitialize Enn Framework. Framework degenerates context in a caller's process. |\n### Functions Documentation\n#### function EnnInitialize\n```cpp\nEnnReturn EnnInitialize(\n\tvoid\n)\n```\nInitialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.\n**Return**: EnnReturn result, 0 is success\n#### function EnnDeinitialize\n```cpp\nEnnReturn EnnDeinitialize(\n\tvoid\n)\n```\nDeinitialize Enn Framework. Framework degenerates context in a caller's process.\n**Return**: EnnReturn result, 0 is success ",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Using JNI Library from Kotlin\nContent:\nFollowing is an example for using JNI function in Kotlin.\n1. Load the JNI library\n\t(example):\n\t```kotlin\n\tinit {\n\t\tSystem.loadLibrary(\"enn_jni\")\n\t\t...\n\t}\n\t```\n2. Declare the external function\n\t(example):\n\t```kotlin\n\tprivate external fun ennOpenModel(filename: String): Long\n\t...\n\t```\n3. Use the JNI function (example):\n\t```kotlin\n\t// to open a model\n\tprivate fun setupENN() {\n\t\t...\n\t\tmodelId = ennOpenModel(fileAbsoluteDirectory)\n\t\t...\n\t}\n\t```",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnExecuteModel**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers. |\n| EnnReturn | **EnnExecuteModelAsync**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously. |\n| EnnReturn | **EnnExecuteModelWait**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling EnnExecuteModelAsync() If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-2\nContent:\n| EnnReturn | **EnnGetPreferencePerfConfigId**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. |\n| EnnReturn | **EnnGetPreferencePerfMode**(uint32_t * val_ptr)<br>Get current information for Performance Mode. |\n| EnnReturn | **EnnGetPreferenceTimeOut**(uint32_t * val_ptr)<br>Get current information for Time Out. |\n| EnnReturn | **EnnGetPreferencePriority**(uint32_t * val_ptr)<br>Get current information for NPU Priority. |\n| EnnReturn | **EnnGetPreferenceCoreAffinity**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. |\n| EnnReturn | **EnnGetMetaInfo**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. |\n| EnnReturn | **EnnSetExecMsgAlwaysOn**()<br>Set frequency of execution message print. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnGetMetaInfo\n```cpp\nEnnReturn EnnGetMetaInfo(\n\tconst EnnMetaTypeId info_id,\n\tconst EnnModelId model_id,\n\tchar output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]\n)\n```\nGet Meta Information.\n**Parameters**:\n- **info_id** info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done.\n```cpp\nENN_META_VERSION_FRAMEWORK\nENN_META_VERSION_COMMIT\nENN_META_VERSION_MODEL_COMPILER_NNC\nENN_META_VERSION_MODEL_COMPILER_NPU\nENN_META_VERSION_MODEL_COMPILER_DSP\nENN_META_VERSION_MODEL_SCHEMA\nENN_META_VERSION_MODEL_VERSION\nENN_META_VERSION_DD\nENN_META_VERSION_UNIFIED_FW\nENN_META_VERSION_NPU_FW\nENN_META_VERSION_DSP_FW\n```\n- **model_id**\n- **output_str**\n**Return**: EnnReturn result, 0 is success\nThis API includes loaded model information as well as framework information\n#### function EnnSetExecMsgAlwaysOn\n```cpp\nEnnReturn EnnSetExecMsgAlwaysOn()\n```\nSet frequency of execution message print.\n**Parameters**:\n- **rate** if rate is N, the exe msg shows every {1, N+1, 2N+1..} times.\n**Return**: EnnReturn ",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: Quick Start Guide\\Quick Start Guide\\Basic Workflow\nContent:\nFollowing figure illustrates the three steps for converting and executing an NN model:\n```mermaid\nflowchart LR\n\tsubgraph \"ENN SDK Service\"\n\t\tdirection LR\n\t\tconvert(\"Convert The Model\")\n\tend\n\tsubgraph \"ENN Framework\"\n\t\tdirection LR\n\t\texecute(\"Execute The Model\")\n\tend\n\tmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```\n### Model Conversion\nTo convert TensorFlow Lite models, ENN SDK provides an online conversion tool through the Samsung Exynos Developer Society.\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.\nFor more information on the process of converting NN models, refer to Converting NN Models with ENN SDK Service.\n### Model Execution\nTo execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe ENN SDK provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.\nFor more information on the process of executing NN models, refer to Executing Models Using Native Program and Executing Models Using Android Application.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Adding C++ to Module\nContent:\n1. Right click **Project** panel with **Android** option being selected.\n\t\n2. Select the **Add C++ to Module** option and click OK.\n\t",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK."
            ]
        ],
        "answer": "The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is designed to optimize and implement AI and machine learning models on Exynos-based devices. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.\n\nThe SDK comprises the ENN SDK service, an online platform to convert trained TensorFlow Lite neural network models to Neural Network Container (NNC) format, and the ENN framework, which executes the converted models on Exynos platforms.\n\nIf you have specific questions about the features, tools, APIs, or sample applications provided by the ENN SDK, feel free to ask for more detailed information!",
        "ground_truth": "The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor."
    },
    {
        "question": "I want to download the ENN SDK and install it on a local PC or server to use it.",
        "contexts": [
            [
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n>adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n>```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## The ENN SDK\nThe ENN SDK tool facilitates the conversion of TensorFlow Lite neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the Samsung Exynos hardware to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of the Sample Native Program\nThe sample native program accepts a model file and raw input data files as parameters.\nIf golden output data files are provided, the program compares these files and displays the result. If no file is provided, the program generates a raw output file.\n### Model and Data Used\nIn this sample native program, a converted NNC file and raw input/output file available in the Github Repository are used.\n### Using Native Program\nTo execute the sample native program, refer to README file.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Converting NN Models with ENN SDK Service\nContent:\nIn this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.\nTo get started, you must be a member of the Exynos Developer Society.\n- If you are not a member of the Exynos Developer Society, sign up from here.\n- If you already have an account, log in to the Exynos Developer Society.\nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from here.\n2. Navigate to the ENN SDK service page and provide a title for your project.\n3. Then, upload the downloaded TFLite model\n4. Next, select hardware preferences.\n\t- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n\t- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n5. After confirming your selections, click **Convert** to convert the model.\n6. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n7. Click **NNC Download** to download the converted NNC model.\n\tYou can now integrate the NNC model into the desired application.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Adding C++ to Module\nContent:\n1. Right click **Project** panel with **Android** option being selected.\n\t\n2. Select the **Add C++ to Module** option and click OK.\n\t",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Using JNI Library from Kotlin\nContent:\nFollowing is an example for using JNI function in Kotlin.\n1. Load the JNI library\n\t(example):\n\t```kotlin\n\tinit {\n\t\tSystem.loadLibrary(\"enn_jni\")\n\t\t...\n\t}\n\t```\n2. Declare the external function\n\t(example):\n\t```kotlin\n\tprivate external fun ennOpenModel(filename: String): Long\n\t...\n\t```\n3. Use the JNI function (example):\n\t```kotlin\n\t// to open a model\n\tprivate fun setupENN() {\n\t\t...\n\t\tmodelId = ennOpenModel(fileAbsoluteDirectory)\n\t\t...\n\t}\n\t```",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Copying Data to ERD Board\nContent:\nThe following commands copy the necessary files to the ERD board:\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/model.nnc /data/local/tmp/\nadb push example/input.bin /data/local/tmp/\nadb push example/golden.bin /data/local/tmp/\n```\nThese commands perform the following actions:\n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK."
            ]
        ],
        "answer": "Currently, the ENN SDK is not publicly available for download and installation on local PCs or servers. The version distributed externally will be provided as a separate GUI version with enhanced usability in the future. If you have further questions or need assistance, feel free to ask.",
        "ground_truth": "Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger."
    },
    {
        "question": "Please provide operating system information to use the ENN SDK.",
        "contexts": [
            [
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n>adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n>```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-2\nContent:\n| EnnReturn | **EnnGetPreferencePerfConfigId**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. |\n| EnnReturn | **EnnGetPreferencePerfMode**(uint32_t * val_ptr)<br>Get current information for Performance Mode. |\n| EnnReturn | **EnnGetPreferenceTimeOut**(uint32_t * val_ptr)<br>Get current information for Time Out. |\n| EnnReturn | **EnnGetPreferencePriority**(uint32_t * val_ptr)<br>Get current information for NPU Priority. |\n| EnnReturn | **EnnGetPreferenceCoreAffinity**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. |\n| EnnReturn | **EnnGetMetaInfo**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. |\n| EnnReturn | **EnnSetExecMsgAlwaysOn**()<br>Set frequency of execution message print. |",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Specifying ABI\nContent:\nThe ENN framework currently supports only the `arm64-v8a` ABI. To set ABI, modify the `build.gradle` file and specify the ABI as `arm64-v8a`.\n```\ndefaultConfig {\n\t...\n\tndk {\n\t\tabiFilters \"arm64-v8a\"\n\t}\n\t...\n}\n```",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnGetMetaInfo\n```cpp\nEnnReturn EnnGetMetaInfo(\n\tconst EnnMetaTypeId info_id,\n\tconst EnnModelId model_id,\n\tchar output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]\n)\n```\nGet Meta Information.\n**Parameters**:\n- **info_id** info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done.\n```cpp\nENN_META_VERSION_FRAMEWORK\nENN_META_VERSION_COMMIT\nENN_META_VERSION_MODEL_COMPILER_NNC\nENN_META_VERSION_MODEL_COMPILER_NPU\nENN_META_VERSION_MODEL_COMPILER_DSP\nENN_META_VERSION_MODEL_SCHEMA\nENN_META_VERSION_MODEL_VERSION\nENN_META_VERSION_DD\nENN_META_VERSION_UNIFIED_FW\nENN_META_VERSION_NPU_FW\nENN_META_VERSION_DSP_FW\n```\n- **model_id**\n- **output_str**\n**Return**: EnnReturn result, 0 is success\nThis API includes loaded model information as well as framework information\n#### function EnnSetExecMsgAlwaysOn\n```cpp\nEnnReturn EnnSetExecMsgAlwaysOn()\n```\nSet frequency of execution message print.\n**Parameters**:\n- **rate** if rate is N, the exe msg shows every {1, N+1, 2N+1..} times.\n**Return**: EnnReturn ",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnGetPreferencePerfMode\n```cpp\nEnnReturn EnnGetPreferencePerfMode(\n\tuint32_t * val_ptr\n)\n```\nGet current information for Performance Mode.\n**Parameters**:\n- **val** [OUT] current value of Performance Mode\n**Return**: EnnReturn result, 0 is success\n#### function EnnGetPreferenceTimeOut\n```cpp\nEnnReturn EnnGetPreferenceTimeOut(\n\tuint32_t * val_ptr\n)\n```\nGet current information for Time Out.\n**Parameters**:\n- **val** [OUT] current value of Time Out\n**Return**: EnnReturn result, 0 is success ",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\nContent:\nThis section provides an overview of Android (Kotlin) sample applications.\nEach sample application entry provides the details of the functionality of the sample application, its location, and instructions for running it.\nFor more information on implementing the sample applications, refer to Getting Started With Android Samples guide.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application"
            ]
        ],
        "answer": "The ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
        "ground_truth": "ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability."
    },
    {
        "question": "Please tell us the minimum specifications required to compile via ENN SDK?",
        "contexts": [
            [
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Specifying ABI\nContent:\nThe ENN framework currently supports only the `arm64-v8a` ABI. To set ABI, modify the `build.gradle` file and specify the ABI as `arm64-v8a`.\n```\ndefaultConfig {\n\t...\n\tndk {\n\t\tabiFilters \"arm64-v8a\"\n\t}\n\t...\n}\n```",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Compiling Using NDK\\Compiling Using NDK\\Creating the Makefile\nContent:\nThe Makefile is a crucial component in the build process.\nIt instructs the compiler on how to build the program.\nThe Makefile for this project is divided into two parts such as `Android.mk` and `Application.mk`.\n### Android.mk\nThe `Android.mk` file defines the module and its properties.\n(example):\n```cmake\nLOCAL_PATH := $(call my-dir)\ninclude $(CLEAR_VARS)\nLOCAL_MODULE := enn_public_api_ndk_v1\nLOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so\ninclude $(PREBUILT_SHARED_LIBRARY)\ninclude $(CLEAR_VARS)\nLOCAL_MODULE := enn_nnc_model_tester\nLOCAL_C_INCLUDES += \\\n\t${LOCAL_PATH} \\\n\t${LOCAL_PATH}/include\nLOCAL_LDLIBS := -llog\nLOCAL_CFLAGS += -Wall -std=c++14 -O3\nLOCAL_CPPFLAGS += -fexceptions -frtti\nLOCAL_SRC_FILES := enn_nnc_model_tester.cpp\nLOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1\ninclude $(BUILD_EXECUTABLE)\n```\n### Application.mk:\nThe `Application.mk` file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used.\n(example):\n```cmake\nAPP_ABI := arm64-v8a\nAPP_STL := c++_static\n```",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: Compiling Using NDK\\Compiling Using NDK\nContent:\n## Build Using NDK\nAfter the Makefile is set up, the build process with NDK can be initiated.\n- Windows\n\t```shell\n\t%ANDROID_NDK_HOME%\ndk-build.cmd -C .\\jni\n\t```\n- Linux\n\t```shell\n\t$ANDROID_NDK_HOME/ndk-build -C ./jni\n\t```\nThis command instructs NDK to start the build process in the current directory.\n## Verifying the Build\nAfter the build process is complete, the compiled program can be verified by checking the `libs` directory:\n- Windows\n\t```shell\n\tdir libs\\arm64-v8a\\\n\t```\n- Linux\n\t```shell\n\tls libs/arm64-v8a/\n\t```\n\tThe compiled program (`enn_nnc_model_tester`) is visible in the output.\n## Troubleshooting\nIf you encounter any issues during the build process, ensure the following:\n- The `NDK_PROJECT_PATH` environment variable is correctly set.\n- The `ANDROID_NDK_HOME` environment variable points to the correct location of the NDK installation.\n- The paths in the `Android.mk` file are correct.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Adding C++ to Module\nContent:\n1. Right click **Project** panel with **Android** option being selected.\n\t\n2. Select the **Add C++ to Module** option and click OK.\n\t",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnGetMetaInfo\n```cpp\nEnnReturn EnnGetMetaInfo(\n\tconst EnnMetaTypeId info_id,\n\tconst EnnModelId model_id,\n\tchar output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]\n)\n```\nGet Meta Information.\n**Parameters**:\n- **info_id** info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done.\n```cpp\nENN_META_VERSION_FRAMEWORK\nENN_META_VERSION_COMMIT\nENN_META_VERSION_MODEL_COMPILER_NNC\nENN_META_VERSION_MODEL_COMPILER_NPU\nENN_META_VERSION_MODEL_COMPILER_DSP\nENN_META_VERSION_MODEL_SCHEMA\nENN_META_VERSION_MODEL_VERSION\nENN_META_VERSION_DD\nENN_META_VERSION_UNIFIED_FW\nENN_META_VERSION_NPU_FW\nENN_META_VERSION_DSP_FW\n```\n- **model_id**\n- **output_str**\n**Return**: EnnReturn result, 0 is success\nThis API includes loaded model information as well as framework information\n#### function EnnSetExecMsgAlwaysOn\n```cpp\nEnnReturn EnnSetExecMsgAlwaysOn()\n```\nSet frequency of execution message print.\n**Parameters**:\n- **rate** if rate is N, the exe msg shows every {1, N+1, 2N+1..} times.\n**Return**: EnnReturn ",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-2\nContent:\n| EnnReturn | **EnnGetPreferencePerfConfigId**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. |\n| EnnReturn | **EnnGetPreferencePerfMode**(uint32_t * val_ptr)<br>Get current information for Performance Mode. |\n| EnnReturn | **EnnGetPreferenceTimeOut**(uint32_t * val_ptr)<br>Get current information for Time Out. |\n| EnnReturn | **EnnGetPreferencePriority**(uint32_t * val_ptr)<br>Get current information for NPU Priority. |\n| EnnReturn | **EnnGetPreferenceCoreAffinity**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. |\n| EnnReturn | **EnnGetMetaInfo**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. |\n| EnnReturn | **EnnSetExecMsgAlwaysOn**()<br>Set frequency of execution message print. |",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: Quick Start Guide\\Quick Start Guide\\Basic Workflow\nContent:\nFollowing figure illustrates the three steps for converting and executing an NN model:\n```mermaid\nflowchart LR\n\tsubgraph \"ENN SDK Service\"\n\t\tdirection LR\n\t\tconvert(\"Convert The Model\")\n\tend\n\tsubgraph \"ENN Framework\"\n\t\tdirection LR\n\t\texecute(\"Execute The Model\")\n\tend\n\tmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```\n### Model Conversion\nTo convert TensorFlow Lite models, ENN SDK provides an online conversion tool through the Samsung Exynos Developer Society.\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.\nFor more information on the process of converting NN models, refer to Converting NN Models with ENN SDK Service.\n### Model Execution\nTo execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe ENN SDK provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.\nFor more information on the process of executing NN models, refer to Executing Models Using Native Program and Executing Models Using Android Application.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n>adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n>```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |"
            ]
        ],
        "answer": "The minimum specifications required to compile via ENN SDK depend on the complexity of the algorithms used in the model. For general use, a recently widely used general-purpose PC or laptop (such as Intel Core i5 or AMD Ryzen 5 series) can be utilized. However, for algorithms with high complexity, server-level specifications may be required depending on the model.\n\nIf you require further details regarding the specific system requirements for compiling via the ENN SDK, please let me know.",
        "ground_truth": "For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model."
    },
    {
        "question": "Please tell us about the advantages of ENN SDK compared to other SDKs?",
        "contexts": [
            [
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Using JNI Library from Kotlin\nContent:\nFollowing is an example for using JNI function in Kotlin.\n1. Load the JNI library\n\t(example):\n\t```kotlin\n\tinit {\n\t\tSystem.loadLibrary(\"enn_jni\")\n\t\t...\n\t}\n\t```\n2. Declare the external function\n\t(example):\n\t```kotlin\n\tprivate external fun ennOpenModel(filename: String): Long\n\t...\n\t```\n3. Use the JNI function (example):\n\t```kotlin\n\t// to open a model\n\tprivate fun setupENN() {\n\t\t...\n\t\tmodelId = ennOpenModel(fileAbsoluteDirectory)\n\t\t...\n\t}\n\t```",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Converting NN Models with ENN SDK Service\nContent:\nIn this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.\nTo get started, you must be a member of the Exynos Developer Society.\n- If you are not a member of the Exynos Developer Society, sign up from here.\n- If you already have an account, log in to the Exynos Developer Society.\nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from here.\n2. Navigate to the ENN SDK service page and provide a title for your project.\n3. Then, upload the downloaded TFLite model\n4. Next, select hardware preferences.\n\t- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n\t- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n5. After confirming your selections, click **Convert** to convert the model.\n6. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n7. Click **NNC Download** to download the converted NNC model.\n\tYou can now integrate the NNC model into the desired application.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Adding C++ to Module\nContent:\n1. Right click **Project** panel with **Android** option being selected.\n\t\n2. Select the **Add C++ to Module** option and click OK.\n\t",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |"
            ]
        ],
        "answer": "ENN SDK offers several advantages compared to other SDKs:\n\n1. Optimized for Exynos Chipsets: ENN SDK is specifically optimized for Exynos chipsets, providing efficient AI and machine learning model execution on Exynos-based devices.\n\n2. Support for Various AI Algorithms: The SDK supports a wide range of AI algorithms, facilitating the implementation of diverse AI applications and usage scenarios.\n\n3. User-Friendly Interface Improvement: The SDK continuously improves its user interface to provide developers with efficient tools, ensuring ease of use and seamless integration into development workflows.\n\n4. Continuous Updates and Improvement: Regular updates and improvements are planned to enhance the functionality and usability of the SDK, ensuring that developers have access to the latest features and optimizations.\n\n5. Reflection of Developer Needs: The ENN SDK aims to reflect the needs and voices of various developers, providing a platform for developers to participate in the development process.\n\nThese advantages position ENN SDK as a valuable tool for developers working with Exynos chipsets and seeking optimized AI and machine learning solutions.",
        "ground_truth": "The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program."
    },
    {
        "question": "Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.",
        "contexts": [
            [
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: API References\\API reference\nContent:\n## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references)\n- **_NumberOfBuffersInfo**\n- **_ennBuffer**\n- **_ennBufferInfo**\n## [ENN Framwork API Functions](api-reference/enn-framework-api-functions)\n- **Context initialize / deinitialize**\n- **OpenModel / CloseModel related**\n- **Memory Handling**\n- **Setters and Getters for model**\n- **Commit Buffer**\n- **Execute Models**\n- **Security, preference, get meta information..** ",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnGetPreferencePriority\n```cpp\nEnnReturn EnnGetPreferencePriority(\n\tuint32_t * val_ptr\n)\n```\nGet current information for NPU Priority.\n**Parameters**:\n- **val** [OUT] current value of NPU Priority\n**Return**: EnnReturn result, 0 is success\n#### function EnnGetPreferenceCoreAffinity\n```cpp\nEnnReturn EnnGetPreferenceCoreAffinity(\n\tuint32_t * val_ptr\n)\n```\nGet current information for NPU Core affinity.\n**Parameters**:\n- **val** [OUT] current value of NPU Core affinity\n**Return**: EnnReturn result, 0 is success ",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Using JNI Library from Kotlin\nContent:\nFollowing is an example for using JNI function in Kotlin.\n1. Load the JNI library\n\t(example):\n\t```kotlin\n\tinit {\n\t\tSystem.loadLibrary(\"enn_jni\")\n\t\t...\n\t}\n\t```\n2. Declare the external function\n\t(example):\n\t```kotlin\n\tprivate external fun ennOpenModel(filename: String): Long\n\t...\n\t```\n3. Use the JNI function (example):\n\t```kotlin\n\t// to open a model\n\tprivate fun setupENN() {\n\t\t...\n\t\tmodelId = ennOpenModel(fileAbsoluteDirectory)\n\t\t...\n\t}\n\t```"
            ]
        ],
        "answer": "The weaknesses of ENN SDK are not explicitly documented in the provided context. However, based on the information available, it appears that one area for potential improvement may be the support for expansion into various IoT sectors. The SDK's current focus on mobile terminals may limit its use in certain IoT applications. Additionally, there is no direct comparison of ENN SDK with similar SDKs or tools from other companies in the provided context.\n\nIf you would like to explore potential weaknesses or limitations of ENN SDK in more detail, I can help you look for additional resources or information.",
        "ground_truth": "Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers."
    },
    {
        "question": "Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.",
        "contexts": [
            [
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: API References\\API reference\nContent:\n## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references)\n- **_NumberOfBuffersInfo**\n- **_ennBuffer**\n- **_ennBufferInfo**\n## [ENN Framwork API Functions](api-reference/enn-framework-api-functions)\n- **Context initialize / deinitialize**\n- **OpenModel / CloseModel related**\n- **Memory Handling**\n- **Setters and Getters for model**\n- **Commit Buffer**\n- **Execute Models**\n- **Security, preference, get meta information..** ",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-2\nContent:\n| EnnReturn | **EnnGetPreferencePerfConfigId**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. |\n| EnnReturn | **EnnGetPreferencePerfMode**(uint32_t * val_ptr)<br>Get current information for Performance Mode. |\n| EnnReturn | **EnnGetPreferenceTimeOut**(uint32_t * val_ptr)<br>Get current information for Time Out. |\n| EnnReturn | **EnnGetPreferencePriority**(uint32_t * val_ptr)<br>Get current information for NPU Priority. |\n| EnnReturn | **EnnGetPreferenceCoreAffinity**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. |\n| EnnReturn | **EnnGetMetaInfo**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. |\n| EnnReturn | **EnnSetExecMsgAlwaysOn**()<br>Set frequency of execution message print. |",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-1\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnSetPreferencePresetId**(const uint32_t val)<br>Setting Preset ID for operation performance. |\n| EnnReturn | **EnnSetPreferencePerfConfigId**(const uint32_t val)<br>Setting PerfConfig ID for operation performance. |\n| EnnReturn | **EnnSetPreferencePerfMode**(const uint32_t val)<br>Setting Performance Mode. |\n| EnnReturn | **EnnSetPreferenceTimeOut**(const uint32_t val)<br>Setting Preset ID for time out. |\n| EnnReturn | **EnnSetPreferencePriority**(const uint32_t val)<br>Setting priority value for NPU. |\n| EnnReturn | **EnnSetPreferenceCoreAffinity**(const uint32_t val)<br>Setting affinity to set NPU core operation. |\n| EnnReturn | **EnnGetPreferencePresetId**(uint32_t * val_ptr)<br>Get current information for Preset ID. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Setters and Getters for model\\Functions-1\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnGetBuffersInfo**(NumberOfBuffersInfo * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model. |\n| EnnReturn | **EnnGetBufferInfoByIndex**(EnnBufferInfo * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model. |\n| EnnReturn | **EnnGetBufferInfoByLabel**(EnnBufferInfo * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model. |",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Memory Handling\\Functions\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnCreateBuffer**(EnnBufferPtr * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU) |\n| EnnReturn | **EnnAllocateAllBuffers**(const EnnModelId model_id, EnnBufferPtr ** out_buffers, NumberOfBuffersInfo * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate. |\n| EnnReturn | **EnnReleaseBuffers**(EnnBufferPtr * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array. |\n| EnnReturn | **EnnReleaseBuffer**(EnnBufferPtr buffer)<br>release buffer from EnnCreateBuffer() |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnGetPreferencePriority\n```cpp\nEnnReturn EnnGetPreferencePriority(\n\tuint32_t * val_ptr\n)\n```\nGet current information for NPU Priority.\n**Parameters**:\n- **val** [OUT] current value of NPU Priority\n**Return**: EnnReturn result, 0 is success\n#### function EnnGetPreferenceCoreAffinity\n```cpp\nEnnReturn EnnGetPreferenceCoreAffinity(\n\tuint32_t * val_ptr\n)\n```\nGet current information for NPU Core affinity.\n**Parameters**:\n- **val** [OUT] current value of NPU Core affinity\n**Return**: EnnReturn result, 0 is success ",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Context initialize / deinitialize\nContent:\n### Functions\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnInitialize**(void )<br>Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. |\n| EnnReturn | **EnnDeinitialize**(void )<br>Deinitialize Enn Framework. Framework degenerates context in a caller's process. |\n### Functions Documentation\n#### function EnnInitialize\n```cpp\nEnnReturn EnnInitialize(\n\tvoid\n)\n```\nInitialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.\n**Return**: EnnReturn result, 0 is success\n#### function EnnDeinitialize\n```cpp\nEnnReturn EnnDeinitialize(\n\tvoid\n)\n```\nDeinitialize Enn Framework. Framework degenerates context in a caller's process.\n**Return**: EnnReturn result, 0 is success ",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\\Preparing Framework and Model\nContent:\nBefore executing ML models on the ENN framework, initialize the framework, load the model, and allocate the required buffers.\n1. EnnInitialize:\n\tThis function initializes the ENN Framework.\n\t(example):\n\t```cpp\n\tenn::api::EnnInitialize();\n\t```\n2. EnnOpenModel:\n\tThis function opens the specified model and returns a model ID.\n\t(example):\n\t```cpp\n\tEnnModelId model_id;\n\tenn::api::EnnOpenModel(model_name.c_str(), &model_id);\n\t```\n3. EnnAllocateAllBuffers:\n\tThis function allocates the required buffers for the model.\n\tIt also provides the number of input/output buffers (`buffer_info`), their locations, and sizes (`buffer_set`).\n\t(example):\n\t```cpp\n\tEnnBufferPtr *buffer_set;\n\tNumberOfBuffersInfo buffer_info;\n\tenn::api::EnnAllocateAllBuffers(model_id, &buffer_set, &buffer_info);\n\t```\nFollowing is the data structure of `EnnBufferPtr` and `NumberOfBuffersInfo`.\n```cpp\ntypedef struct _ennBuffer {\n\tvoid *va;\n\tuint32_t size;\n\tuint32_t offset;\n} EnnBuffer;\ntypedef EnnBuffer* EnnBufferPtr;\ntypedef struct _NumberOfBuffersInfo {\n\tuint32_t n_in_buf;\n\tuint32_t n_out_buf;\n} NumberOfBuffersInfo;\n```",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnGetMetaInfo\n```cpp\nEnnReturn EnnGetMetaInfo(\n\tconst EnnMetaTypeId info_id,\n\tconst EnnModelId model_id,\n\tchar output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]\n)\n```\nGet Meta Information.\n**Parameters**:\n- **info_id** info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done.\n```cpp\nENN_META_VERSION_FRAMEWORK\nENN_META_VERSION_COMMIT\nENN_META_VERSION_MODEL_COMPILER_NNC\nENN_META_VERSION_MODEL_COMPILER_NPU\nENN_META_VERSION_MODEL_COMPILER_DSP\nENN_META_VERSION_MODEL_SCHEMA\nENN_META_VERSION_MODEL_VERSION\nENN_META_VERSION_DD\nENN_META_VERSION_UNIFIED_FW\nENN_META_VERSION_NPU_FW\nENN_META_VERSION_DSP_FW\n```\n- **model_id**\n- **output_str**\n**Return**: EnnReturn result, 0 is success\nThis API includes loaded model information as well as framework information\n#### function EnnSetExecMsgAlwaysOn\n```cpp\nEnnReturn EnnSetExecMsgAlwaysOn()\n```\nSet frequency of execution message print.\n**Parameters**:\n- **rate** if rate is N, the exe msg shows every {1, N+1, 2N+1..} times.\n**Return**: EnnReturn ",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\nContent:\nThis section provides an overview of Android (Kotlin) sample applications.\nEach sample application entry provides the details of the functionality of the sample application, its location, and instructions for running it.\nFor more information on implementing the sample applications, refer to Getting Started With Android Samples guide.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Setters and Getters for model\\Functions Documentation\nContent:\n#### function EnnGetBuffersInfo\n```cpp\nEnnReturn EnnGetBuffersInfo(\n\tNumberOfBuffersInfo * buffers_info,\n\tconst EnnModelId model_id\n)\n```\nGet buffers information from loaded model.\n**Parameters**:\n- **buffers_info** [OUT] number of in / out buffer which caller should commit.\n- **model_id** [IN] model id from OpenModel()\n**Return**: EnnReturn result, 0 is success\n#### function EnnGetBufferInfoByIndex\n```cpp\nEnnReturn EnnGetBufferInfoByIndex(\n\tEnnBufferInfo * out_buf_info,\n\tconst EnnModelId model_id,\n\tconst enn_buf_dir_e direction,\n\tconst uint32_t index\n)\n```\nGet one buffer information from loaded model.\n**Parameters**:\n- **out_buf_info** [OUT] output buffer information\n- **model_id** [IN] model ID from load_model\n- **direction** [IN] direction (IN, OUT)\n- **index** [IN] buffer's index number in model\n**Return**: EnnReturn result, 0 is success\n```cpp\ntypedef struct _ennBufferInfo {\n\tbool is_able_to_update; // this is not used\n\tuint32_t n;\n\tuint32_t width;\n\tuint32_t height;\n\tuint32_t channel;\n\tuint32_t size;\n\tconst char *label;\n} EnnBufferInfo;\n```\na caller can identify a buffer as {DIR, Index} such as {IN, 0}",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnGetPreferencePerfMode\n```cpp\nEnnReturn EnnGetPreferencePerfMode(\n\tuint32_t * val_ptr\n)\n```\nGet current information for Performance Mode.\n**Parameters**:\n- **val** [OUT] current value of Performance Mode\n**Return**: EnnReturn result, 0 is success\n#### function EnnGetPreferenceTimeOut\n```cpp\nEnnReturn EnnGetPreferenceTimeOut(\n\tuint32_t * val_ptr\n)\n```\nGet current information for Time Out.\n**Parameters**:\n- **val** [OUT] current value of Time Out\n**Return**: EnnReturn result, 0 is success ",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\n#### Executing Model\nThe execution phase of model involves copying the input layer, executing the model, and copying the output layer.\nFor more information on copying data to the input layer and comparing data with the output layer, refer to Section Processing Input and Outputs section.\n1. EnnExecuteModel:\n\tThis function executes the model using the specified model ID.\n\t(example):\n\t```cpp\n\tenn::api::EnnExecuteModel(model_id)\n\t```\n#### Deinitializing Framework\nAfter executing the model, deinitialize the framework to release resources.\n1. EnnReleaseBuffers:\n\tThis function releases the allocated buffers.\n\t(example):\n\t```cpp\n\tenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n\t```\n2. EnnCloseModel:\n\tThis function closes the specified model.\n\t(example):\n\t```cpp\n\tenn::api::EnnCloseModel(model_id)\n\t```\n3. EnnDeinitialize:\n\tThis function deinitializes the ENN framework.\n\t(example):\n\t```cpp\n\tenn::api::EnnDeinitialize()\n\t```",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE."
            ]
        ],
        "answer": "Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability. If you have further questions, feel free to ask.",
        "ground_truth": "Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability."
    },
    {
        "question": "I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.",
        "contexts": [
            [
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference."
            ]
        ],
        "answer": "To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model. If you have specific questions about the ENN SDK or need guidance on a particular aspect of your project, feel free to ask!",
        "ground_truth": "To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model."
    },
    {
        "question": "I want to know about the AI Challenger.",
        "contexts": [
            [
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc694\uac74\uc744 \ucda9\uc871\ud558\uace0 \uc778\uac04\uc758 \ud589\ub3d9\uc744 \uc644\ubcbd\ud558\uac8c \ubaa8\ubc29\ud55c\ub2e4\ub294 \uad81\uadf9\uc801\uc778 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 \uc0bc\uc131\uc740 \uc800\uc9c0\uc5f0\uacfc \uc800\uc804\ub825\uc744 \ub450 \uac00\uc9c0 \ud575\uc2ec \ubc1c\uc804 \ubd84\uc57c\ub85c \uc120\uc815\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uac1c\ubc1c\ud300 \ud64d\uae30\uc900 \uc0c1\ubb34\ub294 \u201c\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\u201d\uc774\ub77c\ub294 \uc81c\ubaa9\uc758 \uc138\uc158\uc5d0\uc11c \uc774 \ub450 \uac00\uc9c0 \ud575\uc2ec \ubd84\uc57c\ub97c \uc5b4\ub5bb\uac8c \ubc1c\uc804\uc2dc\ucf1c\uc654\ub294\uc9c0 \uc124\uba85\ud588\ub2e4.\n**\uc0c1\ud669 \uc778\uc9c0 \ucef4\ud4e8\ud305**\n\ub2e4\uc591\ud55c \uae30\ub2a5\uc774 \ud0d1\uc7ac\ub41c \ucd5c\uc2e0 \ub514\ubc14\uc774\uc2a4 \uc911 \uc0c1\ub2f9\uc218\ub294 \uc0c1\ud669 \uc778\uc9c0 \ucef4\ud4e8\ud3053\uc73c\ub85c \uad6c\ud604\ub418\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud55c\ub2e4. \uc0c1\ud669 \uc778\uc9c0\ub294 \uc801\uc678\uc120 \uc13c\uc2f1\uc5d0\uc11c \uc2dc\uc791\ub418\uc5c8\uc9c0\ub9cc, \ucd5c\uc2e0 \uc13c\uc11c\ub294 \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 \uc0c8\ub85c\uc6b4 \uac10\uac01 \ub370\uc774\ud130\uc758 \uc138\uacc4\ub97c \uc5f4\uc5b4 \uc8fc\uc5c8",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _* *_\n**\u25a1 5\uc77c (\ud604\uc9c0\uc2dc\uac04), \uc751\uc6a9\ucc98\ubcc4 \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \ud604\ud669\u00b7\ube44\uc804 \ubc1c\ud45c**\n - \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\uc220 \uac1c\ubc1c \uad00\ub828 \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\n\n**\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d, \uc0dd\uc131\ud615 AI \uae30\uc220 \ud0d1\uc7ac\ud55c '\uc5d1\uc2dc\ub178\uc2a4 2400' \uacf5\uac1c**\n - \uc804\uc791 \ub300\ube44 CPU \uc131\ub2a5 1.7\ubc30, AI \uc131\ub2a5 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\n - \uc5d1\uc2dc\ub178\uc2a4 2400 \uae30\ubc18 \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \uac8c\uc784 \ud504\ub9ac\ubdf0 \uc601\uc0c1 \uacf5\uac1c\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n - \uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4 \uacf5\uac1c\u2026\ucd5c\ub300 4\ubc30 \uc90c\uc5d0\ub3c4 \ucd08\uace0\ud654\uc9c8, AI \ud2b8\ub798\ud0b9 \uae30\uc220 \uc801\uc6a9\n - \ucc28\uc138\ub300 \ucc28\ub7c9\uc6a9 \uc194\ub8e8\uc158, \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c(NTN) \uc601\uc0c1 \uc2dc\uc5f0",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc0dd\uc131\ud615 \uc778\uacf5\uc9c0\ub2a5(AI) \ubc0f \uac01\uc885 \ucca8\ub2e8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ub4f1\uc7a5\uc73c\ub85c \uc778\ud574 \uc2dc\uc7a5\uc5d0\uc11c\ub294 \ud6a8\uc728\uc131, \ud655\uc7a5\uc131, \uc800\uc804\ub825\uc744 \uac16\ucd98 \ub354\uc6b1 \uc2a4\ub9c8\ud2b8\ud558\uace0 \uac15\ub825\ud55c \ucef4\ud4e8\ud305\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4. \uc774\uc5d0 \ub530\ub77c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\uc758 \uc8fc\uc694 \ud14c\ub9c8 \uc911 \ud558\ub098\uc778 \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\ub97c \uc8fc\uc81c\ub85c \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220, \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158, \uc0bc\uc131 \uc624\ud1a0 SoC(System-on-Chips) \ub4f1 \uc138 \uac1c\uc758 \uac15\uc5f0\uc774 \uc9c4\ud589\ub418\uc5c8\ub2e4.\n**\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220**\n\uccab\ubc88\uc9f8 \uc138\uc158\uc778 \"\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220\"\uc740 \uc0bc\uc131 \uc624\uc2a4\ud2f4 \uc5f0\uad6c\uc13c\ud130 \ubca0\ub2c8 \uce74\ud2f0\ube44\uc548(Benny Katibian) \ubd80\uc0ac\uc7a5\uc774 \uc9c4\ud589\ud588\ub2e4",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc911\uc559 \ucef4\ud4e8\ud305\uc5d0 \ub300\ud55c \uc218\uc694\uac00 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ud558\ub4dc\uc6e8\uc5b4\uc640 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \ud655\uc7a5\uc131\uc774 \ub354\uc6b1 \uc911\uc694\ud574\uc9c8 \uac83\uc774\ub2e4. \ub530\ub77c\uc11c \uc0bc\uc131\uc758 \ud575\uc2ec \uc194\ub8e8\uc158 \uc911 \ud558\ub098\ub294 \uc18c\ud504\ud2b8\uc6e8\uc5b4\ub97c \uc218\uc815\ud558\uc9c0 \uc54a\uace0\ub3c4 \ucef4\ud4e8\ud305 \uc6a9\ub7c9\uc744 \ub450 \ubc30\ub85c \ub298\ub9ac\uae30 \uc704\ud574 \ub450 SoC \uac04\uc758 \uc9c1\uc811 \uc5f0\uacb0(die-to-die)\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. \ub610\ud55c \uc624\ud1a0 SoC\ub294 OEM\uc758 \uc2dc\uc2a4\ud15c\uc5d0 \ub530\ub77c \ud328\ud0a4\uc9c0 \uac04 \uba40\ud2f0 SoC \uc5f0\uacb0\uc744 \uc704\ud574 PCIe \ub610\ub294 \uc774\ub354\ub137 \uc0ac\uc6a9\uc744 \uc9c0\uc6d0\ud55c\ub2e4.\n_*AI \ud1b5\ud569, \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc815\uc758 \ucc28\ub7c9\uc758 \ubbf8\ub798\ub97c \uc704\ud55c \uc900\ube44 *_\n\uc55e\uc73c\ub85c \uc6b4\uc804\uc790\ub294 \ucd5c\ub300 150\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uc2e4\uc2dc\uac04\uc73c\ub85c \uc791\ub3d9\ud560 \uc218 \uc788\ub294 \uc0bc\uc131\uc758 \uc804\uc6a9 AI \uac00\uc18d\uae30\ub97c \ud1b5\ud574 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0 \uae30\ubc18\ud55c AI \ube44\uc11c\uc640 \uc6d0\ud65c\ud558\uac8c \uc0c1\ud638 \uc791\uc6a9\ud560 \uc218 \uc788\uac8c \ub420 \uac83\uc774\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uba40\ud2f0\ubbf8\ub514\uc5b4\uc758 \uae30\uc220\uc758 \ubc1c\uc804\uc73c\ub85c \ud734\uba38\ub178\uc774\ub4dc\uac00 \uc778\uac04\ucc98\ub7fc \uc790\uc5f0\uc2a4\ub7fd\uac8c \uac10\uc9c0\ud558\uace0, \uc778\uc9c0\ud558\uace0, \ud589\ub3d9\ud558\ub294 \ubbf8\ub798\uc5d0 \ub354 \uac00\uae4c\uc6cc\uc9c0\uace0 \uc788\ub2e4. \ud64d \uc0c1\ubb34\ub294 \"\uc774\ub7ec\ud55c \ubbf8\ub798\ub97c \uc2e4\ud604\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc544\uc9c1 \ud604\uc2e4\uacfc\uc758 \uae30\uc220\uc801\uc778 \uaca9\ucc28\ub97c \uc881\ud600\uc57c \ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac \uc2dc\uc2a4\ud15c \uae30\uc220\ub85c \ubab0\uc785\uac10 \ub118\uce58\uace0 \uc778\ud130\ub799\ud2f0\ube0c\ud55c \ubbf8\ub798\ub97c \uad6c\ud604\ud560 \uac83\u201d\uc774\ub77c\uace0 \ud655\uc2e0\ud588\ub2e4.\n\n1 MobileBERT: \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc758 \ubaa8\ubc14\uc77c \ubc84\uc804\n2 AP: Application Processor\n3 \ucef4\ud4e8\ud305\uc5d0\uc11c \uc0c1\ud669 \uc778\uc9c0\ub294 \uc2dc\uc2a4\ud15c/\uad6c\uc131 \uc694\uc18c\uac00 \ud658\uacbd\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc218\uc9d1\ud558\uace0 \uadf8\uc5d0 \ub530\ub77c \ub3d9\uc791\uc744 \uc870\uc815\ud558\ub294 \uae30\ub2a5\uc744 \ub9d0\ud568\n ",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc544\ud0a4\ud14d\ucc98 \ubcc0\uacbd\uc758 \uacb0\uacfc\ub85c MobileBERT1 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc740 \uc774\uc804 \uc138\ub300\uc5d0 \ube44\ud574 3\ubc30 \ud5a5\uc0c1\ub418\uc5c8\ub2e4\n**\ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc704\ud55c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC**\n\uc790\ub3d9\ucc28\ub294 \ud604\uc7ac \uc804\ud654\uac00 \uc2a4\ub9c8\ud2b8\ud3f0\uc73c\ub85c \uc9c4\ud654\ud55c \uac83\uacfc \uc720\uc0ac\ud558\uac8c \uae09\uaca9\ud558\uac8c \ubcc0\ud654\ud558\uace0 \uc788\ub2e4. \uc2e4\uc81c\ub85c \ud604\ub300\uc758 \uc790\ub3d9\ucc28\ub294 \ub2e8\uc21c\ud55c \uc774\ub3d9 \uc218\ub2e8\uc744 \ub118\uc5b4 \uc0dd\uc131\ud615 AI\uc640 \uac19\uc740 \uad11\ubc94\uc704\ud55c \ucef4\ud4e8\ud305 \uae30\ub2a5\uc758 \uc9d1\ud569\uccb4\uac00 \ub418\uc5c8\ub2e4.\n\uc774\ub7ec\ud55c \ud604\uc2e4\uc744 \ubc30\uacbd\uc73c\ub85c \uc0bc\uc131\uc804\uc790 AP2 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uac1c\ubc1c\ud300 \ubc29\uc9c0\ud6c8 \uc0c1\ubb34\ub294 \uc0bc\uc131 \uc624\ud1a0 SoC \uc194\ub8e8\uc158\uacfc \ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ub300\ud55c \uc138\uc158\uc744 \uc9c4\ud589\ud588\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\ub2a5\uc744 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub3c4\uc785\ud558\uae30 \uc704\ud55c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 IP\uac1c\ubc1c \ubc29\ud5a5\uc131\uc744 \uc18c\uac1c\ud588\ub2e4.\n**\ucd5c\uc801\ud654\ub41c 4\ud074\ub7ec\uc2a4\ud130 \uad6c\uc870\uc758 CPU**\n\uc138 \uac1c\uc758 \uc8fc\uc694 IP \uc911 \uccab \ubc88\uc9f8\ub294 CPU\ub85c, \uae30\uc874\uc5d0\ub294 \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud558\uc774\uc5d4\ub4dc(high-end) \ud074\ub7ec\uc2a4\ud130, \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ub85c\uc6b0\uc5d4\ub4dc(low-end) \ud074\ub7ec\uc2a4\ud130, \uc911\uac04 \ud074\ub7ec\uc2a4\ud130 \ub4f1 \uc138 \uac1c\uc758 \ud074\ub7ec\uc2a4\ud130\ub85c \uad6c\uc131\ub418\uc5c8\ub2e4. \uc0bc\uc131\uc740 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 CPU \uc6b4\uc601\uc744 \uc704\ud574 \uc911\uac04 \ud074\ub7ec\uc2a4\ud130\ub97c \ubbf8\ub4dc \ud558\uc774(mid-high)\uc640 \ubbf8\ub4dc \ub85c\uc6b0(mid-low)\uc758 \ub450 \uac00\uc9c0 \uacc4\uce35\uc73c\ub85c \ub098\ub204\uace0, \ubbf8\ub4dc \ud558\uc774 \uacc4\uce35\uc740 \uac8c\uc784\uacfc \uac19\uc740 \ucef4\ud4e8\ud305 \uc9d1\uc57d\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud588",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc5d1\uc2dc\ub178\uc2a4 \ub808\ud37c\ub7f0\uc2a4 \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub41c \ubaa8\ubc14\uc77c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5 \ub370\ubaa8 \uc601\uc0c1\uc744 \ud1b5\ud574 \uadf8\ub9bc\uc790, \ubc18\uc0ac \ub4f1\uacfc \uac19\uc740 \uc5ec\ub7ec \uac1c\uc758 \uc138\ubd80 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5\ub4e4\uc774 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.\n**\uc0dd\uc131\ud615 AI \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 NPU**\n\uc0bc\uc131\uc758 \ucd5c\uc2e0 NPU \uc194\ub8e8\uc158\uc740 \uba54\ubaa8\ub9ac \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcc0\uacbd\ud558\uace0 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \ube44\uc120\ud615 \uc5f0\uc0b0\uc758 \ud65c\uc6a9\ub3c4\ub97c \ud06c\uac8c \ub192\uc77c \uac83\uc774\ub2e4",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uacfc\uc911\ud55c \uac8c\uc784 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc804\ub825\uacfc \uc131\ub2a5\uc744 \ub354\uc6b1 \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 CPU \uc804\ub825 \ubd80\ubd84\uc744 \uc904\uc774\uace0 \ub354 \ub9ce\uc740 \uc804\ub825\uc744 CPU \ucf54\uc5b4\uc5d0\uc11c GPU\ub85c \uc7ac\ud560\ub2f9\ud558\uc5ec \ubaa8\ubc14\uc77c \uc7a5\uce58\uc5d0\uc11c \ucd5c\uace0 \uc218\uc900\uc758 \uac8c\uc784 \uadf8\ub798\ud53d\uc744 \uc9c0\uc6d0\ud560 \uc218 \uc788\ub3c4\ub85d \ucef4\ud4e8\ud305 \uc131\ub2a5\uc744 \ub192\uc600\ub2e4.\n**\ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub418\ub294 \ud48d\ubd80\ud55c \uadf8\ub798\ud53d \uacbd\ud5d8**\n\uc0bc\uc131\uc740 \ud504\ub9ac\ubbf8\uc5c4\ubd80\ud130 \uc800\uac00\ud615\uc5d0 \uc774\ub974\ub294 \ub113\uc740 \ubc94\uc704\uc758 \ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0 \ucf58\uc194 \uc218\uc900\uc758 \uac8c\uc784\uc744 \uc81c\uacf5\ud55c\ub2e4\ub294 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 AMD\uc758 RDNATM \uc544\ud0a4\ud14d\ucc98\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 Xclipse GPU\ub97c \uac1c\ubc1c\ud574\uc654\uace0, \uc774\ub97c \ud1b5\ud574 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(ray tracing) \uae30\ub2a5\uc744 \ucd5c\ucd08\ub85c \ub3c4\uc785\ud588\ub2e4",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790 \uc2dc\uc2a4\ud15cLSI\uc0ac\uc5c5\ubd80 \ubc15\uc6a9\uc778 \uc0ac\uc7a5\uc740 \"\ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\uace0 \ucc98\ub9ac\ud558\ub294 '\uc0dd\uc131\ud615 AI'\uac00 \uc62c\ud574 \uac00\uc7a5 \uc911\uc694\ud55c \uae30\uc220 \ud2b8\ub79c\ub4dc\ub85c \uc790\ub9ac \uc7a1\uc73c\uba70 \ub354 \uace0\ub3c4\ud654\ub41c \uae30\ubc18 \uae30\uc220 \ud655\ubcf4\uc758 \ud544\uc694\uc131\uc774 \ub300\ub450\ub418\uace0 \uc788\ub2e4\" \uba70 \"\uc0bc\uc131\uc804\uc790\ub294 \uace0\uc131\ub2a5 IP\ubd80\ud130 \uc7a5\ub2e8\uac70\ub9ac \ud1b5\uc2e0 \uc194\ub8e8\uc158, \uc778\uac04\uc758 \uc624\uac10\uc744 \ubaa8\ubc29\ud55c \uc13c\uc11c \uae30\ubc18 '\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc(System LSI Humanoid)'\ub97c \uad6c\ud604\ud574\ub098\uac00\uba70 \uc0dd\uc131\ud615 AI\uc5d0\uc11c \ub354 \ubc1c\uc804\ub41c '\uc120\ud589\uc801 AI(Proactive AI)' \uc2dc\ub300\ub97c \uc5f4 \uac83\uc774\ub2e4\"\uace0 \ubc1d\ud614\ub2e4.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\uac00 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \ubbf8\uc8fc\ucd1d\uad04(Device Solutions America office)\uc5d0\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023'\uc744 \uac1c\ucd5c\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \ucd08\uc9c0\ub2a5\ud654(Hyper-Intelligence)\u00b7\ucd08\uc5f0\uacb0\uc131(Hyper-Connected)\u00b7\ucd08\ub370\uc774\ud130(Hyper-Data)\ub97c \uac00\ub2a5\ud558\uac8c \ud560 \uc8fc\uc694 \uc751\uc6a9\ucc98\ubcc4 \ucd5c\uc2e0 \ubc18\ub3c4\uccb4 \uc124\uacc4 \ud604\ud669\uacfc \ube44\uc804\uc744 \uacf5\uc720\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \uace0\uac1d\uc0ac\uc640 \ud30c\ud2b8\ub108\uc0ac \uad00\uacc4\uc790 300\uc5ec\uba85\uc774 \ucc38\uc11d\ud55c \uac00\uc6b4\ub370, \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \ubd84\uc57c \uae00\ub85c\ubc8c \uc804\ubb38\uac00, \uc11d\ud559\ub4e4\uacfc \ud568\uaed8 \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM, Large Language Model) \uae30\uc220\uc5d0 \ub300\ud55c \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\ub97c \uc9c4\ud589\ud588\ub2e4.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \ud5a5\ud6c4 \ubcf4\uc548 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \ubaa8\ub4e0 \uac1c\ubc1c\uc740 ISO 21434 \ud45c\uc900\uc744 \ub530\ub97c \uc608\uc815\uc774\ub2e4.\n**\ud55c \uce35 \ub192\uc544\uc9c4 \ud655\uc7a5\uc131**\n\ud655\uc7a5\uc131 \uce21\uba74\uc5d0\uc11c \uc0bc\uc131\uc740 \uac01 \uc790\ub3d9\ucc28 \ub3c4\uba54\uc778\uc758 \uc99d\uac00\ud558\ub294 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc694\uad6c \uc0ac\ud56d\uc744 \ucda9\uc871\ud558\uae30 \uc704\ud574 \ub3c5\uc790\uc801\uc73c\ub85c Type-1 \ud558\uc774\ud37c\ubc14\uc774\uc800\ub97c \uac1c\ubc1c\ud588\ub2e4. \uc774 \ud558\uc774\ud37c\ubc14\uc774\uc800\ub294 \uc131\ub2a5 \uc800\ud558 \uc5c6\uc774 \ub2e4\uc591\ud55c OS\ub97c \uac00\uc0c1\ud654 \ud560 \uc218 \uc788\uc73c\uba70 \uc5c5\uacc4 \ud45c\uc900 API\uc778 VirtIO\ub97c \uc9c0\uc6d0\ud55c\ub2e4. \ub610\ud55c \ud2b9\uc815 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc5d0 \uad6d\ud55c\ub418\uc9c0 \uc54a\uace0 \uc11c\ub4dc\ud30c\ud2f0 \ud558\uc774\ud37c\ubc14\uc774\uc800\ub97c \uc9c0\uc6d0\ud55c\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub610\ud55c \uc790\ub3d9\ucc28 \uae30\uc220\uc774 \ub354\uc6b1 \ubc1c\uc804\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub294 \uc0c1\ud669\uc5d0\uc11c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uac01 \uc601\uc5ed\uc758 \uc5ec\ub7ec \uc774\uae30\uc885 OS\ub97c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ud560 \uc218 \uc788\uc5b4 \ud68d\uae30\uc801\uc778 \ubbf8\ub798\ub97c \uc900\ube44\ud560 \uc218 \uc788\ub2e4. \ubc29 \uc0c1\ubb34\ub294 \"\uc0bc\uc131 \uc624\ud1a0 SoC\uc758 \uc911\uc559 \ucef4\ud4e8\ud305 \uae30\ub2a5 \ub355\ubd84\uc5d0 \ud55c\ub54c \uc0c1\uc0c1 \uc18d\uc5d0\ub9cc \uc874\uc7ac\ud588\ub358 \uac83\ub4e4\uc774 \uace7 \ud604\uc2e4\uc774 \ub420 \uac83\"\uc774\ub77c\uba70 \ubbf8\ub798\uc5d0 \ub300\ud55c \ubc1d\uc740 \uc804\ub9dd\uc744 \ub4dc\ub7ec\ub0c8\ub2e4.\n**\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158**\n\ub354\uc6b1 \uac15\ub825\ud55c SoC\uc640 \uc13c\uc11c \uae30\ub2a5\uc774 \uac1c\ubc1c\ub428\uc5d0 \ub530\ub77c \ucd5c\uc2e0 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc810\uc810 \ub354 \uc2a4\ub9c8\ud2b8\ud574\uc9c0\uace0 \ub192\uc740 \uc5f0\uc0b0 \ub2a5\ub825\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uc774 \uac1c\ub150\uc740 \uc13c\uc11c\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\uace0, \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uace0, \ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790\uc5d0\uac8c \ub354 \ub098\uc740 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud558\ub294 \uc138 \ub2e8\uacc4\ub85c \uc138\ubd84\ud654\ud560 \uc218 \uc788\ub2e4.\n\ubaa8\ub4e0 \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub824\uba74 \ub9ce\uc740 \uc5f0\uc0b0\uc774 \ud544\uc694\ud558\ubbc0\ub85c \uc804\ub825 \uc18c\ube44\uac00 \uc99d\uac00\ud55c\ub2e4. \ubc14\ub85c \uc774 \ubd80\ubd84\uc5d0\uc11c \uc0bc\uc131\uc758 \uae30\uc220\uc801 \ud601\uc2e0\uc774 \ube5b\uc744 \ubc1c\ud55c\ub2e4. \uc0bc\uc131\uc740 \uc804\uc6a9 \ub3c4\uba54\uc778\ubcc4 \uc2dc\uc2a4\ud15c\uc744 \ud3ec\ud568\ud558\ub294 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\ub97c \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub294 \ub354 \uac04\ub2e8\ud558\uba74\uc11c \ud2b9\uc218\ud55c \uc5f0\uc0b0 \ubd80\ud558\ub97c \ucc98\ub9ac\ud558\uae30 \ub54c\ubb38\uc5d0 \uae30\ubcf8\uc801\uc73c\ub85c \ud6a8\uc728\uc131\uc774 \ub192\ub2e4. \uc774 \uc2dc\uc2a4\ud15c\uc740 \uc804\ub825 \uc18c\ube44\uac00 \uc801\uace0 \ucc98\ub9ac \uc9c0\uc5f0 \uc2dc\uac04\uc774 \uc9e7\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc790\ub3d9\ucc28 \uc544\ud0a4\ud14d\ucc98\ub97c \uc911\uc559 \ucef4\ud4e8\ud305\uc73c\ub85c \uc804\ud658**\n\ubc29 \uc0c1\ubb34\ub294 \ucc28\ub7c9\uc774 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c \ub3c4\uba54\uc778 \uc911\uc559 \uc9d1\uc911\uc2dd \uc2dc\uc2a4\ud15c\uc73c\ub85c, \uadf8\ub9ac\uace0 \uc774\uc81c\ub294 \ud1b5\ud569 \uc911\uc559 \ucef4\ud4e8\ud305\uc73c\ub85c \ubc1c\uc804\ud574 \uc628 \uacfc\uc815\uc5d0 \ub300\ud574 \uc124\uba85\ud588\ub2e4. \uc774\ub7ec\ud55c \ubcc0\ud654\ub85c \uc778\ud574 \ud6a8\uc728\uc131\uc774 \ub192\uc544\uc9c0\uace0 \uac04\uc18c\ud654\ub418\uc5c8\uc9c0\ub9cc \uace0\ub3c4\ub85c \uc5f0\uacb0\ub41c \uc544\ud0a4\ud14d\ucc98\uc758 \ud2b9\uc131\uc73c\ub85c \uc778\ud574 \uc548\uc804\uacfc \uac19\uc740 \uc601\uc5ed\uc5d0 \ub300\ud55c \uc0c8\ub85c\uc6b4 \uae30\uc220\uc801 \uace0\ub824\uc0ac\ud56d\uc774 \ubc1c\uc0dd\ud588\ub2e4. \uc774\uc5d0 \uc644\ubcbd\ud558\uac8c \ub300\uc751\ud558\uae30 \uc704\ud574 \ucc28\uc138\ub300 \uc0bc\uc131 \uc624\ud1a0 SoC\ub294 \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc9c0\ud5a5\ud558\uba70 \uc548\uc804\uc131, \ubcf4\uc548, \ud655\uc7a5\uc131\uc744 \uac15\ud654\ud55c \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud558\uace0 \uc788\ub2e4.\n_*\uac15\ud654\ub41c \uc548\uc804 \ubc0f \ubcf4\uc548 *_",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "URL: news/40ed73bb-4b81-47ed-b2ea-96181bdaa61b/detail\nCategory: News\nTitle: \uc774\uc81c\ub294 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc\u2019\ub85c! \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \ucd08\uc5f0\uacb0 \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 \uae30\uc220\nContent: <br>\n\uace7\uc774\uc5b4 GPU \uc124\uacc4 \ud611\uc5c5 \uc18c\uac1c\uc640 \ub354\ubd88\uc5b4 \u2018\ucc28\ubcc4\ud654\ub41c GPU\uc640 NPU \uae30\uc220\u2019, \u2018\ubbf8\ub798 \uae30\uc220 \ub3d9\ud5a5\u2019, \u2018\uc774\ubbf8\uc9c0\uc13c\uc11c\uc758 \uc9c4\ud654\uc640 \uae30\uc220\ub825\u2019 \ub4f1\uc758 \uac15\uc5f0\uc774 \uc774\uc5b4\uc84c\ub2e4. \uc5ec\uae30\uc5d0 \u2018\uc0dd\uc131\ud615 AI \ubc0f \ub300\ud615\uc5b8\uc5b4\ubaa8\ub378(LLM)\uc758 \ucd5c\uadfc \ub3d9\ud5a5\uc774 \ucef4\ud4e8\ud305 \ud50c\ub7ab\ud3fc\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\u2019\uc5d0 \ub300\ud55c \ud328\ub110 \ud1a0\ub860\uc744 \ub354\ud574, \uad00\ub828 \uc5c5\uacc4\uc640 \uc774\ud574 \uad00\uacc4\uc790\ub4e4\uc774 \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4\uc5d0 \ub300\ud574 \ud55c\uce35 \ub354 \uc2ec\ub3c4 \uae4a\uac8c \uc774\ud574\ud560 \uc218 \uc788\ub294 \uc790\ub9ac\ub97c \ub9c8\ub828\ud588\ub2e4.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking."
            ]
        ],
        "answer": "The AI Challenger is an open innovation program developed by Samsung System LSI to cultivate AI expertise among students and postgraduates, specifically in Korea, through practical engagement and innovation. The program aims to advance Edge AI technology by equipping participants with essential skills and fostering industry-academia collaboration. It focuses on transforming innovative ideas into tangible prototypes or solutions, offering practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field, fuel the growth of AI technology and industry, and serve as a breeding ground for applicable tech solutions.",
        "ground_truth": "The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK."
    },
    {
        "question": "I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.",
        "contexts": [
            [
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#changing-the-model-in-an-android-sample-application\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: \t\tconst val INPUT_CONVERSION_SCALE = 256F\n\t\tconst val INPUT_CONVERSION_OFFSET = 0F\n\t\tval OUTPUT_DATA_TYPE = DataType.FLOAT32\n\t\tval OUTPUT_DATA_LAYER = LayerType.HWC\n\t\tconst val OUTPUT_SIZE_W = 600\n\t\tconst val OUTPUT_SIZE_H = 400\n\t\tconst val OUTPUT_SIZE_C = INPUT_SIZE_C\n\t\tconst val OUTPUT_CONVERSION_SCALE = 256F\n\t\tconst val OUTPUT_CONVERSION_OFFSET = 0F\n\t}\n\t```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n object ModelConstants {\n const val MODEL_NAME = \\\"float32_pose.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val HEATMAP_DATA_TYPE = DataType.FLOAT32\n\n const val HEATMAP_SIZE_W = 9\n const val HEATMAP_SIZE_H = 9\n const val HEATMAP_SIZE_C = 17\n\n val OFFSET_DATA_TYPE = DataType.FLOAT32\n\n const val OFFSET_SIZE_W = 9\n const val OFFSET_SIZE_H = 9\n const val OFFSET_SIZE_C = 34\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.objectdetection.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"yolov5s-new.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 640\n const val INPUT_SIZE_H = 640\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 25200\n const val OUTPUT_SIZE_H = 85\n\n const val LABEL_FILE = \\\"coco.txt\\\"\n }\n ```\n3. If the new model uses a different set of labels, add the corresponding label text file to the assets directory.\n4. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n```kotlin\npackage com.samsung.depthestimation.data\n\nobject ModelConstants {\n const val MODEL_NAME = \\\"MiDaS_v2.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 256\n const val INPUT_SIZE_H = 256\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 256\n const val OUTPUT_SIZE_H = 256\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n}\n```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.imageclassification.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"inception_v4_quant.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.UINT8\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 299\n const val INPUT_SIZE_H = 299\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 1F\n const val INPUT_CONVERSION_OFFSET = 0F\n\n val OUTPUT_DATA_TYPE = DataType.UINT8\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n\n const val LABEL_FILE = \\\"labels1001.txt\\\"\n }\n ```\n3. If the new model uses a different set of labels, add the corresponding label text file to the assets directory.\n4. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#changing-the-model-in-an-android-sample-application\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: To update the model in your Android project, do the following:\n1. Place the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2. In the `ModelConstants.kt` file, update the specifications to match the new model.\n\t```kotlin\n\tpackage com.samsung.imageenhance.data\n\tobject ModelConstants {\n\t\tconst val MODEL_NAME = \"zero-dce.nnc\"\n\t\tval INPUT_DATA_TYPE = DataType.FLOAT32\n\t\tval INPUT_DATA_LAYER = LayerType.HWC\n\t\tconst val INPUT_SIZE_W = 600\n\t\tconst val INPUT_SIZE_H = 400\n\t\tconst val INPUT_SIZE_C = 3",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.segmentation.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"deeplabv3.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_H = INPUT_SIZE_H\n const val OUTPUT_SIZE_W = INPUT_SIZE_W\n const val OUTPUT_SIZE_C = 21\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## The ENN SDK\nThe ENN SDK tool facilitates the conversion of TensorFlow Lite neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the Samsung Exynos hardware to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of the Sample Native Program\nThe sample native program accepts a model file and raw input data files as parameters.\nIf golden output data files are provided, the program compares these files and displays the result. If no file is provided, the program generates a raw output file.\n### Model and Data Used\nIn this sample native program, a converted NNC file and raw input/output file available in the Github Repository are used.\n### Using Native Program\nTo execute the sample native program, refer to README file.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Golden Data and Comparing it with Output (Optional)\nContent:\nAfter executing the model, load the golden binary data into memory and compare it with the data in the output memory location of model.\nThis optional step validates whether the output of model matches the expected results.\nUse the EnnAllocateAllBuffer function to obtain the output memory location.\n(example):\n```cpp\ntemplate <typename T>\nint compare_data(T *data1, T *data2, int size, T threshold) {\n\tint diff = 0;\n\tfor (int idx = 0; idx < size; idx++) {\n\t\tif (std::abs(data1[idx] - data2[idx]) > threshold) {\n\t\t\tdiff++;\n\t\t}\n\t}\n\treturn diff;\n}\n```",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Converting NN Models with ENN SDK Service\nContent:\nIn this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.\nTo get started, you must be a member of the Exynos Developer Society.\n- If you are not a member of the Exynos Developer Society, sign up from here.\n- If you already have an account, log in to the Exynos Developer Society.\nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from here.\n2. Navigate to the ENN SDK service page and provide a title for your project.\n3. Then, upload the downloaded TFLite model\n4. Next, select hardware preferences.\n\t- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n\t- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n5. After confirming your selections, click **Convert** to convert the model.\n6. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n7. Click **NNC Download** to download the converted NNC model.\n\tYou can now integrate the NNC model into the desired application."
            ]
        ],
        "answer": "In addition to the sample algorithms, other algorithms can be used to develop models. However, there may be restrictions on the areas supported, so it's advisable to check the scope of support available on the portal bulletin board. If you encounter any issues during the application, feel free to reach out for assistance.",
        "ground_truth": "In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board."
    },
    {
        "question": "I would like to see the source code related to the ENN SDK or portal in person.",
        "contexts": [
            [
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Using JNI Library from Kotlin\nContent:\nFollowing is an example for using JNI function in Kotlin.\n1. Load the JNI library\n\t(example):\n\t```kotlin\n\tinit {\n\t\tSystem.loadLibrary(\"enn_jni\")\n\t\t...\n\t}\n\t```\n2. Declare the external function\n\t(example):\n\t```kotlin\n\tprivate external fun ennOpenModel(filename: String): Long\n\t...\n\t```\n3. Use the JNI function (example):\n\t```kotlin\n\t// to open a model\n\tprivate fun setupENN() {\n\t\t...\n\t\tmodelId = ennOpenModel(fileAbsoluteDirectory)\n\t\t...\n\t}\n\t```",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## The ENN SDK\nThe ENN SDK tool facilitates the conversion of TensorFlow Lite neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the Samsung Exynos hardware to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of the Sample Native Program\nThe sample native program accepts a model file and raw input data files as parameters.\nIf golden output data files are provided, the program compares these files and displays the result. If no file is provided, the program generates a raw output file.\n### Model and Data Used\nIn this sample native program, a converted NNC file and raw input/output file available in the Github Repository are used.\n### Using Native Program\nTo execute the sample native program, refer to README file.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Writing JNI Library\nContent:\nFollowing function is an example of an implemented JNI wrapper.<br>\nFor more information, refer to the Android Developer Documentation.\n(example):\n```cpp\nextern \"C\"\nJNIEXPORT jlong JNICALL\nJava_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel(\n\t\tJNIEnv *env,\n\t\tjobject thiz,\n\t\tjstring j_filename\n) {\n\tEnnModelId model_id;\n\tconst char *filename = env->GetStringUTFChars(j_filename, 0);\n\tif (enn::api::EnnOpenModel(filename, &model_id)) {\n\t\t__android_log_print(ANDROID_LOG_ERROR, LOG_TAG, \"EnnOpenModel of [%s] Failed\", filename);\n\t}\n\treturn static_cast<jlong>(model_id);\n}\n```",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Adding C++ to Module\nContent:\n1. Right click **Project** panel with **Android** option being selected.\n\t\n2. Select the **Add C++ to Module** option and click OK.\n\t",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Copying Data to ERD Board\nContent:\nThe following commands copy the necessary files to the ERD board:\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/model.nnc /data/local/tmp/\nadb push example/input.bin /data/local/tmp/\nadb push example/golden.bin /data/local/tmp/\n```\nThese commands perform the following actions:\n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnExecuteModel**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers. |\n| EnnReturn | **EnnExecuteModelAsync**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously. |\n| EnnReturn | **EnnExecuteModelWait**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling EnnExecuteModelAsync() If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions Documentation\nContent:\n#### function EnnExecuteModel\n```cpp\nEnnReturn EnnExecuteModel(\n\tconst EnnModelId model_id,\n\tconst int session_id =0\n)\n```\nRequest to service core to execute model with commited buffers.\n**Parameters**:\n- **model_id** [IN] model ID from load_model\n- **session_id** [IN] session ID\n**Return**: EnnReturn result, 0 is success\n**Note**: this function runs in block mode\n#### function EnnExecuteModelAsync\n```cpp\nEnnReturn EnnExecuteModelAsync(\n\tconst EnnModelId model_id,\n\tconst int session_id =0\n)\n```\nRequest to service core to execute model in background asynchronously.\n**Parameters**:\n- **model_id** [IN] model ID from load_model\n- **session_id** [IN] session ID\n**Return**: EnnReturn result, 0 is success ",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n>adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n>```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Context initialize / deinitialize\nContent:\n### Functions\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnInitialize**(void )<br>Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. |\n| EnnReturn | **EnnDeinitialize**(void )<br>Deinitialize Enn Framework. Framework degenerates context in a caller's process. |\n### Functions Documentation\n#### function EnnInitialize\n```cpp\nEnnReturn EnnInitialize(\n\tvoid\n)\n```\nInitialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.\n**Return**: EnnReturn result, 0 is success\n#### function EnnDeinitialize\n```cpp\nEnnReturn EnnDeinitialize(\n\tvoid\n)\n```\nDeinitialize Enn Framework. Framework degenerates context in a caller's process.\n**Return**: EnnReturn result, 0 is success ",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: API References\\API reference\nContent:\n## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references)\n- **_NumberOfBuffersInfo**\n- **_ennBuffer**\n- **_ennBufferInfo**\n## [ENN Framwork API Functions](api-reference/enn-framework-api-functions)\n- **Context initialize / deinitialize**\n- **OpenModel / CloseModel related**\n- **Memory Handling**\n- **Setters and Getters for model**\n- **Commit Buffer**\n- **Execute Models**\n- **Security, preference, get meta information..** ",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-2\nContent:\n| EnnReturn | **EnnGetPreferencePerfConfigId**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. |\n| EnnReturn | **EnnGetPreferencePerfMode**(uint32_t * val_ptr)<br>Get current information for Performance Mode. |\n| EnnReturn | **EnnGetPreferenceTimeOut**(uint32_t * val_ptr)<br>Get current information for Time Out. |\n| EnnReturn | **EnnGetPreferencePriority**(uint32_t * val_ptr)<br>Get current information for NPU Priority. |\n| EnnReturn | **EnnGetPreferenceCoreAffinity**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. |\n| EnnReturn | **EnnGetMetaInfo**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. |\n| EnnReturn | **EnnSetExecMsgAlwaysOn**()<br>Set frequency of execution message print. |",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: Function Implementation\\Implementing Function\\Preparing NNC Model\nContent:\nUse `copyNNCFromAssetsToInternalStorage` function to copy the NNC model file from the asset directory of app to its internal storage.\nIt is necessary to copy the NNC model file because the model file needs to be accessed from the internal storage when used by the ENN Framework.\n(example):\n```kotlin\nprivate fun copyNNCFromAssetsToInternalStorage(filename: String) {\n\ttry {\n\t\tval inputStream = context.assets.open(filename)\n\t\tval outputFile = File(context.filesDir, filename)\n\t\tval outputStream = FileOutputStream(outputFile)\n\t\tval buffer = ByteArray(2048)\n\t\tvar bytesRead: Int\n\t\twhile (inputStream.read(buffer).also { bytesRead = it } != -1) {\n\t\t\toutputStream.write(buffer, 0, bytesRead)\n\t\t}\n\t\tinputStream.close()\n\t\toutputStream.close()\n\t} catch (e: IOException) {\n\t\te.printStackTrace()\n\t}\n}\n```"
            ]
        ],
        "answer": "We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for the program are provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum, and we will provide support on a case-by-case basis. If you have specific questions about integrating the ENN SDK into your application or utilizing specific functionalities, please feel free to ask for assistance.",
        "ground_truth": "We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis."
    },
    {
        "question": "I would like to know the AI model categories supported by ENN SDK.",
        "contexts": [
            [
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Section: API References\\API reference\nContent:\n## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references)\n- **_NumberOfBuffersInfo**\n- **_ennBuffer**\n- **_ennBufferInfo**\n## [ENN Framwork API Functions](api-reference/enn-framework-api-functions)\n- **Context initialize / deinitialize**\n- **OpenModel / CloseModel related**\n- **Memory Handling**\n- **Setters and Getters for model**\n- **Commit Buffer**\n- **Execute Models**\n- **Security, preference, get meta information..** ",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Required Parameters\nContent:\nTo execute NNC models using the ENN framework, the following parameters are required:\n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector<string>|List of input file paths|\n|`goldens` (optional)|vector<string>|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:"
            ]
        ],
        "answer": "The ENN SDK supports a wide range of AI model categories, including:\n- Image Classification\n- Object Detection\n- Segmentation\n- Pose Estimation\n- Image Enhancement\n- Depth Estimation\n- Recommendation Systems\n- Anomaly Detection\n- Computer Vision for Healthcare\n- Augmented and Virtual Reality\n\nThese categories demonstrate the versatility of the ENN SDK in accommodating various AI applications and use cases. If you have specific questions about any of these categories or their implementation using the ENN SDK, feel free to ask!",
        "ground_truth": "The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Image Classification.",
        "contexts": [
            [
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application"
            ]
        ],
        "answer": "The ENN SDK supports various operators for developing Image Classification models. These include the following operators: CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION. These operators can be used to develop efficient and optimized Image Classification models using the ENN SDK.",
        "ground_truth": "For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Object Detection.",
        "contexts": [
            [
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions."
            ]
        ],
        "answer": "The Object Detection category supported operators include CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION. If you have any more questions or need further assistance, feel free to ask!",
        "ground_truth": "The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing segmentation",
        "contexts": [
            [
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Section: Quick Start Guide\\Quick Start Guide\\Basic Workflow\nContent:\nFollowing figure illustrates the three steps for converting and executing an NN model:\n```mermaid\nflowchart LR\n\tsubgraph \"ENN SDK Service\"\n\t\tdirection LR\n\t\tconvert(\"Convert The Model\")\n\tend\n\tsubgraph \"ENN Framework\"\n\t\tdirection LR\n\t\texecute(\"Execute The Model\")\n\tend\n\tmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```\n### Model Conversion\nTo convert TensorFlow Lite models, ENN SDK provides an online conversion tool through the Samsung Exynos Developer Society.\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.\nFor more information on the process of converting NN models, refer to Converting NN Models with ENN SDK Service.\n### Model Execution\nTo execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe ENN SDK provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.\nFor more information on the process of executing NN models, refer to Executing Models Using Native Program and Executing Models Using Android Application.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Required Parameters\nContent:\nTo execute NNC models using the ENN framework, the following parameters are required:\n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector<string>|List of input file paths|\n|`goldens` (optional)|vector<string>|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model."
            ]
        ],
        "answer": "In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
        "ground_truth": "In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.",
        "contexts": [
            [
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnExecuteModel**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers. |\n| EnnReturn | **EnnExecuteModelAsync**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously. |\n| EnnReturn | **EnnExecuteModelWait**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling EnnExecuteModelAsync() If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished. |",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n object ModelConstants {\n const val MODEL_NAME = \\\"float32_pose.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val HEATMAP_DATA_TYPE = DataType.FLOAT32\n\n const val HEATMAP_SIZE_W = 9\n const val HEATMAP_SIZE_H = 9\n const val HEATMAP_SIZE_C = 17\n\n val OFFSET_DATA_TYPE = DataType.FLOAT32\n\n const val OFFSET_SIZE_W = 9\n const val OFFSET_SIZE_H = 9\n const val OFFSET_SIZE_C = 34\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions Documentation\nContent:\n#### function EnnExecuteModel\n```cpp\nEnnReturn EnnExecuteModel(\n\tconst EnnModelId model_id,\n\tconst int session_id =0\n)\n```\nRequest to service core to execute model with commited buffers.\n**Parameters**:\n- **model_id** [IN] model ID from load_model\n- **session_id** [IN] session ID\n**Return**: EnnReturn result, 0 is success\n**Note**: this function runs in block mode\n#### function EnnExecuteModelAsync\n```cpp\nEnnReturn EnnExecuteModelAsync(\n\tconst EnnModelId model_id,\n\tconst int session_id =0\n)\n```\nRequest to service core to execute model in background asynchronously.\n**Parameters**:\n- **model_id** [IN] model ID from load_model\n- **session_id** [IN] session ID\n**Return**: EnnReturn result, 0 is success ",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Setters and Getters for model\\Functions-1\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnGetBuffersInfo**(NumberOfBuffersInfo * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model. |\n| EnnReturn | **EnnGetBufferInfoByIndex**(EnnBufferInfo * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model. |\n| EnnReturn | **EnnGetBufferInfoByLabel**(EnnBufferInfo * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model. |",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model."
            ]
        ],
        "answer": "When developing Pose Estimation with the ENN SDK, the supported operators include CONVOLUTION, RELU, and CONCATENATION. These operators enable the implementation of pose estimation models efficiently on Samsung Exynos hardware. If you need further details or assistance, feel free to ask!",
        "ground_truth": "For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION."
    },
    {
        "question": "Please tell us which operators are supported by ENN SDK when developing Image Enhancement.",
        "contexts": [
            [
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Channel Alignment\nMaintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources.\n| Architecture | Channel Alignment |\n| -- | -- |\n| Gen-4 | 32 |\n#### Bilinear Resize Parameters\nTo ensure optimal image resizing using bilinear interpolation, configure the following settings:\n- **Option A**:\n\t- Aligned corner: `False`\n\t- Half pixel centers: `False`\n\t- Performance: High speed\n- **Option B**:\n\t- Aligned corner: `True`\n\t- Half pixel centers: `False`\n\t- Compatibility: Gen-4 and later NPUs\n\t- Performance: Medium speed\n- **Option C**:\n\t- Aligned corner: `False`\n\t- Half pixel centers: `True`\n\t- Note: Requires workaround\n\t- Performance: Reduced speed",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnExecuteModel**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers. |\n| EnnReturn | **EnnExecuteModelAsync**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously. |\n| EnnReturn | **EnnExecuteModelWait**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling EnnExecuteModelAsync() If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished. |",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-1\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnSetPreferencePresetId**(const uint32_t val)<br>Setting Preset ID for operation performance. |\n| EnnReturn | **EnnSetPreferencePerfConfigId**(const uint32_t val)<br>Setting PerfConfig ID for operation performance. |\n| EnnReturn | **EnnSetPreferencePerfMode**(const uint32_t val)<br>Setting Performance Mode. |\n| EnnReturn | **EnnSetPreferenceTimeOut**(const uint32_t val)<br>Setting Preset ID for time out. |\n| EnnReturn | **EnnSetPreferencePriority**(const uint32_t val)<br>Setting priority value for NPU. |\n| EnnReturn | **EnnSetPreferenceCoreAffinity**(const uint32_t val)<br>Setting affinity to set NPU core operation. |\n| EnnReturn | **EnnGetPreferencePresetId**(uint32_t * val_ptr)<br>Get current information for Preset ID. |",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples."
            ]
        ],
        "answer": "When developing Image Enhancement using ENN SDK, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
        "ground_truth": "The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.\n"
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.",
        "contexts": [
            [
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnExecuteModel**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers. |\n| EnnReturn | **EnnExecuteModelAsync**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously. |\n| EnnReturn | **EnnExecuteModelWait**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling EnnExecuteModelAsync() If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished. |"
            ]
        ],
        "answer": "In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
        "ground_truth": "In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Recommendation System.",
        "contexts": [
            [
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions-2\nContent:\n| EnnReturn | **EnnGetPreferencePerfConfigId**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. |\n| EnnReturn | **EnnGetPreferencePerfMode**(uint32_t * val_ptr)<br>Get current information for Performance Mode. |\n| EnnReturn | **EnnGetPreferenceTimeOut**(uint32_t * val_ptr)<br>Get current information for Time Out. |\n| EnnReturn | **EnnGetPreferencePriority**(uint32_t * val_ptr)<br>Get current information for NPU Priority. |\n| EnnReturn | **EnnGetPreferenceCoreAffinity**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. |\n| EnnReturn | **EnnGetMetaInfo**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. |\n| EnnReturn | **EnnSetExecMsgAlwaysOn**()<br>Set frequency of execution message print. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnExecuteModel**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers. |\n| EnnReturn | **EnnExecuteModelAsync**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously. |\n| EnnReturn | **EnnExecuteModelWait**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling EnnExecuteModelAsync() If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished. |",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: Quick Start Guide\\Quick Start Guide\\Basic Workflow\nContent:\nFollowing figure illustrates the three steps for converting and executing an NN model:\n```mermaid\nflowchart LR\n\tsubgraph \"ENN SDK Service\"\n\t\tdirection LR\n\t\tconvert(\"Convert The Model\")\n\tend\n\tsubgraph \"ENN Framework\"\n\t\tdirection LR\n\t\texecute(\"Execute The Model\")\n\tend\n\tmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```\n### Model Conversion\nTo convert TensorFlow Lite models, ENN SDK provides an online conversion tool through the Samsung Exynos Developer Society.\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.\nFor more information on the process of converting NN models, refer to Converting NN Models with ENN SDK Service.\n### Model Execution\nTo execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe ENN SDK provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.\nFor more information on the process of executing NN models, refer to Executing Models Using Native Program and Executing Models Using Android Application.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Required Parameters\nContent:\nTo execute NNC models using the ENN framework, the following parameters are required:\n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector<string>|List of input file paths|\n|`goldens` (optional)|vector<string>|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\nExecuting NN models on the ENN framework comprises of three steps such as initializing the framework, inferring the models, and deinitializing the framework.\nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.\n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\n\tA1[Initialize]\n\tA1 --> A2[Open Model]\n\tA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\n\tB1[Copy Input Layer]\n\tB1 --> B2[Execute Model]\n\tB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\n\tC1[Release Buffers]\n\tC1 --> C2[Close Model]\n\tC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```\nTo infer multiple data, repeat `Inference`."
            ]
        ],
        "answer": "For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
        "ground_truth": "For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection",
        "contexts": [
            [
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Required Parameters\nContent:\nTo execute NNC models using the ENN framework, the following parameters are required:\n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector<string>|List of input file paths|\n|`goldens` (optional)|vector<string>|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: API References\\API reference\nContent:\n## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references)\n- **_NumberOfBuffersInfo**\n- **_ennBuffer**\n- **_ennBufferInfo**\n## [ENN Framwork API Functions](api-reference/enn-framework-api-functions)\n- **Context initialize / deinitialize**\n- **OpenModel / CloseModel related**\n- **Memory Handling**\n- **Setters and Getters for model**\n- **Commit Buffer**\n- **Execute Models**\n- **Security, preference, get meta information..** ",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\\Preparing Framework and Model\nContent:\nBefore executing ML models on the ENN framework, initialize the framework, load the model, and allocate the required buffers.\n1. EnnInitialize:\n\tThis function initializes the ENN Framework.\n\t(example):\n\t```cpp\n\tenn::api::EnnInitialize();\n\t```\n2. EnnOpenModel:\n\tThis function opens the specified model and returns a model ID.\n\t(example):\n\t```cpp\n\tEnnModelId model_id;\n\tenn::api::EnnOpenModel(model_name.c_str(), &model_id);\n\t```\n3. EnnAllocateAllBuffers:\n\tThis function allocates the required buffers for the model.\n\tIt also provides the number of input/output buffers (`buffer_info`), their locations, and sizes (`buffer_set`).\n\t(example):\n\t```cpp\n\tEnnBufferPtr *buffer_set;\n\tNumberOfBuffersInfo buffer_info;\n\tenn::api::EnnAllocateAllBuffers(model_id, &buffer_set, &buffer_info);\n\t```\nFollowing is the data structure of `EnnBufferPtr` and `NumberOfBuffersInfo`.\n```cpp\ntypedef struct _ennBuffer {\n\tvoid *va;\n\tuint32_t size;\n\tuint32_t offset;\n} EnnBuffer;\ntypedef EnnBuffer* EnnBufferPtr;\ntypedef struct _NumberOfBuffersInfo {\n\tuint32_t n_in_buf;\n\tuint32_t n_out_buf;\n} NumberOfBuffersInfo;\n```",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnExecuteModel**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers. |\n| EnnReturn | **EnnExecuteModelAsync**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously. |\n| EnnReturn | **EnnExecuteModelWait**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling EnnExecuteModelAsync() If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished. |"
            ]
        ],
        "answer": "The ENN SDK supports the following operators for Anomaly Detection:\n- DENSE\n- RELU\n- SOFTMAX\n- SUB\n- MUL",
        "ground_truth": "In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.",
        "contexts": [
            [
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```"
            ]
        ],
        "answer": "For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
        "ground_truth": "For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.",
        "contexts": [
            [
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```"
            ]
        ],
        "answer": "In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
        "ground_truth": "In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE."
    },
    {
        "question": "I would like to know examples of applications that can be created using Image Classification.",
        "contexts": [
            [
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: Setting Necessary UI\\Setting the UI Components\\Required UI Components\nContent:\nFollowing are the UI components that are required or recommended for implementation:\n### Image Input\n- **ImageView**: Displays the image that must be classified.\n- **Image Load Button**: Triggers the loading of an image.\n- **Inference Button**: Initiates the image classification process.\n- **TextView**: Displays the classification result and score.\n### Camera Feed Input\n- **PreviewView**: Displays a live camera feed for real-time classification.\n- **TextView**: Displays the classification result and score.\nThe implemented UI resembles the figure as illustrated below.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\\A.4 Fragment Classes (`fragments` package)-2\nContent:\n- `ImageFragment.kt`: A fragment class responsible for the image mode UI.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `onCreateView` | Called when the fragment is created. |\n\t| `onViewCreated` | Called immediately after `onCreateView` to set up UI components. |\n\t| `getContent` | Loads the image for processing. |\n\t| `setUI` | Initializes UI components. |\n\t| `process` | Invokes the `process` method from the `ModelExecutor` class. |\n\t| `processImage` | Adjusts the image to match the input size of the model. |\n\t| `updateUI` | Updates the UI after model execution is completed. |\n- `SelectFragment.kt`: A fragment class responsible for the mode selection UI.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Function Implementation\\Implementing Function\\Listener Interface\\Usage\nContent:\nCreate the `ModelExecutor` object in the `fragment` class with the current context and the fragment as `executorListener`. The `process` method of `modelExecutor` is called to start the image classification.\n(example):\n```kotlin\nmodelExecutor = ModelExecutor(\n\tcontext = requireContext(), executorListener = this\n)\n...\nmodelExecutor.process(bitmapBuffer)\n```\nIn the `ModelExecutor.kt` class, the `process` method processes the image and calls the `onResults` method of `executorListener` to pass the results back to the fragment.\n(example):\n```kotlin\nfun process(image: Bitmap) {\n\t...\n\texecutorListener?.onResults(\n\t\tresult, inferenceTime\n\t)\n}\n```",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\\A.4 Fragment Classes (`fragments` package)-1\nContent:\n- `CameraFragment.kt`: A fragment class responsible for the camera mode UI.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `onCreateView` | Called when the fragment is created. |\n\t| `onViewCreated` | Called immediately after `onCreateView` to set up UI components. |\n\t| `setCamera` | Sets up the camera for preview and model input. |\n\t| `setPreview` | Establishes the camera preview. |\n\t| `setImageAnalyzer` | Configures the image analyzer for the camera feed. |\n\t| `process` | Invokes the `process` method from the `ModelExecutor` class. |\n\t| `processImage` | Adjusts the image to match the input size of the model. |\n\t| `setUI` | Initializes UI components. |\n\t| `updateUI` | Updates the UI after model execution is completed. |",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required."
            ]
        ],
        "answer": "AI Applications that can be created using Image Classification include:\n1. Face recognition\n2. Plant disease diagnosis\n3. Product identification",
        "ground_truth": "AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification."
    },
    {
        "question": "I would like to know examples of applications that can be created using Object Detection.",
        "contexts": [
            [
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program."
            ]
        ],
        "answer": "AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, and Wildlife tracking.",
        "ground_truth": "AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking."
    },
    {
        "question": "I would like to know examples of applications that can be created using Segmentation.",
        "contexts": [
            [
                "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Implementing Sample Application-2\nContent:\n```mermaid\ngraph TB\nA[Start]\nsubgraph B[Starting Android Project]\nB1[Install Android Studio] --> B2[Create a New Android Project]\nB2 --> B3[Connect the ERD Board to Android Studio]\nend\nsubgraph C[Setting Necessary UI]\nC1[Set up Android UI View Components]\nend\nsubgraph D[Function Implementation]\nD1[Implement UI Functions]\nD1 --> D2[Implement Listener]\nD2 --> D3[Implement Data Processing Functions]\nend\nsubgraph E[ENNTools]\nE1[Use ENNTools to Convert the TFLite Model to the NNC Model]\nend\nsubgraph F[ENN Framework]\nF1[Implement the ENN Framework in the Sample Application]\nend\nG[End]\nA --> B\nB --> C\nC --> D\nD --> E\nE --> F\nF --> G\n```",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Implementing Sample Application-1\nContent:\nImplementing sample application involves the following five steps:\n1. Starting Android project: Covers the basics of starting an Android project, from installing Android studio to connecting the ERD board to Android studio.\n2. Setting necessary UI: Provides essential information on Android UI view components.\n3. Function implementation: Explains the functions available in the sample application.\n4. ENN SDK service: Provides the step-by-step process for converting the TFLite model to NNC model.\n5. ENN framework: Explains the implementation of the ENN framework in the sample application.\nThe general workflow of writing and executing an Android application using the ENN SDK is discribed in the following flowchart.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\nContent:\nThis section describes the steps for executing NN models on the ENN framework.\nFor more information on the ENN framework, refer to the documentation.",
                "Section: Writing Native Program\\Writing Native Program\\Introduction\\Program Flow\nContent:\nThis program receives the locations of the model file, input file, and output file as parameters.\nInitially, the program initializes the ENN SDK framework.\nSubsequently, it opens the model and allocates the required buffers.\nThen, it loads the input files into the specified buffer locations.\nAfter this, the model is executed.\nPost-execution, the program loads and compares the output with the output files that are provided.\nIf the output files are not available, it dumps the buffer data into a file.\nBefore the program terminates, it releases all the buffers, closes the model, and de-initializes the ENN SDK framework.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\nContent:\nThis section provides an overview of Android (Kotlin) sample applications.\nEach sample application entry provides the details of the functionality of the sample application, its location, and instructions for running it.\nFor more information on implementing the sample applications, refer to Getting Started With Android Samples guide.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\nExecuting NN models on the ENN framework comprises of three steps such as initializing the framework, inferring the models, and deinitializing the framework.\nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.\n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\n\tA1[Initialize]\n\tA1 --> A2[Open Model]\n\tA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\n\tB1[Copy Input Layer]\n\tB1 --> B2[Execute Model]\n\tB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\n\tC1[Release Buffers]\n\tC1 --> C2[Close Model]\n\tC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```\nTo infer multiple data, repeat `Inference`.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Section: Quick Start Guide\\Quick Start Guide\\Basic Workflow\nContent:\nFollowing figure illustrates the three steps for converting and executing an NN model:\n```mermaid\nflowchart LR\n\tsubgraph \"ENN SDK Service\"\n\t\tdirection LR\n\t\tconvert(\"Convert The Model\")\n\tend\n\tsubgraph \"ENN Framework\"\n\t\tdirection LR\n\t\texecute(\"Execute The Model\")\n\tend\n\tmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```\n### Model Conversion\nTo convert TensorFlow Lite models, ENN SDK provides an online conversion tool through the Samsung Exynos Developer Society.\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.\nFor more information on the process of converting NN models, refer to Converting NN Models with ENN SDK Service.\n### Model Execution\nTo execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe ENN SDK provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.\nFor more information on the process of executing NN models, refer to Executing Models Using Native Program and Executing Models Using Android Application.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\\A.4 Fragment Classes (`fragments` package)-2\nContent:\n- `ImageFragment.kt`: A fragment class responsible for the image mode UI.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `onCreateView` | Called when the fragment is created. |\n\t| `onViewCreated` | Called immediately after `onCreateView` to set up UI components. |\n\t| `getContent` | Loads the image for processing. |\n\t| `setUI` | Initializes UI components. |\n\t| `process` | Invokes the `process` method from the `ModelExecutor` class. |\n\t| `processImage` | Adjusts the image to match the input size of the model. |\n\t| `updateUI` | Updates the UI after model execution is completed. |\n- `SelectFragment.kt`: A fragment class responsible for the mode selection UI.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: Starting Android Project\\Starting the Android Project\\Prerequisite\nContent:\nBefore you proceed, ensure you have a basic understanding of the following Android development concepts:\n- **Kotlin**: The primary programming language for Android development.\n- **View Binding**: A feature that allows you to easily write code that interacts with views.\n- **Fragments**: A reusable piece of an user interface or behavior of Android application.\n- **Navigation Component**: A component that helps to implement navigation.\nIf you are new to Android, it is recommended to review Chapters 1 to 3 of the Android Basics in Kotlin course.\nThis course describes the basics of developing Android application using Kotlin.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Converting NN Models with ENN SDK Service\nContent:\nIn this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.\nTo get started, you must be a member of the Exynos Developer Society.\n- If you are not a member of the Exynos Developer Society, sign up from here.\n- If you already have an account, log in to the Exynos Developer Society.\nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from here.\n2. Navigate to the ENN SDK service page and provide a title for your project.\n3. Then, upload the downloaded TFLite model\n4. Next, select hardware preferences.\n\t- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n\t- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n5. After confirming your selections, click **Convert** to convert the model.\n6. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n7. Click **NNC Download** to download the converted NNC model.\n\tYou can now integrate the NNC model into the desired application.",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Section: Using Adb to Execute\\Using Adb to Execute\nContent:\nThis section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board.",
                "Section: Changing the Model\\Changing the Model in an Android Sample Application\nContent:\nThis guide provides a step-by-step process to replace the model in an Android sample application.",
                "Section: Setting Necessary UI\\Setting the UI Components\\Programming UI Components\nContent:\nThis section demonstrates sample code for UI components used in the sample application.\nEach component has XML code and Kotlin code pair.\nNote that the code snippets provided in this guide primarily focus on the essential functional elements.\nAs a result, implementing additional aspects, such as the positioning of views, is mandatory."
            ]
        ],
        "answer": "AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
        "ground_truth": "AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring."
    },
    {
        "question": "I would like to know examples of applications that can be created using Pose Estimation.",
        "contexts": [
            [
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n object ModelConstants {\n const val MODEL_NAME = \\\"float32_pose.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val HEATMAP_DATA_TYPE = DataType.FLOAT32\n\n const val HEATMAP_SIZE_W = 9\n const val HEATMAP_SIZE_H = 9\n const val HEATMAP_SIZE_C = 17\n\n val OFFSET_DATA_TYPE = DataType.FLOAT32\n\n const val OFFSET_SIZE_W = 9\n const val OFFSET_SIZE_H = 9\n const val OFFSET_SIZE_C = 34\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement."
            ]
        ],
        "answer": "AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
        "ground_truth": "AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming."
    },
    {
        "question": "I would like to know examples of applications that can be created using Image Enhancement.",
        "contexts": [
            [
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\\A.4 Fragment Classes (`fragments` package)-2\nContent:\n- `ImageFragment.kt`: A fragment class responsible for the image mode UI.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `onCreateView` | Called when the fragment is created. |\n\t| `onViewCreated` | Called immediately after `onCreateView` to set up UI components. |\n\t| `getContent` | Loads the image for processing. |\n\t| `setUI` | Initializes UI components. |\n\t| `process` | Invokes the `process` method from the `ModelExecutor` class. |\n\t| `processImage` | Adjusts the image to match the input size of the model. |\n\t| `updateUI` | Updates the UI after model execution is completed. |\n- `SelectFragment.kt`: A fragment class responsible for the mode selection UI.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Section: Function Implementation\\Implementing Function\\Processing Data\\Converting Input Data to Bitmap\\Image Data\nContent:\nThe `ActivityResult` object `getContent` retrieves an image from the device media.\nThe selected image is displayed in an ImageView and converted to a bitmap.\n(example):\n```kotlin\nprivate val getContent =\n\tregisterForActivityResult(ActivityResultContracts.GetContent()) { uri: Uri? ->\n\t\turi?.let {\n\t\t\tbinding.imageView.setImageURI(it)\n\t\t\tbinding.buttonProcess.isEnabled = true\n\t\t\tbitmapBuffer = ImageDecoder.decodeBitmap(\n\t\t\t\tImageDecoder.createSource(\n\t\t\t\t\trequireContext().contentResolver,\n\t\t\t\t\tit\n\t\t\t\t)\n\t\t\t) { decoder, _, _ ->\n\t\t\t\tdecoder.setTargetColorSpace(ColorSpace.get(ColorSpace.Named.SRGB))\n\t\t\t\tdecoder.allocator = ImageDecoder.ALLOCATOR_SOFTWARE\n\t\t\t\tdecoder.setTargetSampleSize(1)\n\t\t\t}\n\t\t}\n\t}\n```\nClick **Load** to launch the `ActivityResult`.\nIt allows the user to select an image from their device.\n(example):\n```kotlin\nbinding.buttonLoad.setOnClickListener {\n\tgetContent.launch(\"image/*\")\n}\n```",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Implementing Sample Application-1\nContent:\nImplementing sample application involves the following five steps:\n1. Starting Android project: Covers the basics of starting an Android project, from installing Android studio to connecting the ERD board to Android studio.\n2. Setting necessary UI: Provides essential information on Android UI view components.\n3. Function implementation: Explains the functions available in the sample application.\n4. ENN SDK service: Provides the step-by-step process for converting the TFLite model to NNC model.\n5. ENN framework: Explains the implementation of the ENN framework in the sample application.\nThe general workflow of writing and executing an Android application using the ENN SDK is discribed in the following flowchart.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: Function Implementation\\Implementing Function\\Processing Data\\Converting Input Data to Bitmap\\Process Image\nContent:\nClick **Process** to crop and process the image.\n(example):\n```kotlin\nprivate fun processImage(image: Bitmap): Bitmap {\n\tval rotatedCroppedImage = createCroppedBitmap(image)\n\treturn Bitmap.createScaledBitmap(\n\t\trotatedCroppedImage, INPUT_SIZE_W, INPUT_SIZE_H, true\n\t)\n}\nprivate fun createCroppedBitmap(image: Bitmap): Bitmap {\n\tval matrix = Matrix().apply { postRotate(90F) }\n\tval cropDim = calculateCropDimensions(image)\n\treturn Bitmap.createBitmap(\n\t\timage, cropDim[0], cropDim[1], cropDim[2], cropDim[2], matrix, true\n\t)\n}\n```",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Section: Setting Necessary UI\\Setting the UI Components\\Required UI Components\nContent:\nFollowing are the UI components that are required or recommended for implementation:\n### Image Input\n- **ImageView**: Displays the image that must be classified.\n- **Image Load Button**: Triggers the loading of an image.\n- **Inference Button**: Initiates the image classification process.\n- **TextView**: Displays the classification result and score.\n### Camera Feed Input\n- **PreviewView**: Displays a live camera feed for real-time classification.\n- **TextView**: Displays the classification result and score.\nThe implemented UI resembles the figure as illustrated below.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |"
            ]
        ],
        "answer": "AI Applications that can be created using Image Enhancement include:\n1. Restoration of old photos\n2. Clarity improvement in medical imaging\n3. Enhancement of security camera images\n\nAdditionally, you can find a sample application for Image Enhancement in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository, which demonstrates the execution of a converted Zero-DCE model using the ENN framework. This model is converted using ENN SDK service with the **Default** hardware type option.",
        "ground_truth": "AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image."
    },
    {
        "question": "I would like to know examples of applications that can be created using Depth Estimation.",
        "contexts": [
            [
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Implementing Sample Application-1\nContent:\nImplementing sample application involves the following five steps:\n1. Starting Android project: Covers the basics of starting an Android project, from installing Android studio to connecting the ERD board to Android studio.\n2. Setting necessary UI: Provides essential information on Android UI view components.\n3. Function implementation: Explains the functions available in the sample application.\n4. ENN SDK service: Provides the step-by-step process for converting the TFLite model to NNC model.\n5. ENN framework: Explains the implementation of the ENN framework in the sample application.\nThe general workflow of writing and executing an Android application using the ENN SDK is discribed in the following flowchart.",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-2\nContent:\n| DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| AVGPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\n| MAXPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\nFor `AVGPOOL` and `MAXPOOL`, when the kernel size is larger than the supported size, it is split into multiple pooling layers as the origin.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Implementing Sample Application-2\nContent:\n```mermaid\ngraph TB\nA[Start]\nsubgraph B[Starting Android Project]\nB1[Install Android Studio] --> B2[Create a New Android Project]\nB2 --> B3[Connect the ERD Board to Android Studio]\nend\nsubgraph C[Setting Necessary UI]\nC1[Set up Android UI View Components]\nend\nsubgraph D[Function Implementation]\nD1[Implement UI Functions]\nD1 --> D2[Implement Listener]\nD2 --> D3[Implement Data Processing Functions]\nend\nsubgraph E[ENNTools]\nE1[Use ENNTools to Convert the TFLite Model to the NNC Model]\nend\nsubgraph F[ENN Framework]\nF1[Implement the ENN Framework in the Sample Application]\nend\nG[End]\nA --> B\nB --> C\nC --> D\nD --> E\nE --> F\nF --> G\n```",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |"
            ]
        ],
        "answer": "Applications that can be created using Depth Estimation include 3D modeling, Augmented reality, and robot navigation.",
        "ground_truth": "AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation."
    },
    {
        "question": "I would like to know examples of applications that can be created using Recommendation System.",
        "contexts": [
            [
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Question: How do I post an article in the Best Lab?\nAnswer: Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, up to six projects are featured, receiving special attention. There is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Setting Necessary UI\\Setting the UI Components\\Required UI Components\nContent:\nFollowing are the UI components that are required or recommended for implementation:\n### Image Input\n- **ImageView**: Displays the image that must be classified.\n- **Image Load Button**: Triggers the loading of an image.\n- **Inference Button**: Initiates the image classification process.\n- **TextView**: Displays the classification result and score.\n### Camera Feed Input\n- **PreviewView**: Displays a live camera feed for real-time classification.\n- **TextView**: Displays the classification result and score.\nThe implemented UI resembles the figure as illustrated below.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Implementing Sample Application-1\nContent:\nImplementing sample application involves the following five steps:\n1. Starting Android project: Covers the basics of starting an Android project, from installing Android studio to connecting the ERD board to Android studio.\n2. Setting necessary UI: Provides essential information on Android UI view components.\n3. Function implementation: Explains the functions available in the sample application.\n4. ENN SDK service: Provides the step-by-step process for converting the TFLite model to NNC model.\n5. ENN framework: Explains the implementation of the ENN framework in the sample application.\nThe general workflow of writing and executing an Android application using the ENN SDK is discribed in the following flowchart.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\nExecuting NN models on the ENN framework comprises of three steps such as initializing the framework, inferring the models, and deinitializing the framework.\nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.\n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\n\tA1[Initialize]\n\tA1 --> A2[Open Model]\n\tA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\n\tB1[Copy Input Layer]\n\tB1 --> B2[Execute Model]\n\tB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\n\tC1[Release Buffers]\n\tC1 --> C2[Close Model]\n\tC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```\nTo infer multiple data, repeat `Inference`.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\nContent:\nThis section describes the steps for executing NN models on the ENN framework.\nFor more information on the ENN framework, refer to the documentation."
            ]
        ],
        "answer": "AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, and Customized news feed.",
        "ground_truth": "AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed."
    },
    {
        "question": "I would like to know examples of applications that can be created using Anomaly Detection",
        "contexts": [
            [
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Golden Data and Comparing it with Output (Optional)\nContent:\nAfter executing the model, load the golden binary data into memory and compare it with the data in the output memory location of model.\nThis optional step validates whether the output of model matches the expected results.\nUse the EnnAllocateAllBuffer function to obtain the output memory location.\n(example):\n```cpp\ntemplate <typename T>\nint compare_data(T *data1, T *data2, int size, T threshold) {\n\tint diff = 0;\n\tfor (int idx = 0; idx < size; idx++) {\n\t\tif (std::abs(data1[idx] - data2[idx]) > threshold) {\n\t\t\tdiff++;\n\t\t}\n\t}\n\treturn diff;\n}\n```",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\\Preparing Framework and Model\nContent:\nBefore executing ML models on the ENN framework, initialize the framework, load the model, and allocate the required buffers.\n1. EnnInitialize:\n\tThis function initializes the ENN Framework.\n\t(example):\n\t```cpp\n\tenn::api::EnnInitialize();\n\t```\n2. EnnOpenModel:\n\tThis function opens the specified model and returns a model ID.\n\t(example):\n\t```cpp\n\tEnnModelId model_id;\n\tenn::api::EnnOpenModel(model_name.c_str(), &model_id);\n\t```\n3. EnnAllocateAllBuffers:\n\tThis function allocates the required buffers for the model.\n\tIt also provides the number of input/output buffers (`buffer_info`), their locations, and sizes (`buffer_set`).\n\t(example):\n\t```cpp\n\tEnnBufferPtr *buffer_set;\n\tNumberOfBuffersInfo buffer_info;\n\tenn::api::EnnAllocateAllBuffers(model_id, &buffer_set, &buffer_info);\n\t```\nFollowing is the data structure of `EnnBufferPtr` and `NumberOfBuffersInfo`.\n```cpp\ntypedef struct _ennBuffer {\n\tvoid *va;\n\tuint32_t size;\n\tuint32_t offset;\n} EnnBuffer;\ntypedef EnnBuffer* EnnBufferPtr;\ntypedef struct _NumberOfBuffersInfo {\n\tuint32_t n_in_buf;\n\tuint32_t n_out_buf;\n} NumberOfBuffersInfo;\n```",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Required Parameters\nContent:\nTo execute NNC models using the ENN framework, the following parameters are required:\n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector<string>|List of input file paths|\n|`goldens` (optional)|vector<string>|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\nExecuting NN models on the ENN framework comprises of three steps such as initializing the framework, inferring the models, and deinitializing the framework.\nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.\n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\n\tA1[Initialize]\n\tA1 --> A2[Open Model]\n\tA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\n\tB1[Copy Input Layer]\n\tB1 --> B2[Execute Model]\n\tB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\n\tC1[Release Buffers]\n\tC1 --> C2[Close Model]\n\tC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```\nTo infer multiple data, repeat `Inference`.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment."
            ]
        ],
        "answer": "AI Applications that can be created using Anomaly Detection include:\n\n1. Credit card fraud detection\n2. Network security\n3. Predictive maintenance in manufacturing\n\nIf you need further assistance on developing such applications using the ENN SDK or any other related information, feel free to ask!",
        "ground_truth": "AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing."
    },
    {
        "question": "I would like to know examples of applications that can be created using Computer Vision for Healthcare.",
        "contexts": [
            [
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Section: Function Implementation\\Implementing Function\\Processing Data\\Converting Input Data to Bitmap\\Image Data\nContent:\nThe `ActivityResult` object `getContent` retrieves an image from the device media.\nThe selected image is displayed in an ImageView and converted to a bitmap.\n(example):\n```kotlin\nprivate val getContent =\n\tregisterForActivityResult(ActivityResultContracts.GetContent()) { uri: Uri? ->\n\t\turi?.let {\n\t\t\tbinding.imageView.setImageURI(it)\n\t\t\tbinding.buttonProcess.isEnabled = true\n\t\t\tbitmapBuffer = ImageDecoder.decodeBitmap(\n\t\t\t\tImageDecoder.createSource(\n\t\t\t\t\trequireContext().contentResolver,\n\t\t\t\t\tit\n\t\t\t\t)\n\t\t\t) { decoder, _, _ ->\n\t\t\t\tdecoder.setTargetColorSpace(ColorSpace.get(ColorSpace.Named.SRGB))\n\t\t\t\tdecoder.allocator = ImageDecoder.ALLOCATOR_SOFTWARE\n\t\t\t\tdecoder.setTargetSampleSize(1)\n\t\t\t}\n\t\t}\n\t}\n```\nClick **Load** to launch the `ActivityResult`.\nIt allows the user to select an image from their device.\n(example):\n```kotlin\nbinding.buttonLoad.setOnClickListener {\n\tgetContent.launch(\"image/*\")\n}\n```",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Section: Function Implementation\\Implementing Function\\Processing Data\\Converting Input Data to Bitmap\\Camera Data\nContent:\nThe `setImageAnalyzer` function sets an `ImageAnalysis` object to process the camera feed.\nIt creates a bitmap buffer and processes each image frame.\n(example):\n```kotlin\nprivate fun setImageAnalyzer() {\n\timageAnalyzer =\n\tImageAnalysis.Builder()\n\t\t.setTargetRotation(binding.viewFinder.display.rotation)\n\t\t.setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)\n\t\t.setOutputImageFormat(ImageAnalysis.OUTPUT_IMAGE_FORMAT_RGBA_8888)\n\t\t.build().also {\n\t\t\tit.setAnalyzer(cameraExecutor) { image ->\n\t\t\t\tif (!::bitmapBuffer.isInitialized) {\n\t\t\t\t\tbitmapBuffer = Bitmap.createBitmap(\n\t\t\t\t\t\timage.width, image.height, Bitmap.Config.ARGB_8888\n\t\t\t\t\t)\n\t\t\t\t}\n\t\t\t\tprocess(image)\n\t\t\t}\n\t\t}\n}\n```\nThe `setCamera` function introduced here is updated to include the `setImageAnalyzer` method.\nThis inclusion allows the camera feed to be analyzed and processed.\n(example):\n```kotlin\ncameraProviderFuture.addListener(\n\t{\n\t\t...\n\t\tsetPreview()\n\t\tsetImageAnalyzer()\n\t\ttry {\n\t\t\tcameraProvider.unbindAll()\n\t\t\tcamera = cameraProvider.bindToLifecycle(\n\t\t\t\tthis, cameraSelector, preview, imageAnalyzer\n\t\t\t)\n\t\t\tpreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n\t\t}\n\t\t...\n\t},\n\t...\n)\n```",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Setting Necessary UI\\Setting the UI Components\\Required UI Components\nContent:\nFollowing are the UI components that are required or recommended for implementation:\n### Image Input\n- **ImageView**: Displays the image that must be classified.\n- **Image Load Button**: Triggers the loading of an image.\n- **Inference Button**: Initiates the image classification process.\n- **TextView**: Displays the classification result and score.\n### Camera Feed Input\n- **PreviewView**: Displays a live camera feed for real-time classification.\n- **TextView**: Displays the classification result and score.\nThe implemented UI resembles the figure as illustrated below.",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Golden Data and Comparing it with Output (Optional)\nContent:\nAfter executing the model, load the golden binary data into memory and compare it with the data in the output memory location of model.\nThis optional step validates whether the output of model matches the expected results.\nUse the EnnAllocateAllBuffer function to obtain the output memory location.\n(example):\n```cpp\ntemplate <typename T>\nint compare_data(T *data1, T *data2, int size, T threshold) {\n\tint diff = 0;\n\tfor (int idx = 0; idx < size; idx++) {\n\t\tif (std::abs(data1[idx] - data2[idx]) > threshold) {\n\t\t\tdiff++;\n\t\t}\n\t}\n\treturn diff;\n}\n```",
                "Section: Setting Necessary UI\\Setting the UI Components\\Programming UI Components\\Camera Preview-1\nContent:\nThe PreviewView displays a live camera feed for real-time classification.\nIn the layout XML file, define a PreviewView with a unique ID.\nIn the Kotlin file, use this ID to reference the PreviewView and set up the camera preview.\nThis process involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of PreviewView.\n- Layout `app/res/layout/*.xml` (example):\n\t```xml\n\t<androidx.camera.view.PreviewView\n\t\tandroid:id=\"@+id/view_finder\" />\n\t```\n- Kotlin `app/java/package/fragments/Fragment.kt` (example):\n\t```kotlin\n\tprivate fun setCamera() {\n\t\tcameraExecutor = Executors.newSingleThreadExecutor()\n\t\tval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions."
            ]
        ],
        "answer": "AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, and Surgical assistance.",
        "ground_truth": "AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance."
    },
    {
        "question": "I would like to know examples of applications that can be created using Augmented and Virtual Reality.",
        "contexts": [
            [
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\nContent:\nThis section provides an overview of Android (Kotlin) sample applications.\nEach sample application entry provides the details of the functionality of the sample application, its location, and instructions for running it.\nFor more information on implementing the sample applications, refer to Getting Started With Android Samples guide.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: Starting Android Project\\Starting the Android Project\\Prerequisite\nContent:\nBefore you proceed, ensure you have a basic understanding of the following Android development concepts:\n- **Kotlin**: The primary programming language for Android development.\n- **View Binding**: A feature that allows you to easily write code that interacts with views.\n- **Fragments**: A reusable piece of an user interface or behavior of Android application.\n- **Navigation Component**: A component that helps to implement navigation.\nIf you are new to Android, it is recommended to review Chapters 1 to 3 of the Android Basics in Kotlin course.\nThis course describes the basics of developing Android application using Kotlin.",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\\A.4 Fragment Classes (`fragments` package)-1\nContent:\n- `CameraFragment.kt`: A fragment class responsible for the camera mode UI.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `onCreateView` | Called when the fragment is created. |\n\t| `onViewCreated` | Called immediately after `onCreateView` to set up UI components. |\n\t| `setCamera` | Sets up the camera for preview and model input. |\n\t| `setPreview` | Establishes the camera preview. |\n\t| `setImageAnalyzer` | Configures the image analyzer for the camera feed. |\n\t| `process` | Invokes the `process` method from the `ModelExecutor` class. |\n\t| `processImage` | Adjusts the image to match the input size of the model. |\n\t| `setUI` | Initializes UI components. |\n\t| `updateUI` | Updates the UI after model execution is completed. |",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: Setting Necessary UI\\Setting the UI Components\\Programming UI Components\\Camera Preview-1\nContent:\nThe PreviewView displays a live camera feed for real-time classification.\nIn the layout XML file, define a PreviewView with a unique ID.\nIn the Kotlin file, use this ID to reference the PreviewView and set up the camera preview.\nThis process involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of PreviewView.\n- Layout `app/res/layout/*.xml` (example):\n\t```xml\n\t<androidx.camera.view.PreviewView\n\t\tandroid:id=\"@+id/view_finder\" />\n\t```\n- Kotlin `app/java/package/fragments/Fragment.kt` (example):\n\t```kotlin\n\tprivate fun setCamera() {\n\t\tcameraExecutor = Executors.newSingleThreadExecutor()\n\t\tval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Using JNI Library from Kotlin\nContent:\nFollowing is an example for using JNI function in Kotlin.\n1. Load the JNI library\n\t(example):\n\t```kotlin\n\tinit {\n\t\tSystem.loadLibrary(\"enn_jni\")\n\t\t...\n\t}\n\t```\n2. Declare the external function\n\t(example):\n\t```kotlin\n\tprivate external fun ennOpenModel(filename: String): Long\n\t...\n\t```\n3. Use the JNI function (example):\n\t```kotlin\n\t// to open a model\n\tprivate fun setupENN() {\n\t\t...\n\t\tmodelId = ennOpenModel(fileAbsoluteDirectory)\n\t\t...\n\t}\n\t```"
            ]
        ],
        "answer": "AI Applications that can be created using augmented reality and virtual reality include:\n\n1. Virtual training environments\n2. Immersive gaming\n3. Interactive educational contents",
        "ground_truth": "AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents."
    },
    {
        "question": "Please tell us about the AI Challenger.",
        "contexts": [
            [
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _* *_\n**\u25a1 5\uc77c (\ud604\uc9c0\uc2dc\uac04), \uc751\uc6a9\ucc98\ubcc4 \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \ud604\ud669\u00b7\ube44\uc804 \ubc1c\ud45c**\n - \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\uc220 \uac1c\ubc1c \uad00\ub828 \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\n\n**\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d, \uc0dd\uc131\ud615 AI \uae30\uc220 \ud0d1\uc7ac\ud55c '\uc5d1\uc2dc\ub178\uc2a4 2400' \uacf5\uac1c**\n - \uc804\uc791 \ub300\ube44 CPU \uc131\ub2a5 1.7\ubc30, AI \uc131\ub2a5 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\n - \uc5d1\uc2dc\ub178\uc2a4 2400 \uae30\ubc18 \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \uac8c\uc784 \ud504\ub9ac\ubdf0 \uc601\uc0c1 \uacf5\uac1c\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n - \uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4 \uacf5\uac1c\u2026\ucd5c\ub300 4\ubc30 \uc90c\uc5d0\ub3c4 \ucd08\uace0\ud654\uc9c8, AI \ud2b8\ub798\ud0b9 \uae30\uc220 \uc801\uc6a9\n - \ucc28\uc138\ub300 \ucc28\ub7c9\uc6a9 \uc194\ub8e8\uc158, \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c(NTN) \uc601\uc0c1 \uc2dc\uc5f0",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc694\uac74\uc744 \ucda9\uc871\ud558\uace0 \uc778\uac04\uc758 \ud589\ub3d9\uc744 \uc644\ubcbd\ud558\uac8c \ubaa8\ubc29\ud55c\ub2e4\ub294 \uad81\uadf9\uc801\uc778 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 \uc0bc\uc131\uc740 \uc800\uc9c0\uc5f0\uacfc \uc800\uc804\ub825\uc744 \ub450 \uac00\uc9c0 \ud575\uc2ec \ubc1c\uc804 \ubd84\uc57c\ub85c \uc120\uc815\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uac1c\ubc1c\ud300 \ud64d\uae30\uc900 \uc0c1\ubb34\ub294 \u201c\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\u201d\uc774\ub77c\ub294 \uc81c\ubaa9\uc758 \uc138\uc158\uc5d0\uc11c \uc774 \ub450 \uac00\uc9c0 \ud575\uc2ec \ubd84\uc57c\ub97c \uc5b4\ub5bb\uac8c \ubc1c\uc804\uc2dc\ucf1c\uc654\ub294\uc9c0 \uc124\uba85\ud588\ub2e4.\n**\uc0c1\ud669 \uc778\uc9c0 \ucef4\ud4e8\ud305**\n\ub2e4\uc591\ud55c \uae30\ub2a5\uc774 \ud0d1\uc7ac\ub41c \ucd5c\uc2e0 \ub514\ubc14\uc774\uc2a4 \uc911 \uc0c1\ub2f9\uc218\ub294 \uc0c1\ud669 \uc778\uc9c0 \ucef4\ud4e8\ud3053\uc73c\ub85c \uad6c\ud604\ub418\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud55c\ub2e4. \uc0c1\ud669 \uc778\uc9c0\ub294 \uc801\uc678\uc120 \uc13c\uc2f1\uc5d0\uc11c \uc2dc\uc791\ub418\uc5c8\uc9c0\ub9cc, \ucd5c\uc2e0 \uc13c\uc11c\ub294 \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 \uc0c8\ub85c\uc6b4 \uac10\uac01 \ub370\uc774\ud130\uc758 \uc138\uacc4\ub97c \uc5f4\uc5b4 \uc8fc\uc5c8",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790 \uc2dc\uc2a4\ud15cLSI\uc0ac\uc5c5\ubd80 \ubc15\uc6a9\uc778 \uc0ac\uc7a5\uc740 \"\ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\uace0 \ucc98\ub9ac\ud558\ub294 '\uc0dd\uc131\ud615 AI'\uac00 \uc62c\ud574 \uac00\uc7a5 \uc911\uc694\ud55c \uae30\uc220 \ud2b8\ub79c\ub4dc\ub85c \uc790\ub9ac \uc7a1\uc73c\uba70 \ub354 \uace0\ub3c4\ud654\ub41c \uae30\ubc18 \uae30\uc220 \ud655\ubcf4\uc758 \ud544\uc694\uc131\uc774 \ub300\ub450\ub418\uace0 \uc788\ub2e4\" \uba70 \"\uc0bc\uc131\uc804\uc790\ub294 \uace0\uc131\ub2a5 IP\ubd80\ud130 \uc7a5\ub2e8\uac70\ub9ac \ud1b5\uc2e0 \uc194\ub8e8\uc158, \uc778\uac04\uc758 \uc624\uac10\uc744 \ubaa8\ubc29\ud55c \uc13c\uc11c \uae30\ubc18 '\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc(System LSI Humanoid)'\ub97c \uad6c\ud604\ud574\ub098\uac00\uba70 \uc0dd\uc131\ud615 AI\uc5d0\uc11c \ub354 \ubc1c\uc804\ub41c '\uc120\ud589\uc801 AI(Proactive AI)' \uc2dc\ub300\ub97c \uc5f4 \uac83\uc774\ub2e4\"\uace0 \ubc1d\ud614\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uba40\ud2f0\ubbf8\ub514\uc5b4\uc758 \uae30\uc220\uc758 \ubc1c\uc804\uc73c\ub85c \ud734\uba38\ub178\uc774\ub4dc\uac00 \uc778\uac04\ucc98\ub7fc \uc790\uc5f0\uc2a4\ub7fd\uac8c \uac10\uc9c0\ud558\uace0, \uc778\uc9c0\ud558\uace0, \ud589\ub3d9\ud558\ub294 \ubbf8\ub798\uc5d0 \ub354 \uac00\uae4c\uc6cc\uc9c0\uace0 \uc788\ub2e4. \ud64d \uc0c1\ubb34\ub294 \"\uc774\ub7ec\ud55c \ubbf8\ub798\ub97c \uc2e4\ud604\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc544\uc9c1 \ud604\uc2e4\uacfc\uc758 \uae30\uc220\uc801\uc778 \uaca9\ucc28\ub97c \uc881\ud600\uc57c \ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac \uc2dc\uc2a4\ud15c \uae30\uc220\ub85c \ubab0\uc785\uac10 \ub118\uce58\uace0 \uc778\ud130\ub799\ud2f0\ube0c\ud55c \ubbf8\ub798\ub97c \uad6c\ud604\ud560 \uac83\u201d\uc774\ub77c\uace0 \ud655\uc2e0\ud588\ub2e4.\n\n1 MobileBERT: \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc758 \ubaa8\ubc14\uc77c \ubc84\uc804\n2 AP: Application Processor\n3 \ucef4\ud4e8\ud305\uc5d0\uc11c \uc0c1\ud669 \uc778\uc9c0\ub294 \uc2dc\uc2a4\ud15c/\uad6c\uc131 \uc694\uc18c\uac00 \ud658\uacbd\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc218\uc9d1\ud558\uace0 \uadf8\uc5d0 \ub530\ub77c \ub3d9\uc791\uc744 \uc870\uc815\ud558\ub294 \uae30\ub2a5\uc744 \ub9d0\ud568\n ",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc0dd\uc131\ud615 \uc778\uacf5\uc9c0\ub2a5(AI) \ubc0f \uac01\uc885 \ucca8\ub2e8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ub4f1\uc7a5\uc73c\ub85c \uc778\ud574 \uc2dc\uc7a5\uc5d0\uc11c\ub294 \ud6a8\uc728\uc131, \ud655\uc7a5\uc131, \uc800\uc804\ub825\uc744 \uac16\ucd98 \ub354\uc6b1 \uc2a4\ub9c8\ud2b8\ud558\uace0 \uac15\ub825\ud55c \ucef4\ud4e8\ud305\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4. \uc774\uc5d0 \ub530\ub77c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\uc758 \uc8fc\uc694 \ud14c\ub9c8 \uc911 \ud558\ub098\uc778 \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\ub97c \uc8fc\uc81c\ub85c \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220, \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158, \uc0bc\uc131 \uc624\ud1a0 SoC(System-on-Chips) \ub4f1 \uc138 \uac1c\uc758 \uac15\uc5f0\uc774 \uc9c4\ud589\ub418\uc5c8\ub2e4.\n**\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220**\n\uccab\ubc88\uc9f8 \uc138\uc158\uc778 \"\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220\"\uc740 \uc0bc\uc131 \uc624\uc2a4\ud2f4 \uc5f0\uad6c\uc13c\ud130 \ubca0\ub2c8 \uce74\ud2f0\ube44\uc548(Benny Katibian) \ubd80\uc0ac\uc7a5\uc774 \uc9c4\ud589\ud588\ub2e4",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\ub2a5\uc744 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub3c4\uc785\ud558\uae30 \uc704\ud55c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 IP\uac1c\ubc1c \ubc29\ud5a5\uc131\uc744 \uc18c\uac1c\ud588\ub2e4.\n**\ucd5c\uc801\ud654\ub41c 4\ud074\ub7ec\uc2a4\ud130 \uad6c\uc870\uc758 CPU**\n\uc138 \uac1c\uc758 \uc8fc\uc694 IP \uc911 \uccab \ubc88\uc9f8\ub294 CPU\ub85c, \uae30\uc874\uc5d0\ub294 \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud558\uc774\uc5d4\ub4dc(high-end) \ud074\ub7ec\uc2a4\ud130, \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ub85c\uc6b0\uc5d4\ub4dc(low-end) \ud074\ub7ec\uc2a4\ud130, \uc911\uac04 \ud074\ub7ec\uc2a4\ud130 \ub4f1 \uc138 \uac1c\uc758 \ud074\ub7ec\uc2a4\ud130\ub85c \uad6c\uc131\ub418\uc5c8\ub2e4. \uc0bc\uc131\uc740 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 CPU \uc6b4\uc601\uc744 \uc704\ud574 \uc911\uac04 \ud074\ub7ec\uc2a4\ud130\ub97c \ubbf8\ub4dc \ud558\uc774(mid-high)\uc640 \ubbf8\ub4dc \ub85c\uc6b0(mid-low)\uc758 \ub450 \uac00\uc9c0 \uacc4\uce35\uc73c\ub85c \ub098\ub204\uace0, \ubbf8\ub4dc \ud558\uc774 \uacc4\uce35\uc740 \uac8c\uc784\uacfc \uac19\uc740 \ucef4\ud4e8\ud305 \uc9d1\uc57d\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud588",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc911\uc559 \ucef4\ud4e8\ud305\uc5d0 \ub300\ud55c \uc218\uc694\uac00 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ud558\ub4dc\uc6e8\uc5b4\uc640 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \ud655\uc7a5\uc131\uc774 \ub354\uc6b1 \uc911\uc694\ud574\uc9c8 \uac83\uc774\ub2e4. \ub530\ub77c\uc11c \uc0bc\uc131\uc758 \ud575\uc2ec \uc194\ub8e8\uc158 \uc911 \ud558\ub098\ub294 \uc18c\ud504\ud2b8\uc6e8\uc5b4\ub97c \uc218\uc815\ud558\uc9c0 \uc54a\uace0\ub3c4 \ucef4\ud4e8\ud305 \uc6a9\ub7c9\uc744 \ub450 \ubc30\ub85c \ub298\ub9ac\uae30 \uc704\ud574 \ub450 SoC \uac04\uc758 \uc9c1\uc811 \uc5f0\uacb0(die-to-die)\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. \ub610\ud55c \uc624\ud1a0 SoC\ub294 OEM\uc758 \uc2dc\uc2a4\ud15c\uc5d0 \ub530\ub77c \ud328\ud0a4\uc9c0 \uac04 \uba40\ud2f0 SoC \uc5f0\uacb0\uc744 \uc704\ud574 PCIe \ub610\ub294 \uc774\ub354\ub137 \uc0ac\uc6a9\uc744 \uc9c0\uc6d0\ud55c\ub2e4.\n_*AI \ud1b5\ud569, \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc815\uc758 \ucc28\ub7c9\uc758 \ubbf8\ub798\ub97c \uc704\ud55c \uc900\ube44 *_\n\uc55e\uc73c\ub85c \uc6b4\uc804\uc790\ub294 \ucd5c\ub300 150\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uc2e4\uc2dc\uac04\uc73c\ub85c \uc791\ub3d9\ud560 \uc218 \uc788\ub294 \uc0bc\uc131\uc758 \uc804\uc6a9 AI \uac00\uc18d\uae30\ub97c \ud1b5\ud574 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0 \uae30\ubc18\ud55c AI \ube44\uc11c\uc640 \uc6d0\ud65c\ud558\uac8c \uc0c1\ud638 \uc791\uc6a9\ud560 \uc218 \uc788\uac8c \ub420 \uac83\uc774\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uacfc\uc911\ud55c \uac8c\uc784 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc804\ub825\uacfc \uc131\ub2a5\uc744 \ub354\uc6b1 \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 CPU \uc804\ub825 \ubd80\ubd84\uc744 \uc904\uc774\uace0 \ub354 \ub9ce\uc740 \uc804\ub825\uc744 CPU \ucf54\uc5b4\uc5d0\uc11c GPU\ub85c \uc7ac\ud560\ub2f9\ud558\uc5ec \ubaa8\ubc14\uc77c \uc7a5\uce58\uc5d0\uc11c \ucd5c\uace0 \uc218\uc900\uc758 \uac8c\uc784 \uadf8\ub798\ud53d\uc744 \uc9c0\uc6d0\ud560 \uc218 \uc788\ub3c4\ub85d \ucef4\ud4e8\ud305 \uc131\ub2a5\uc744 \ub192\uc600\ub2e4.\n**\ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub418\ub294 \ud48d\ubd80\ud55c \uadf8\ub798\ud53d \uacbd\ud5d8**\n\uc0bc\uc131\uc740 \ud504\ub9ac\ubbf8\uc5c4\ubd80\ud130 \uc800\uac00\ud615\uc5d0 \uc774\ub974\ub294 \ub113\uc740 \ubc94\uc704\uc758 \ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0 \ucf58\uc194 \uc218\uc900\uc758 \uac8c\uc784\uc744 \uc81c\uacf5\ud55c\ub2e4\ub294 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 AMD\uc758 RDNATM \uc544\ud0a4\ud14d\ucc98\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 Xclipse GPU\ub97c \uac1c\ubc1c\ud574\uc654\uace0, \uc774\ub97c \ud1b5\ud574 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(ray tracing) \uae30\ub2a5\uc744 \ucd5c\ucd08\ub85c \ub3c4\uc785\ud588\ub2e4",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\uac00 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \ubbf8\uc8fc\ucd1d\uad04(Device Solutions America office)\uc5d0\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023'\uc744 \uac1c\ucd5c\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \ucd08\uc9c0\ub2a5\ud654(Hyper-Intelligence)\u00b7\ucd08\uc5f0\uacb0\uc131(Hyper-Connected)\u00b7\ucd08\ub370\uc774\ud130(Hyper-Data)\ub97c \uac00\ub2a5\ud558\uac8c \ud560 \uc8fc\uc694 \uc751\uc6a9\ucc98\ubcc4 \ucd5c\uc2e0 \ubc18\ub3c4\uccb4 \uc124\uacc4 \ud604\ud669\uacfc \ube44\uc804\uc744 \uacf5\uc720\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \uace0\uac1d\uc0ac\uc640 \ud30c\ud2b8\ub108\uc0ac \uad00\uacc4\uc790 300\uc5ec\uba85\uc774 \ucc38\uc11d\ud55c \uac00\uc6b4\ub370, \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \ubd84\uc57c \uae00\ub85c\ubc8c \uc804\ubb38\uac00, \uc11d\ud559\ub4e4\uacfc \ud568\uaed8 \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM, Large Language Model) \uae30\uc220\uc5d0 \ub300\ud55c \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\ub97c \uc9c4\ud589\ud588\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc544\ud0a4\ud14d\ucc98 \ubcc0\uacbd\uc758 \uacb0\uacfc\ub85c MobileBERT1 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc740 \uc774\uc804 \uc138\ub300\uc5d0 \ube44\ud574 3\ubc30 \ud5a5\uc0c1\ub418\uc5c8\ub2e4\n**\ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc704\ud55c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC**\n\uc790\ub3d9\ucc28\ub294 \ud604\uc7ac \uc804\ud654\uac00 \uc2a4\ub9c8\ud2b8\ud3f0\uc73c\ub85c \uc9c4\ud654\ud55c \uac83\uacfc \uc720\uc0ac\ud558\uac8c \uae09\uaca9\ud558\uac8c \ubcc0\ud654\ud558\uace0 \uc788\ub2e4. \uc2e4\uc81c\ub85c \ud604\ub300\uc758 \uc790\ub3d9\ucc28\ub294 \ub2e8\uc21c\ud55c \uc774\ub3d9 \uc218\ub2e8\uc744 \ub118\uc5b4 \uc0dd\uc131\ud615 AI\uc640 \uac19\uc740 \uad11\ubc94\uc704\ud55c \ucef4\ud4e8\ud305 \uae30\ub2a5\uc758 \uc9d1\ud569\uccb4\uac00 \ub418\uc5c8\ub2e4.\n\uc774\ub7ec\ud55c \ud604\uc2e4\uc744 \ubc30\uacbd\uc73c\ub85c \uc0bc\uc131\uc804\uc790 AP2 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uac1c\ubc1c\ud300 \ubc29\uc9c0\ud6c8 \uc0c1\ubb34\ub294 \uc0bc\uc131 \uc624\ud1a0 SoC \uc194\ub8e8\uc158\uacfc \ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ub300\ud55c \uc138\uc158\uc744 \uc9c4\ud589\ud588\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc5d1\uc2dc\ub178\uc2a4 \ub808\ud37c\ub7f0\uc2a4 \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub41c \ubaa8\ubc14\uc77c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5 \ub370\ubaa8 \uc601\uc0c1\uc744 \ud1b5\ud574 \uadf8\ub9bc\uc790, \ubc18\uc0ac \ub4f1\uacfc \uac19\uc740 \uc5ec\ub7ec \uac1c\uc758 \uc138\ubd80 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5\ub4e4\uc774 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.\n**\uc0dd\uc131\ud615 AI \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 NPU**\n\uc0bc\uc131\uc758 \ucd5c\uc2e0 NPU \uc194\ub8e8\uc158\uc740 \uba54\ubaa8\ub9ac \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcc0\uacbd\ud558\uace0 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \ube44\uc120\ud615 \uc5f0\uc0b0\uc758 \ud65c\uc6a9\ub3c4\ub97c \ud06c\uac8c \ub192\uc77c \uac83\uc774\ub2e4",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38 \ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc2dc\uc2a4\ud15cLSI \uc0ac\uc5c5\ubd80 \ubc15\uc6a9\uc778 \uc0ac\uc7a5\uc774 \ubc1c\ud45c\ub97c \ud558\uace0 \uc788\ub294 \ubaa8\uc2b5**\n_* *_\n_* *_\n\n**\u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38\ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \ubc1c\ud45c\ub97c \ub4e3\ub294 \uccad\uc911\ub4e4\uc758 \ubaa8\uc2b5**\n_* *_\n_* *_\n\n**\u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38 \ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019 \ud604\uc7a5",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-4\nContent:\n| 43 | MAXPOOL | MAX_POOL_2D | O | O | O | O |\n| 44 | MEAN | MEAN | | O | O | O |\n| 45 | MINIMUM | MINIMUM | | O | O | O |\n| 46 | MIRROR_PAD | MIRROR_PAD | O | O | O | O |\n| 47 | MUL | MUL | | | O | O |\n| 48 | NEG | NEG | O | O | O | O |\n| 49 | PACK | PACK | | | O | O |\n| 50 | PAD | PAD | O | O | O | O |\n| 51 | PADV2 | PADV2 | O | | O | O |\n| 52 | PERMUTE | TRANSPOSE | O | | O | O |\n| 53 | POW | POW | | | O | O |\n| 54 | PRELU | PRELU | O | O | O | O |\n| 55 | QUANTIZE | QUANTIZE | O | O | O | O |\n| 56 | REDUCE_MAX | REDUCE_MAX | O | O | O | O |\n| 57 | REDUCE_MEAN | REDUCE_MEAN | O | O | O | O |\n| 58 | REDUCE_MIN | REDUCE_MIN | O | O | O | O |\n| 59 | REDUCE_SUM | SUM | O | | O | O |",
                "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub610\ud55c \uc790\ub3d9\ucc28 \uae30\uc220\uc774 \ub354\uc6b1 \ubc1c\uc804\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub294 \uc0c1\ud669\uc5d0\uc11c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uac01 \uc601\uc5ed\uc758 \uc5ec\ub7ec \uc774\uae30\uc885 OS\ub97c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ud560 \uc218 \uc788\uc5b4 \ud68d\uae30\uc801\uc778 \ubbf8\ub798\ub97c \uc900\ube44\ud560 \uc218 \uc788\ub2e4. \ubc29 \uc0c1\ubb34\ub294 \"\uc0bc\uc131 \uc624\ud1a0 SoC\uc758 \uc911\uc559 \ucef4\ud4e8\ud305 \uae30\ub2a5 \ub355\ubd84\uc5d0 \ud55c\ub54c \uc0c1\uc0c1 \uc18d\uc5d0\ub9cc \uc874\uc7ac\ud588\ub358 \uac83\ub4e4\uc774 \uace7 \ud604\uc2e4\uc774 \ub420 \uac83\"\uc774\ub77c\uba70 \ubbf8\ub798\uc5d0 \ub300\ud55c \ubc1d\uc740 \uc804\ub9dd\uc744 \ub4dc\ub7ec\ub0c8\ub2e4.\n**\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158**\n\ub354\uc6b1 \uac15\ub825\ud55c SoC\uc640 \uc13c\uc11c \uae30\ub2a5\uc774 \uac1c\ubc1c\ub428\uc5d0 \ub530\ub77c \ucd5c\uc2e0 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc810\uc810 \ub354 \uc2a4\ub9c8\ud2b8\ud574\uc9c0\uace0 \ub192\uc740 \uc5f0\uc0b0 \ub2a5\ub825\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uc774 \uac1c\ub150\uc740 \uc13c\uc11c\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\uace0, \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uace0, \ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790\uc5d0\uac8c \ub354 \ub098\uc740 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud558\ub294 \uc138 \ub2e8\uacc4\ub85c \uc138\ubd84\ud654\ud560 \uc218 \uc788\ub2e4.\n\ubaa8\ub4e0 \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub824\uba74 \ub9ce\uc740 \uc5f0\uc0b0\uc774 \ud544\uc694\ud558\ubbc0\ub85c \uc804\ub825 \uc18c\ube44\uac00 \uc99d\uac00\ud55c\ub2e4. \ubc14\ub85c \uc774 \ubd80\ubd84\uc5d0\uc11c \uc0bc\uc131\uc758 \uae30\uc220\uc801 \ud601\uc2e0\uc774 \ube5b\uc744 \ubc1c\ud55c\ub2e4. \uc0bc\uc131\uc740 \uc804\uc6a9 \ub3c4\uba54\uc778\ubcc4 \uc2dc\uc2a4\ud15c\uc744 \ud3ec\ud568\ud558\ub294 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\ub97c \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub294 \ub354 \uac04\ub2e8\ud558\uba74\uc11c \ud2b9\uc218\ud55c \uc5f0\uc0b0 \ubd80\ud558\ub97c \ucc98\ub9ac\ud558\uae30 \ub54c\ubb38\uc5d0 \uae30\ubcf8\uc801\uc73c\ub85c \ud6a8\uc728\uc131\uc774 \ub192\ub2e4. \uc774 \uc2dc\uc2a4\ud15c\uc740 \uc804\ub825 \uc18c\ube44\uac00 \uc801\uace0 \ucc98\ub9ac \uc9c0\uc5f0 \uc2dc\uac04\uc774 \uc9e7\ub2e4."
            ]
        ],
        "answer": "The AI Challenger is an open innovation developer program initiated by Samsung System LSI, designed to foster AI innovation and nurture a vibrant community of AI developers. At the core of the program is the Exynos ENN SDK, an advanced toolkit tailored to facilitate AI model creation and optimization. Through the AI Challenger, Samsung System LSI showcases its commitment to advancing AI technology and nurturing a community of impactful AI developers.\n\nThe AI Challenger unfolds through three primary stages. In Stage I, participants familiarize themselves with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on active research and development, providing technical support and coaching to participants. Stage III is dedicated to finalizing projects, conducting thorough testing, and presenting outcomes.\n\nAdditionally, the 2nd AI Challenger introduces advancements to the ENN SDK, including customizable model optimization algorithms, voice model support for speech and voice recognition, generative AI model support, and future updates introducing retraining and device-specific operational analysis tools.\n\nThe program serves as a platform to hone AI expertise among participants, fostering industry-academia collaboration, and transforming innovative ideas into tangible prototypes or solutions. Ultimately, the AI Challenger aims to enhance participants' AI competitiveness, fuel the growth of AI technology, and serve as a breeding ground for applicable tech solutions.",
        "ground_truth": "Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress."
    },
    {
        "question": "I would like to know the AI Challenger schedule.",
        "contexts": [
            [
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc0dd\uc131\ud615 \uc778\uacf5\uc9c0\ub2a5(AI) \ubc0f \uac01\uc885 \ucca8\ub2e8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ub4f1\uc7a5\uc73c\ub85c \uc778\ud574 \uc2dc\uc7a5\uc5d0\uc11c\ub294 \ud6a8\uc728\uc131, \ud655\uc7a5\uc131, \uc800\uc804\ub825\uc744 \uac16\ucd98 \ub354\uc6b1 \uc2a4\ub9c8\ud2b8\ud558\uace0 \uac15\ub825\ud55c \ucef4\ud4e8\ud305\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4. \uc774\uc5d0 \ub530\ub77c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\uc758 \uc8fc\uc694 \ud14c\ub9c8 \uc911 \ud558\ub098\uc778 \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\ub97c \uc8fc\uc81c\ub85c \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220, \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158, \uc0bc\uc131 \uc624\ud1a0 SoC(System-on-Chips) \ub4f1 \uc138 \uac1c\uc758 \uac15\uc5f0\uc774 \uc9c4\ud589\ub418\uc5c8\ub2e4.\n**\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220**\n\uccab\ubc88\uc9f8 \uc138\uc158\uc778 \"\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220\"\uc740 \uc0bc\uc131 \uc624\uc2a4\ud2f4 \uc5f0\uad6c\uc13c\ud130 \ubca0\ub2c8 \uce74\ud2f0\ube44\uc548(Benny Katibian) \ubd80\uc0ac\uc7a5\uc774 \uc9c4\ud589\ud588\ub2e4",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _* *_\n**\u25a1 5\uc77c (\ud604\uc9c0\uc2dc\uac04), \uc751\uc6a9\ucc98\ubcc4 \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \ud604\ud669\u00b7\ube44\uc804 \ubc1c\ud45c**\n - \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\uc220 \uac1c\ubc1c \uad00\ub828 \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\n\n**\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d, \uc0dd\uc131\ud615 AI \uae30\uc220 \ud0d1\uc7ac\ud55c '\uc5d1\uc2dc\ub178\uc2a4 2400' \uacf5\uac1c**\n - \uc804\uc791 \ub300\ube44 CPU \uc131\ub2a5 1.7\ubc30, AI \uc131\ub2a5 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\n - \uc5d1\uc2dc\ub178\uc2a4 2400 \uae30\ubc18 \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \uac8c\uc784 \ud504\ub9ac\ubdf0 \uc601\uc0c1 \uacf5\uac1c\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n - \uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4 \uacf5\uac1c\u2026\ucd5c\ub300 4\ubc30 \uc90c\uc5d0\ub3c4 \ucd08\uace0\ud654\uc9c8, AI \ud2b8\ub798\ud0b9 \uae30\uc220 \uc801\uc6a9\n - \ucc28\uc138\ub300 \ucc28\ub7c9\uc6a9 \uc194\ub8e8\uc158, \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c(NTN) \uc601\uc0c1 \uc2dc\uc5f0",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uacfc\uc911\ud55c \uac8c\uc784 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc804\ub825\uacfc \uc131\ub2a5\uc744 \ub354\uc6b1 \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 CPU \uc804\ub825 \ubd80\ubd84\uc744 \uc904\uc774\uace0 \ub354 \ub9ce\uc740 \uc804\ub825\uc744 CPU \ucf54\uc5b4\uc5d0\uc11c GPU\ub85c \uc7ac\ud560\ub2f9\ud558\uc5ec \ubaa8\ubc14\uc77c \uc7a5\uce58\uc5d0\uc11c \ucd5c\uace0 \uc218\uc900\uc758 \uac8c\uc784 \uadf8\ub798\ud53d\uc744 \uc9c0\uc6d0\ud560 \uc218 \uc788\ub3c4\ub85d \ucef4\ud4e8\ud305 \uc131\ub2a5\uc744 \ub192\uc600\ub2e4.\n**\ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub418\ub294 \ud48d\ubd80\ud55c \uadf8\ub798\ud53d \uacbd\ud5d8**\n\uc0bc\uc131\uc740 \ud504\ub9ac\ubbf8\uc5c4\ubd80\ud130 \uc800\uac00\ud615\uc5d0 \uc774\ub974\ub294 \ub113\uc740 \ubc94\uc704\uc758 \ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0 \ucf58\uc194 \uc218\uc900\uc758 \uac8c\uc784\uc744 \uc81c\uacf5\ud55c\ub2e4\ub294 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 AMD\uc758 RDNATM \uc544\ud0a4\ud14d\ucc98\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 Xclipse GPU\ub97c \uac1c\ubc1c\ud574\uc654\uace0, \uc774\ub97c \ud1b5\ud574 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(ray tracing) \uae30\ub2a5\uc744 \ucd5c\ucd08\ub85c \ub3c4\uc785\ud588\ub2e4",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\uac00 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \ubbf8\uc8fc\ucd1d\uad04(Device Solutions America office)\uc5d0\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023'\uc744 \uac1c\ucd5c\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \ucd08\uc9c0\ub2a5\ud654(Hyper-Intelligence)\u00b7\ucd08\uc5f0\uacb0\uc131(Hyper-Connected)\u00b7\ucd08\ub370\uc774\ud130(Hyper-Data)\ub97c \uac00\ub2a5\ud558\uac8c \ud560 \uc8fc\uc694 \uc751\uc6a9\ucc98\ubcc4 \ucd5c\uc2e0 \ubc18\ub3c4\uccb4 \uc124\uacc4 \ud604\ud669\uacfc \ube44\uc804\uc744 \uacf5\uc720\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \uace0\uac1d\uc0ac\uc640 \ud30c\ud2b8\ub108\uc0ac \uad00\uacc4\uc790 300\uc5ec\uba85\uc774 \ucc38\uc11d\ud55c \uac00\uc6b4\ub370, \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \ubd84\uc57c \uae00\ub85c\ubc8c \uc804\ubb38\uac00, \uc11d\ud559\ub4e4\uacfc \ud568\uaed8 \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM, Large Language Model) \uae30\uc220\uc5d0 \ub300\ud55c \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\ub97c \uc9c4\ud589\ud588\ub2e4.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc911\uc559 \ucef4\ud4e8\ud305\uc5d0 \ub300\ud55c \uc218\uc694\uac00 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ud558\ub4dc\uc6e8\uc5b4\uc640 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \ud655\uc7a5\uc131\uc774 \ub354\uc6b1 \uc911\uc694\ud574\uc9c8 \uac83\uc774\ub2e4. \ub530\ub77c\uc11c \uc0bc\uc131\uc758 \ud575\uc2ec \uc194\ub8e8\uc158 \uc911 \ud558\ub098\ub294 \uc18c\ud504\ud2b8\uc6e8\uc5b4\ub97c \uc218\uc815\ud558\uc9c0 \uc54a\uace0\ub3c4 \ucef4\ud4e8\ud305 \uc6a9\ub7c9\uc744 \ub450 \ubc30\ub85c \ub298\ub9ac\uae30 \uc704\ud574 \ub450 SoC \uac04\uc758 \uc9c1\uc811 \uc5f0\uacb0(die-to-die)\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. \ub610\ud55c \uc624\ud1a0 SoC\ub294 OEM\uc758 \uc2dc\uc2a4\ud15c\uc5d0 \ub530\ub77c \ud328\ud0a4\uc9c0 \uac04 \uba40\ud2f0 SoC \uc5f0\uacb0\uc744 \uc704\ud574 PCIe \ub610\ub294 \uc774\ub354\ub137 \uc0ac\uc6a9\uc744 \uc9c0\uc6d0\ud55c\ub2e4.\n_*AI \ud1b5\ud569, \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc815\uc758 \ucc28\ub7c9\uc758 \ubbf8\ub798\ub97c \uc704\ud55c \uc900\ube44 *_\n\uc55e\uc73c\ub85c \uc6b4\uc804\uc790\ub294 \ucd5c\ub300 150\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uc2e4\uc2dc\uac04\uc73c\ub85c \uc791\ub3d9\ud560 \uc218 \uc788\ub294 \uc0bc\uc131\uc758 \uc804\uc6a9 AI \uac00\uc18d\uae30\ub97c \ud1b5\ud574 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0 \uae30\ubc18\ud55c AI \ube44\uc11c\uc640 \uc6d0\ud65c\ud558\uac8c \uc0c1\ud638 \uc791\uc6a9\ud560 \uc218 \uc788\uac8c \ub420 \uac83\uc774\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc694\uac74\uc744 \ucda9\uc871\ud558\uace0 \uc778\uac04\uc758 \ud589\ub3d9\uc744 \uc644\ubcbd\ud558\uac8c \ubaa8\ubc29\ud55c\ub2e4\ub294 \uad81\uadf9\uc801\uc778 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 \uc0bc\uc131\uc740 \uc800\uc9c0\uc5f0\uacfc \uc800\uc804\ub825\uc744 \ub450 \uac00\uc9c0 \ud575\uc2ec \ubc1c\uc804 \ubd84\uc57c\ub85c \uc120\uc815\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uac1c\ubc1c\ud300 \ud64d\uae30\uc900 \uc0c1\ubb34\ub294 \u201c\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\u201d\uc774\ub77c\ub294 \uc81c\ubaa9\uc758 \uc138\uc158\uc5d0\uc11c \uc774 \ub450 \uac00\uc9c0 \ud575\uc2ec \ubd84\uc57c\ub97c \uc5b4\ub5bb\uac8c \ubc1c\uc804\uc2dc\ucf1c\uc654\ub294\uc9c0 \uc124\uba85\ud588\ub2e4.\n**\uc0c1\ud669 \uc778\uc9c0 \ucef4\ud4e8\ud305**\n\ub2e4\uc591\ud55c \uae30\ub2a5\uc774 \ud0d1\uc7ac\ub41c \ucd5c\uc2e0 \ub514\ubc14\uc774\uc2a4 \uc911 \uc0c1\ub2f9\uc218\ub294 \uc0c1\ud669 \uc778\uc9c0 \ucef4\ud4e8\ud3053\uc73c\ub85c \uad6c\ud604\ub418\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud55c\ub2e4. \uc0c1\ud669 \uc778\uc9c0\ub294 \uc801\uc678\uc120 \uc13c\uc2f1\uc5d0\uc11c \uc2dc\uc791\ub418\uc5c8\uc9c0\ub9cc, \ucd5c\uc2e0 \uc13c\uc11c\ub294 \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 \uc0c8\ub85c\uc6b4 \uac10\uac01 \ub370\uc774\ud130\uc758 \uc138\uacc4\ub97c \uc5f4\uc5b4 \uc8fc\uc5c8",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-4\nContent:\n| 43 | MAXPOOL | MAX_POOL_2D | O | O | O | O |\n| 44 | MEAN | MEAN | | O | O | O |\n| 45 | MINIMUM | MINIMUM | | O | O | O |\n| 46 | MIRROR_PAD | MIRROR_PAD | O | O | O | O |\n| 47 | MUL | MUL | | | O | O |\n| 48 | NEG | NEG | O | O | O | O |\n| 49 | PACK | PACK | | | O | O |\n| 50 | PAD | PAD | O | O | O | O |\n| 51 | PADV2 | PADV2 | O | | O | O |\n| 52 | PERMUTE | TRANSPOSE | O | | O | O |\n| 53 | POW | POW | | | O | O |\n| 54 | PRELU | PRELU | O | O | O | O |\n| 55 | QUANTIZE | QUANTIZE | O | O | O | O |\n| 56 | REDUCE_MAX | REDUCE_MAX | O | O | O | O |\n| 57 | REDUCE_MEAN | REDUCE_MEAN | O | O | O | O |\n| 58 | REDUCE_MIN | REDUCE_MIN | O | O | O | O |\n| 59 | REDUCE_SUM | SUM | O | | O | O |",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38 \ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc2dc\uc2a4\ud15cLSI \uc0ac\uc5c5\ubd80 \ubc15\uc6a9\uc778 \uc0ac\uc7a5\uc774 \ubc1c\ud45c\ub97c \ud558\uace0 \uc788\ub294 \ubaa8\uc2b5**\n_* *_\n_* *_\n\n**\u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38\ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \ubc1c\ud45c\ub97c \ub4e3\ub294 \uccad\uc911\ub4e4\uc758 \ubaa8\uc2b5**\n_* *_\n_* *_\n\n**\u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38 \ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019 \ud604\uc7a5",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc544\ud0a4\ud14d\ucc98 \ubcc0\uacbd\uc758 \uacb0\uacfc\ub85c MobileBERT1 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc740 \uc774\uc804 \uc138\ub300\uc5d0 \ube44\ud574 3\ubc30 \ud5a5\uc0c1\ub418\uc5c8\ub2e4\n**\ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc704\ud55c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC**\n\uc790\ub3d9\ucc28\ub294 \ud604\uc7ac \uc804\ud654\uac00 \uc2a4\ub9c8\ud2b8\ud3f0\uc73c\ub85c \uc9c4\ud654\ud55c \uac83\uacfc \uc720\uc0ac\ud558\uac8c \uae09\uaca9\ud558\uac8c \ubcc0\ud654\ud558\uace0 \uc788\ub2e4. \uc2e4\uc81c\ub85c \ud604\ub300\uc758 \uc790\ub3d9\ucc28\ub294 \ub2e8\uc21c\ud55c \uc774\ub3d9 \uc218\ub2e8\uc744 \ub118\uc5b4 \uc0dd\uc131\ud615 AI\uc640 \uac19\uc740 \uad11\ubc94\uc704\ud55c \ucef4\ud4e8\ud305 \uae30\ub2a5\uc758 \uc9d1\ud569\uccb4\uac00 \ub418\uc5c8\ub2e4.\n\uc774\ub7ec\ud55c \ud604\uc2e4\uc744 \ubc30\uacbd\uc73c\ub85c \uc0bc\uc131\uc804\uc790 AP2 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uac1c\ubc1c\ud300 \ubc29\uc9c0\ud6c8 \uc0c1\ubb34\ub294 \uc0bc\uc131 \uc624\ud1a0 SoC \uc194\ub8e8\uc158\uacfc \ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ub300\ud55c \uc138\uc158\uc744 \uc9c4\ud589\ud588\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\ub2a5\uc744 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub3c4\uc785\ud558\uae30 \uc704\ud55c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 IP\uac1c\ubc1c \ubc29\ud5a5\uc131\uc744 \uc18c\uac1c\ud588\ub2e4.\n**\ucd5c\uc801\ud654\ub41c 4\ud074\ub7ec\uc2a4\ud130 \uad6c\uc870\uc758 CPU**\n\uc138 \uac1c\uc758 \uc8fc\uc694 IP \uc911 \uccab \ubc88\uc9f8\ub294 CPU\ub85c, \uae30\uc874\uc5d0\ub294 \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud558\uc774\uc5d4\ub4dc(high-end) \ud074\ub7ec\uc2a4\ud130, \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ub85c\uc6b0\uc5d4\ub4dc(low-end) \ud074\ub7ec\uc2a4\ud130, \uc911\uac04 \ud074\ub7ec\uc2a4\ud130 \ub4f1 \uc138 \uac1c\uc758 \ud074\ub7ec\uc2a4\ud130\ub85c \uad6c\uc131\ub418\uc5c8\ub2e4. \uc0bc\uc131\uc740 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 CPU \uc6b4\uc601\uc744 \uc704\ud574 \uc911\uac04 \ud074\ub7ec\uc2a4\ud130\ub97c \ubbf8\ub4dc \ud558\uc774(mid-high)\uc640 \ubbf8\ub4dc \ub85c\uc6b0(mid-low)\uc758 \ub450 \uac00\uc9c0 \uacc4\uce35\uc73c\ub85c \ub098\ub204\uace0, \ubbf8\ub4dc \ud558\uc774 \uacc4\uce35\uc740 \uac8c\uc784\uacfc \uac19\uc740 \ucef4\ud4e8\ud305 \uc9d1\uc57d\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud588",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uc774 \uac1c\ub150\uc740 \uc13c\uc11c\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\uace0, \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uace0, \ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790\uc5d0\uac8c \ub354 \ub098\uc740 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud558\ub294 \uc138 \ub2e8\uacc4\ub85c \uc138\ubd84\ud654\ud560 \uc218 \uc788\ub2e4.\n\ubaa8\ub4e0 \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub824\uba74 \ub9ce\uc740 \uc5f0\uc0b0\uc774 \ud544\uc694\ud558\ubbc0\ub85c \uc804\ub825 \uc18c\ube44\uac00 \uc99d\uac00\ud55c\ub2e4. \ubc14\ub85c \uc774 \ubd80\ubd84\uc5d0\uc11c \uc0bc\uc131\uc758 \uae30\uc220\uc801 \ud601\uc2e0\uc774 \ube5b\uc744 \ubc1c\ud55c\ub2e4. \uc0bc\uc131\uc740 \uc804\uc6a9 \ub3c4\uba54\uc778\ubcc4 \uc2dc\uc2a4\ud15c\uc744 \ud3ec\ud568\ud558\ub294 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\ub97c \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub294 \ub354 \uac04\ub2e8\ud558\uba74\uc11c \ud2b9\uc218\ud55c \uc5f0\uc0b0 \ubd80\ud558\ub97c \ucc98\ub9ac\ud558\uae30 \ub54c\ubb38\uc5d0 \uae30\ubcf8\uc801\uc73c\ub85c \ud6a8\uc728\uc131\uc774 \ub192\ub2e4. \uc774 \uc2dc\uc2a4\ud15c\uc740 \uc804\ub825 \uc18c\ube44\uac00 \uc801\uace0 \ucc98\ub9ac \uc9c0\uc5f0 \uc2dc\uac04\uc774 \uc9e7\ub2e4.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "URL: news/40ed73bb-4b81-47ed-b2ea-96181bdaa61b/detail\nCategory: News\nTitle: \uc774\uc81c\ub294 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc\u2019\ub85c! \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \ucd08\uc5f0\uacb0 \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 \uae30\uc220\nContent: <br>\n\uace7\uc774\uc5b4 GPU \uc124\uacc4 \ud611\uc5c5 \uc18c\uac1c\uc640 \ub354\ubd88\uc5b4 \u2018\ucc28\ubcc4\ud654\ub41c GPU\uc640 NPU \uae30\uc220\u2019, \u2018\ubbf8\ub798 \uae30\uc220 \ub3d9\ud5a5\u2019, \u2018\uc774\ubbf8\uc9c0\uc13c\uc11c\uc758 \uc9c4\ud654\uc640 \uae30\uc220\ub825\u2019 \ub4f1\uc758 \uac15\uc5f0\uc774 \uc774\uc5b4\uc84c\ub2e4. \uc5ec\uae30\uc5d0 \u2018\uc0dd\uc131\ud615 AI \ubc0f \ub300\ud615\uc5b8\uc5b4\ubaa8\ub378(LLM)\uc758 \ucd5c\uadfc \ub3d9\ud5a5\uc774 \ucef4\ud4e8\ud305 \ud50c\ub7ab\ud3fc\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\u2019\uc5d0 \ub300\ud55c \ud328\ub110 \ud1a0\ub860\uc744 \ub354\ud574, \uad00\ub828 \uc5c5\uacc4\uc640 \uc774\ud574 \uad00\uacc4\uc790\ub4e4\uc774 \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4\uc5d0 \ub300\ud574 \ud55c\uce35 \ub354 \uc2ec\ub3c4 \uae4a\uac8c \uc774\ud574\ud560 \uc218 \uc788\ub294 \uc790\ub9ac\ub97c \ub9c8\ub828\ud588\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub610\ud55c \uc790\ub3d9\ucc28 \uae30\uc220\uc774 \ub354\uc6b1 \ubc1c\uc804\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub294 \uc0c1\ud669\uc5d0\uc11c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uac01 \uc601\uc5ed\uc758 \uc5ec\ub7ec \uc774\uae30\uc885 OS\ub97c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ud560 \uc218 \uc788\uc5b4 \ud68d\uae30\uc801\uc778 \ubbf8\ub798\ub97c \uc900\ube44\ud560 \uc218 \uc788\ub2e4. \ubc29 \uc0c1\ubb34\ub294 \"\uc0bc\uc131 \uc624\ud1a0 SoC\uc758 \uc911\uc559 \ucef4\ud4e8\ud305 \uae30\ub2a5 \ub355\ubd84\uc5d0 \ud55c\ub54c \uc0c1\uc0c1 \uc18d\uc5d0\ub9cc \uc874\uc7ac\ud588\ub358 \uac83\ub4e4\uc774 \uace7 \ud604\uc2e4\uc774 \ub420 \uac83\"\uc774\ub77c\uba70 \ubbf8\ub798\uc5d0 \ub300\ud55c \ubc1d\uc740 \uc804\ub9dd\uc744 \ub4dc\ub7ec\ub0c8\ub2e4.\n**\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158**\n\ub354\uc6b1 \uac15\ub825\ud55c SoC\uc640 \uc13c\uc11c \uae30\ub2a5\uc774 \uac1c\ubc1c\ub428\uc5d0 \ub530\ub77c \ucd5c\uc2e0 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc810\uc810 \ub354 \uc2a4\ub9c8\ud2b8\ud574\uc9c0\uace0 \ub192\uc740 \uc5f0\uc0b0 \ub2a5\ub825\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4",
                "URL: news/40ed73bb-4b81-47ed-b2ea-96181bdaa61b/detail\nCategory: News\nTitle: \uc774\uc81c\ub294 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc\u2019\ub85c! \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \ucd08\uc5f0\uacb0 \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 \uae30\uc220\nContent: 10\uc6d4 5\uc77c(\ud604\uc9c0 \uc2dc\uac01 \uae30\uc900, \ud55c\uad6d\uc2dc\uac04 6\uc77c \uc0c8\ubcbd) \ubbf8\uad6d \uce98\ub9ac\ud3ec\ub2c8\uc544 \uc0c8\ub108\uc81c\uc774\uc5d0\uc11c \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023(\uc774\ud558 \uc0bc\uc131 \ud14c\ud06c \ub370\uc774)\u2019\ub97c \uac1c\ucd5c\ud588\ub2e4. \ud14c\ud06c \ub370\uc774\ub294 2017\ub144\ubd80\ud130 \uafb8\uc900\ud788 \uc9c4\ud589\ud574 \uc628 \ubc18\ub3c4\uccb4 \uae30\uc220 \ucee8\ud37c\ub7f0\uc2a4\ub85c, \uc0c8\ub85c\uc6b4 \ubc18\ub3c4\uccb4 \uc2e0\uae30\uc220\uacfc \uac1c\ubc1c \uacc4\ud68d\uc744 \uc18c\uac1c\ud558\ub294 \uc790\ub9ac\ub2e4.\n\uc2dc\uc2a4\ud15cLSI \uc0ac\uc5c5\ubd80\uc640 \uba54\ubaa8\ub9ac \uc0ac\uc5c5\ubd80\uac00 \ud568\uaed8 \uc9c4\ud589\ud588\ub358 \uc804\ub144\ub3c4\uc640 \ub2ec\ub9ac, \uc62c\ud574\ub294 \uc0ac\uc5c5\ubd80\ubcc4\ub85c \ubcc4\ub3c4\uc758 \ud14c\ud06c \ub370\uc774\ub97c \uc9c4\ud589\ud55c\ub2e4. \ubbf8\ub798\ub97c \uc120\ub3c4\ud560 \ubcf4\ub2e4 \ub2e4\uc591\ud55c \uc194\ub8e8\uc158\uc744 \uc120\ubcf4\uc774\uae30 \uc704\ud568\uc774\ub2e4. \uc774\ubc88 \uc0bc\uc131 \ud14c\ud06c \ub370\uc774\uc5d0\uc11c \uc0bc\uc131\uc804\uc790 \ubc18\ub3c4\uccb4 \uc2dc\uc2a4\ud15cLSI \uc0ac\uc5c5\ubd80\uac00 \uc18c\uac1c\ud560 \ud575\uc2ec \uae30\uc220\ub825\uc740 \ubb34\uc5c7",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc5d1\uc2dc\ub178\uc2a4 \ub808\ud37c\ub7f0\uc2a4 \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub41c \ubaa8\ubc14\uc77c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5 \ub370\ubaa8 \uc601\uc0c1\uc744 \ud1b5\ud574 \uadf8\ub9bc\uc790, \ubc18\uc0ac \ub4f1\uacfc \uac19\uc740 \uc5ec\ub7ec \uac1c\uc758 \uc138\ubd80 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5\ub4e4\uc774 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.\n**\uc0dd\uc131\ud615 AI \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 NPU**\n\uc0bc\uc131\uc758 \ucd5c\uc2e0 NPU \uc194\ub8e8\uc158\uc740 \uba54\ubaa8\ub9ac \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcc0\uacbd\ud558\uace0 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \ube44\uc120\ud615 \uc5f0\uc0b0\uc758 \ud65c\uc6a9\ub3c4\ub97c \ud06c\uac8c \ub192\uc77c \uac83\uc774\ub2e4",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790 \uc2dc\uc2a4\ud15cLSI\uc0ac\uc5c5\ubd80 \ubc15\uc6a9\uc778 \uc0ac\uc7a5\uc740 \"\ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\uace0 \ucc98\ub9ac\ud558\ub294 '\uc0dd\uc131\ud615 AI'\uac00 \uc62c\ud574 \uac00\uc7a5 \uc911\uc694\ud55c \uae30\uc220 \ud2b8\ub79c\ub4dc\ub85c \uc790\ub9ac \uc7a1\uc73c\uba70 \ub354 \uace0\ub3c4\ud654\ub41c \uae30\ubc18 \uae30\uc220 \ud655\ubcf4\uc758 \ud544\uc694\uc131\uc774 \ub300\ub450\ub418\uace0 \uc788\ub2e4\" \uba70 \"\uc0bc\uc131\uc804\uc790\ub294 \uace0\uc131\ub2a5 IP\ubd80\ud130 \uc7a5\ub2e8\uac70\ub9ac \ud1b5\uc2e0 \uc194\ub8e8\uc158, \uc778\uac04\uc758 \uc624\uac10\uc744 \ubaa8\ubc29\ud55c \uc13c\uc11c \uae30\ubc18 '\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc(System LSI Humanoid)'\ub97c \uad6c\ud604\ud574\ub098\uac00\uba70 \uc0dd\uc131\ud615 AI\uc5d0\uc11c \ub354 \ubc1c\uc804\ub41c '\uc120\ud589\uc801 AI(Proactive AI)' \uc2dc\ub300\ub97c \uc5f4 \uac83\uc774\ub2e4\"\uace0 \ubc1d\ud614\ub2e4.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-6\nContent:\n| 75 | SELECT | SELECT | | | | O |\n| 76 | SHIFT_CLAMP | - | O | O | | |\n| 77 | SIN | SIN | | | O | O |\n| 78 | SLICE | SLICE | O | O | O | O |\n| 79 | SOFTMAX | SOFTMAX | | O | O | O |\n| 80 | SPACE_TO_BATCH_ND | SPACE_TO_BATCH_ND | | | | O |\n| 81 | SPACE_TO_DEPTH | SPACE_TO_DEPTH | O | O | O | O |\n| 82 | SPLIT | SPLIT | O | | O | O |\n| 83 | SPLIT_V | SPLIT_V | O | | | O |\n| 84 | SQRT | SQRT | | O | O | O |\n| 85 | SQUARED_DIFFERENCE | SQUARED_DIFFERENCE | | | O | O |\n| 86 | SQUEEZE | SQUEEZE | | | O | O |\n| 87 | STRIDED_SLICE | STRIDED_SLICE | O | O | O | O |\n| 88 | SUB | SUB | | | O | O |\n| 89 | TANH | TANH | O | O | O | O |\n| 90 | TILE | TILE | O | | O | O |\n| 91 | TRANSPOSE | TRANSPOSE | O | O | O | O |",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL."
            ]
        ],
        "answer": "The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
        "ground_truth": "The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day."
    },
    {
        "question": "I would like to know the ENN SDK support models and environments.",
        "contexts": [
            [
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: API References\\API reference\nContent:\n## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references)\n- **_NumberOfBuffersInfo**\n- **_ennBuffer**\n- **_ennBufferInfo**\n## [ENN Framwork API Functions](api-reference/enn-framework-api-functions)\n- **Context initialize / deinitialize**\n- **OpenModel / CloseModel related**\n- **Memory Handling**\n- **Setters and Getters for model**\n- **Commit Buffer**\n- **Execute Models**\n- **Security, preference, get meta information..** ",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\4. ENN tensor Framework APIs\nContent:\n### Data Type References\nFor more information on the list of data types, refer to API Reference.\n### API Functions\nFor more information on the list of API functions, refer to API Reference.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions\nContent:\n| | Name |\n| -------------- | -------------- |\n| EnnReturn | **EnnExecuteModel**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers. |\n| EnnReturn | **EnnExecuteModelAsync**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously. |\n| EnnReturn | **EnnExecuteModelWait**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling EnnExecuteModelAsync() If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished. |",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions Documentation\nContent:\n#### function EnnExecuteModel\n```cpp\nEnnReturn EnnExecuteModel(\n\tconst EnnModelId model_id,\n\tconst int session_id =0\n)\n```\nRequest to service core to execute model with commited buffers.\n**Parameters**:\n- **model_id** [IN] model ID from load_model\n- **session_id** [IN] session ID\n**Return**: EnnReturn result, 0 is success\n**Note**: this function runs in block mode\n#### function EnnExecuteModelAsync\n```cpp\nEnnReturn EnnExecuteModelAsync(\n\tconst EnnModelId model_id,\n\tconst int session_id =0\n)\n```\nRequest to service core to execute model in background asynchronously.\n**Parameters**:\n- **model_id** [IN] model ID from load_model\n- **session_id** [IN] session ID\n**Return**: EnnReturn result, 0 is success ",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\n#### Executing Model\nThe execution phase of model involves copying the input layer, executing the model, and copying the output layer.\nFor more information on copying data to the input layer and comparing data with the output layer, refer to Section Processing Input and Outputs section.\n1. EnnExecuteModel:\n\tThis function executes the model using the specified model ID.\n\t(example):\n\t```cpp\n\tenn::api::EnnExecuteModel(model_id)\n\t```\n#### Deinitializing Framework\nAfter executing the model, deinitialize the framework to release resources.\n1. EnnReleaseBuffers:\n\tThis function releases the allocated buffers.\n\t(example):\n\t```cpp\n\tenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n\t```\n2. EnnCloseModel:\n\tThis function closes the specified model.\n\t(example):\n\t```cpp\n\tenn::api::EnnCloseModel(model_id)\n\t```\n3. EnnDeinitialize:\n\tThis function deinitializes the ENN framework.\n\t(example):\n\t```cpp\n\tenn::api::EnnDeinitialize()\n\t```",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: Function Implementation\\Implementing Function\\Appendix\\A. Class Information\nContent:\n#### A.1 Data Classes (`data` package)\n- `DataType.kt`: An enum class for input/output buffer data types.\n- `LayerType.kt`: An enum class for defining data formats.\n- `ModelConstants.kt`: An object class that contains model parameters.\n#### A.2 ENN JNI Class (`enn_type` package)\n- `BufferSetInfo.kt`: A class for receiving buffer data from the JNI interface.\n#### A.3 Executor Class (`executor` package)\n- `ModelExecutor.kt`: A class for executing the ENN SDK and processing input and output data.\n\t| Function | Details |\n\t| -------- | ------- |\n\t| `setupENN` | Initializes the ENN SDK framework, opens and allocates buffers of the model. |\n\t| `process` | Calls the data preprocess function, executes the model, and calls the postprocess function. |\n\t| `closeENN` | Releases allocated buffers, closes the model, and deinitializes the ENN SDK framework. |\n\t| `preProcess` | Converts bitmap images to input data byte arrays. |\n\t| `postProcess` | Converts output data byte arrays to a mapping of index and score. |",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\OpenModel / CloseModel related\\Functions Documentation\nContent:\n#### function EnnOpenModel\n```cpp\nEnnReturn EnnOpenModel(\n\tconst char * model_file,\n\tEnnModelId * model_id\n)\n```\nOpenModel with model file.\n**Parameters**:\n- **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n- **model_id** [OUT] model_id, 64 bit unsigned int\n**Return**: EnnReturn result, 0 is success\n#### function EnnOpenModelFromMemory\n```cpp\nEnnReturn EnnOpenModelFromMemory(\n\tconst char * va,\n\tconst uint32_t size,\n\tEnnModelId * model_id\n)\n```\nOpenModel from memory buffer.\n**Parameters**:\n- **va** [IN] address which a model loaded from\n- **size** [IN] size of the buffer\n- **model_id** [OUT] model_id, 64 bit unsigned int\n**Return**: EnnReturn result, 0 is success\n#### function EnnCloseModel\n```cpp\nEnnReturn EnnCloseModel(\n\tconst EnnModelId model_id\n);\n```\nClose model and free all resources in OpenModel().\n**Parameters**:\n- **model_id** [IN] model_id from OpenModel()\n**Return**: EnnReturn result, 0 is success",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\nContent:\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.\n1. Create `enn_jni.cc` in `cpp` directory.\n2. Modify `CMakeLists.txt` created here.\n\t```cmake\n\tadd_library(\n\t\tenn_jni\n\t\tSHARED\n\t\tenn_jni.cc\n\t)\n\ttarget_link_libraries(\n\t\tenn_jni\n\t)\n\t```",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK."
            ]
        ],
        "answer": "The ENN SDK supports a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality. The SDK is primarily designed to optimize and implement AI and machine learning models on Exynos-based devices. It facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processors, providing comprehensive support for a wide range of AI applications and usage scenarios.",
        "ground_truth": "The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios."
    },
    {
        "question": "I want to know the planning intention of AI Challenger.",
        "contexts": [
            [
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc911\uc559 \ucef4\ud4e8\ud305\uc5d0 \ub300\ud55c \uc218\uc694\uac00 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ud558\ub4dc\uc6e8\uc5b4\uc640 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \ud655\uc7a5\uc131\uc774 \ub354\uc6b1 \uc911\uc694\ud574\uc9c8 \uac83\uc774\ub2e4. \ub530\ub77c\uc11c \uc0bc\uc131\uc758 \ud575\uc2ec \uc194\ub8e8\uc158 \uc911 \ud558\ub098\ub294 \uc18c\ud504\ud2b8\uc6e8\uc5b4\ub97c \uc218\uc815\ud558\uc9c0 \uc54a\uace0\ub3c4 \ucef4\ud4e8\ud305 \uc6a9\ub7c9\uc744 \ub450 \ubc30\ub85c \ub298\ub9ac\uae30 \uc704\ud574 \ub450 SoC \uac04\uc758 \uc9c1\uc811 \uc5f0\uacb0(die-to-die)\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. \ub610\ud55c \uc624\ud1a0 SoC\ub294 OEM\uc758 \uc2dc\uc2a4\ud15c\uc5d0 \ub530\ub77c \ud328\ud0a4\uc9c0 \uac04 \uba40\ud2f0 SoC \uc5f0\uacb0\uc744 \uc704\ud574 PCIe \ub610\ub294 \uc774\ub354\ub137 \uc0ac\uc6a9\uc744 \uc9c0\uc6d0\ud55c\ub2e4.\n_*AI \ud1b5\ud569, \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc815\uc758 \ucc28\ub7c9\uc758 \ubbf8\ub798\ub97c \uc704\ud55c \uc900\ube44 *_\n\uc55e\uc73c\ub85c \uc6b4\uc804\uc790\ub294 \ucd5c\ub300 150\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uc2e4\uc2dc\uac04\uc73c\ub85c \uc791\ub3d9\ud560 \uc218 \uc788\ub294 \uc0bc\uc131\uc758 \uc804\uc6a9 AI \uac00\uc18d\uae30\ub97c \ud1b5\ud574 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0 \uae30\ubc18\ud55c AI \ube44\uc11c\uc640 \uc6d0\ud65c\ud558\uac8c \uc0c1\ud638 \uc791\uc6a9\ud560 \uc218 \uc788\uac8c \ub420 \uac83\uc774\ub2e4.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _* *_\n**\u25a1 5\uc77c (\ud604\uc9c0\uc2dc\uac04), \uc751\uc6a9\ucc98\ubcc4 \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \ud604\ud669\u00b7\ube44\uc804 \ubc1c\ud45c**\n - \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\uc220 \uac1c\ubc1c \uad00\ub828 \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\n\n**\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d, \uc0dd\uc131\ud615 AI \uae30\uc220 \ud0d1\uc7ac\ud55c '\uc5d1\uc2dc\ub178\uc2a4 2400' \uacf5\uac1c**\n - \uc804\uc791 \ub300\ube44 CPU \uc131\ub2a5 1.7\ubc30, AI \uc131\ub2a5 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\n - \uc5d1\uc2dc\ub178\uc2a4 2400 \uae30\ubc18 \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \uac8c\uc784 \ud504\ub9ac\ubdf0 \uc601\uc0c1 \uacf5\uac1c\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n - \uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4 \uacf5\uac1c\u2026\ucd5c\ub300 4\ubc30 \uc90c\uc5d0\ub3c4 \ucd08\uace0\ud654\uc9c8, AI \ud2b8\ub798\ud0b9 \uae30\uc220 \uc801\uc6a9\n - \ucc28\uc138\ub300 \ucc28\ub7c9\uc6a9 \uc194\ub8e8\uc158, \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c(NTN) \uc601\uc0c1 \uc2dc\uc5f0",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\ub2a5\uc744 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub3c4\uc785\ud558\uae30 \uc704\ud55c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 IP\uac1c\ubc1c \ubc29\ud5a5\uc131\uc744 \uc18c\uac1c\ud588\ub2e4.\n**\ucd5c\uc801\ud654\ub41c 4\ud074\ub7ec\uc2a4\ud130 \uad6c\uc870\uc758 CPU**\n\uc138 \uac1c\uc758 \uc8fc\uc694 IP \uc911 \uccab \ubc88\uc9f8\ub294 CPU\ub85c, \uae30\uc874\uc5d0\ub294 \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud558\uc774\uc5d4\ub4dc(high-end) \ud074\ub7ec\uc2a4\ud130, \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ub85c\uc6b0\uc5d4\ub4dc(low-end) \ud074\ub7ec\uc2a4\ud130, \uc911\uac04 \ud074\ub7ec\uc2a4\ud130 \ub4f1 \uc138 \uac1c\uc758 \ud074\ub7ec\uc2a4\ud130\ub85c \uad6c\uc131\ub418\uc5c8\ub2e4. \uc0bc\uc131\uc740 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 CPU \uc6b4\uc601\uc744 \uc704\ud574 \uc911\uac04 \ud074\ub7ec\uc2a4\ud130\ub97c \ubbf8\ub4dc \ud558\uc774(mid-high)\uc640 \ubbf8\ub4dc \ub85c\uc6b0(mid-low)\uc758 \ub450 \uac00\uc9c0 \uacc4\uce35\uc73c\ub85c \ub098\ub204\uace0, \ubbf8\ub4dc \ud558\uc774 \uacc4\uce35\uc740 \uac8c\uc784\uacfc \uac19\uc740 \ucef4\ud4e8\ud305 \uc9d1\uc57d\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud588",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc694\uac74\uc744 \ucda9\uc871\ud558\uace0 \uc778\uac04\uc758 \ud589\ub3d9\uc744 \uc644\ubcbd\ud558\uac8c \ubaa8\ubc29\ud55c\ub2e4\ub294 \uad81\uadf9\uc801\uc778 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 \uc0bc\uc131\uc740 \uc800\uc9c0\uc5f0\uacfc \uc800\uc804\ub825\uc744 \ub450 \uac00\uc9c0 \ud575\uc2ec \ubc1c\uc804 \ubd84\uc57c\ub85c \uc120\uc815\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uac1c\ubc1c\ud300 \ud64d\uae30\uc900 \uc0c1\ubb34\ub294 \u201c\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\u201d\uc774\ub77c\ub294 \uc81c\ubaa9\uc758 \uc138\uc158\uc5d0\uc11c \uc774 \ub450 \uac00\uc9c0 \ud575\uc2ec \ubd84\uc57c\ub97c \uc5b4\ub5bb\uac8c \ubc1c\uc804\uc2dc\ucf1c\uc654\ub294\uc9c0 \uc124\uba85\ud588\ub2e4.\n**\uc0c1\ud669 \uc778\uc9c0 \ucef4\ud4e8\ud305**\n\ub2e4\uc591\ud55c \uae30\ub2a5\uc774 \ud0d1\uc7ac\ub41c \ucd5c\uc2e0 \ub514\ubc14\uc774\uc2a4 \uc911 \uc0c1\ub2f9\uc218\ub294 \uc0c1\ud669 \uc778\uc9c0 \ucef4\ud4e8\ud3053\uc73c\ub85c \uad6c\ud604\ub418\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud55c\ub2e4. \uc0c1\ud669 \uc778\uc9c0\ub294 \uc801\uc678\uc120 \uc13c\uc2f1\uc5d0\uc11c \uc2dc\uc791\ub418\uc5c8\uc9c0\ub9cc, \ucd5c\uc2e0 \uc13c\uc11c\ub294 \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 \uc0c8\ub85c\uc6b4 \uac10\uac01 \ub370\uc774\ud130\uc758 \uc138\uacc4\ub97c \uc5f4\uc5b4 \uc8fc\uc5c8",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc0dd\uc131\ud615 \uc778\uacf5\uc9c0\ub2a5(AI) \ubc0f \uac01\uc885 \ucca8\ub2e8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ub4f1\uc7a5\uc73c\ub85c \uc778\ud574 \uc2dc\uc7a5\uc5d0\uc11c\ub294 \ud6a8\uc728\uc131, \ud655\uc7a5\uc131, \uc800\uc804\ub825\uc744 \uac16\ucd98 \ub354\uc6b1 \uc2a4\ub9c8\ud2b8\ud558\uace0 \uac15\ub825\ud55c \ucef4\ud4e8\ud305\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4. \uc774\uc5d0 \ub530\ub77c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\uc758 \uc8fc\uc694 \ud14c\ub9c8 \uc911 \ud558\ub098\uc778 \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\ub97c \uc8fc\uc81c\ub85c \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220, \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158, \uc0bc\uc131 \uc624\ud1a0 SoC(System-on-Chips) \ub4f1 \uc138 \uac1c\uc758 \uac15\uc5f0\uc774 \uc9c4\ud589\ub418\uc5c8\ub2e4.\n**\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220**\n\uccab\ubc88\uc9f8 \uc138\uc158\uc778 \"\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220\"\uc740 \uc0bc\uc131 \uc624\uc2a4\ud2f4 \uc5f0\uad6c\uc13c\ud130 \ubca0\ub2c8 \uce74\ud2f0\ube44\uc548(Benny Katibian) \ubd80\uc0ac\uc7a5\uc774 \uc9c4\ud589\ud588\ub2e4",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\uac00 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \ubbf8\uc8fc\ucd1d\uad04(Device Solutions America office)\uc5d0\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023'\uc744 \uac1c\ucd5c\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \ucd08\uc9c0\ub2a5\ud654(Hyper-Intelligence)\u00b7\ucd08\uc5f0\uacb0\uc131(Hyper-Connected)\u00b7\ucd08\ub370\uc774\ud130(Hyper-Data)\ub97c \uac00\ub2a5\ud558\uac8c \ud560 \uc8fc\uc694 \uc751\uc6a9\ucc98\ubcc4 \ucd5c\uc2e0 \ubc18\ub3c4\uccb4 \uc124\uacc4 \ud604\ud669\uacfc \ube44\uc804\uc744 \uacf5\uc720\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \uace0\uac1d\uc0ac\uc640 \ud30c\ud2b8\ub108\uc0ac \uad00\uacc4\uc790 300\uc5ec\uba85\uc774 \ucc38\uc11d\ud55c \uac00\uc6b4\ub370, \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \ubd84\uc57c \uae00\ub85c\ubc8c \uc804\ubb38\uac00, \uc11d\ud559\ub4e4\uacfc \ud568\uaed8 \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM, Large Language Model) \uae30\uc220\uc5d0 \ub300\ud55c \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\ub97c \uc9c4\ud589\ud588\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uc774 \uac1c\ub150\uc740 \uc13c\uc11c\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\uace0, \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uace0, \ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790\uc5d0\uac8c \ub354 \ub098\uc740 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud558\ub294 \uc138 \ub2e8\uacc4\ub85c \uc138\ubd84\ud654\ud560 \uc218 \uc788\ub2e4.\n\ubaa8\ub4e0 \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub824\uba74 \ub9ce\uc740 \uc5f0\uc0b0\uc774 \ud544\uc694\ud558\ubbc0\ub85c \uc804\ub825 \uc18c\ube44\uac00 \uc99d\uac00\ud55c\ub2e4. \ubc14\ub85c \uc774 \ubd80\ubd84\uc5d0\uc11c \uc0bc\uc131\uc758 \uae30\uc220\uc801 \ud601\uc2e0\uc774 \ube5b\uc744 \ubc1c\ud55c\ub2e4. \uc0bc\uc131\uc740 \uc804\uc6a9 \ub3c4\uba54\uc778\ubcc4 \uc2dc\uc2a4\ud15c\uc744 \ud3ec\ud568\ud558\ub294 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\ub97c \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub294 \ub354 \uac04\ub2e8\ud558\uba74\uc11c \ud2b9\uc218\ud55c \uc5f0\uc0b0 \ubd80\ud558\ub97c \ucc98\ub9ac\ud558\uae30 \ub54c\ubb38\uc5d0 \uae30\ubcf8\uc801\uc73c\ub85c \ud6a8\uc728\uc131\uc774 \ub192\ub2e4. \uc774 \uc2dc\uc2a4\ud15c\uc740 \uc804\ub825 \uc18c\ube44\uac00 \uc801\uace0 \ucc98\ub9ac \uc9c0\uc5f0 \uc2dc\uac04\uc774 \uc9e7\ub2e4.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uacfc\uc911\ud55c \uac8c\uc784 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc804\ub825\uacfc \uc131\ub2a5\uc744 \ub354\uc6b1 \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 CPU \uc804\ub825 \ubd80\ubd84\uc744 \uc904\uc774\uace0 \ub354 \ub9ce\uc740 \uc804\ub825\uc744 CPU \ucf54\uc5b4\uc5d0\uc11c GPU\ub85c \uc7ac\ud560\ub2f9\ud558\uc5ec \ubaa8\ubc14\uc77c \uc7a5\uce58\uc5d0\uc11c \ucd5c\uace0 \uc218\uc900\uc758 \uac8c\uc784 \uadf8\ub798\ud53d\uc744 \uc9c0\uc6d0\ud560 \uc218 \uc788\ub3c4\ub85d \ucef4\ud4e8\ud305 \uc131\ub2a5\uc744 \ub192\uc600\ub2e4.\n**\ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub418\ub294 \ud48d\ubd80\ud55c \uadf8\ub798\ud53d \uacbd\ud5d8**\n\uc0bc\uc131\uc740 \ud504\ub9ac\ubbf8\uc5c4\ubd80\ud130 \uc800\uac00\ud615\uc5d0 \uc774\ub974\ub294 \ub113\uc740 \ubc94\uc704\uc758 \ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0 \ucf58\uc194 \uc218\uc900\uc758 \uac8c\uc784\uc744 \uc81c\uacf5\ud55c\ub2e4\ub294 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 AMD\uc758 RDNATM \uc544\ud0a4\ud14d\ucc98\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 Xclipse GPU\ub97c \uac1c\ubc1c\ud574\uc654\uace0, \uc774\ub97c \ud1b5\ud574 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(ray tracing) \uae30\ub2a5\uc744 \ucd5c\ucd08\ub85c \ub3c4\uc785\ud588\ub2e4",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc544\ud0a4\ud14d\ucc98 \ubcc0\uacbd\uc758 \uacb0\uacfc\ub85c MobileBERT1 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc740 \uc774\uc804 \uc138\ub300\uc5d0 \ube44\ud574 3\ubc30 \ud5a5\uc0c1\ub418\uc5c8\ub2e4\n**\ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc704\ud55c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC**\n\uc790\ub3d9\ucc28\ub294 \ud604\uc7ac \uc804\ud654\uac00 \uc2a4\ub9c8\ud2b8\ud3f0\uc73c\ub85c \uc9c4\ud654\ud55c \uac83\uacfc \uc720\uc0ac\ud558\uac8c \uae09\uaca9\ud558\uac8c \ubcc0\ud654\ud558\uace0 \uc788\ub2e4. \uc2e4\uc81c\ub85c \ud604\ub300\uc758 \uc790\ub3d9\ucc28\ub294 \ub2e8\uc21c\ud55c \uc774\ub3d9 \uc218\ub2e8\uc744 \ub118\uc5b4 \uc0dd\uc131\ud615 AI\uc640 \uac19\uc740 \uad11\ubc94\uc704\ud55c \ucef4\ud4e8\ud305 \uae30\ub2a5\uc758 \uc9d1\ud569\uccb4\uac00 \ub418\uc5c8\ub2e4.\n\uc774\ub7ec\ud55c \ud604\uc2e4\uc744 \ubc30\uacbd\uc73c\ub85c \uc0bc\uc131\uc804\uc790 AP2 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uac1c\ubc1c\ud300 \ubc29\uc9c0\ud6c8 \uc0c1\ubb34\ub294 \uc0bc\uc131 \uc624\ud1a0 SoC \uc194\ub8e8\uc158\uacfc \ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ub300\ud55c \uc138\uc158\uc744 \uc9c4\ud589\ud588\ub2e4.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uba40\ud2f0\ubbf8\ub514\uc5b4\uc758 \uae30\uc220\uc758 \ubc1c\uc804\uc73c\ub85c \ud734\uba38\ub178\uc774\ub4dc\uac00 \uc778\uac04\ucc98\ub7fc \uc790\uc5f0\uc2a4\ub7fd\uac8c \uac10\uc9c0\ud558\uace0, \uc778\uc9c0\ud558\uace0, \ud589\ub3d9\ud558\ub294 \ubbf8\ub798\uc5d0 \ub354 \uac00\uae4c\uc6cc\uc9c0\uace0 \uc788\ub2e4. \ud64d \uc0c1\ubb34\ub294 \"\uc774\ub7ec\ud55c \ubbf8\ub798\ub97c \uc2e4\ud604\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc544\uc9c1 \ud604\uc2e4\uacfc\uc758 \uae30\uc220\uc801\uc778 \uaca9\ucc28\ub97c \uc881\ud600\uc57c \ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac \uc2dc\uc2a4\ud15c \uae30\uc220\ub85c \ubab0\uc785\uac10 \ub118\uce58\uace0 \uc778\ud130\ub799\ud2f0\ube0c\ud55c \ubbf8\ub798\ub97c \uad6c\ud604\ud560 \uac83\u201d\uc774\ub77c\uace0 \ud655\uc2e0\ud588\ub2e4.\n\n1 MobileBERT: \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc758 \ubaa8\ubc14\uc77c \ubc84\uc804\n2 AP: Application Processor\n3 \ucef4\ud4e8\ud305\uc5d0\uc11c \uc0c1\ud669 \uc778\uc9c0\ub294 \uc2dc\uc2a4\ud15c/\uad6c\uc131 \uc694\uc18c\uac00 \ud658\uacbd\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc218\uc9d1\ud558\uace0 \uadf8\uc5d0 \ub530\ub77c \ub3d9\uc791\uc744 \uc870\uc815\ud558\ub294 \uae30\ub2a5\uc744 \ub9d0\ud568\n ",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc5d1\uc2dc\ub178\uc2a4 \ub808\ud37c\ub7f0\uc2a4 \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub41c \ubaa8\ubc14\uc77c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5 \ub370\ubaa8 \uc601\uc0c1\uc744 \ud1b5\ud574 \uadf8\ub9bc\uc790, \ubc18\uc0ac \ub4f1\uacfc \uac19\uc740 \uc5ec\ub7ec \uac1c\uc758 \uc138\ubd80 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5\ub4e4\uc774 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.\n**\uc0dd\uc131\ud615 AI \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 NPU**\n\uc0bc\uc131\uc758 \ucd5c\uc2e0 NPU \uc194\ub8e8\uc158\uc740 \uba54\ubaa8\ub9ac \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcc0\uacbd\ud558\uace0 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \ube44\uc120\ud615 \uc5f0\uc0b0\uc758 \ud65c\uc6a9\ub3c4\ub97c \ud06c\uac8c \ub192\uc77c \uac83\uc774\ub2e4",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790 \uc2dc\uc2a4\ud15cLSI\uc0ac\uc5c5\ubd80 \ubc15\uc6a9\uc778 \uc0ac\uc7a5\uc740 \"\ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\uace0 \ucc98\ub9ac\ud558\ub294 '\uc0dd\uc131\ud615 AI'\uac00 \uc62c\ud574 \uac00\uc7a5 \uc911\uc694\ud55c \uae30\uc220 \ud2b8\ub79c\ub4dc\ub85c \uc790\ub9ac \uc7a1\uc73c\uba70 \ub354 \uace0\ub3c4\ud654\ub41c \uae30\ubc18 \uae30\uc220 \ud655\ubcf4\uc758 \ud544\uc694\uc131\uc774 \ub300\ub450\ub418\uace0 \uc788\ub2e4\" \uba70 \"\uc0bc\uc131\uc804\uc790\ub294 \uace0\uc131\ub2a5 IP\ubd80\ud130 \uc7a5\ub2e8\uac70\ub9ac \ud1b5\uc2e0 \uc194\ub8e8\uc158, \uc778\uac04\uc758 \uc624\uac10\uc744 \ubaa8\ubc29\ud55c \uc13c\uc11c \uae30\ubc18 '\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc(System LSI Humanoid)'\ub97c \uad6c\ud604\ud574\ub098\uac00\uba70 \uc0dd\uc131\ud615 AI\uc5d0\uc11c \ub354 \ubc1c\uc804\ub41c '\uc120\ud589\uc801 AI(Proactive AI)' \uc2dc\ub300\ub97c \uc5f4 \uac83\uc774\ub2e4\"\uace0 \ubc1d\ud614\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub610\ud55c \uc790\ub3d9\ucc28 \uae30\uc220\uc774 \ub354\uc6b1 \ubc1c\uc804\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub294 \uc0c1\ud669\uc5d0\uc11c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uac01 \uc601\uc5ed\uc758 \uc5ec\ub7ec \uc774\uae30\uc885 OS\ub97c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ud560 \uc218 \uc788\uc5b4 \ud68d\uae30\uc801\uc778 \ubbf8\ub798\ub97c \uc900\ube44\ud560 \uc218 \uc788\ub2e4. \ubc29 \uc0c1\ubb34\ub294 \"\uc0bc\uc131 \uc624\ud1a0 SoC\uc758 \uc911\uc559 \ucef4\ud4e8\ud305 \uae30\ub2a5 \ub355\ubd84\uc5d0 \ud55c\ub54c \uc0c1\uc0c1 \uc18d\uc5d0\ub9cc \uc874\uc7ac\ud588\ub358 \uac83\ub4e4\uc774 \uace7 \ud604\uc2e4\uc774 \ub420 \uac83\"\uc774\ub77c\uba70 \ubbf8\ub798\uc5d0 \ub300\ud55c \ubc1d\uc740 \uc804\ub9dd\uc744 \ub4dc\ub7ec\ub0c8\ub2e4.\n**\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158**\n\ub354\uc6b1 \uac15\ub825\ud55c SoC\uc640 \uc13c\uc11c \uae30\ub2a5\uc774 \uac1c\ubc1c\ub428\uc5d0 \ub530\ub77c \ucd5c\uc2e0 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc810\uc810 \ub354 \uc2a4\ub9c8\ud2b8\ud574\uc9c0\uace0 \ub192\uc740 \uc5f0\uc0b0 \ub2a5\ub825\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\nExecuting NN models on the ENN framework comprises of three steps such as initializing the framework, inferring the models, and deinitializing the framework.\nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.\n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\n\tA1[Initialize]\n\tA1 --> A2[Open Model]\n\tA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\n\tB1[Copy Input Layer]\n\tB1 --> B2[Execute Model]\n\tB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\n\tC1[Release Buffers]\n\tC1 --> C2[Close Model]\n\tC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```\nTo infer multiple data, repeat `Inference`.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\Lifecycle of Model Inference\nContent:\nFollowing flowchart describes the lifecycle of inferring NN models using ENN framework.\n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\n\tA1[Initialize]\n\tA1 --> A2[Open Model]\n\tA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\n\tB1[Copy Input Layer]\n\tB1 --> B2[Execute Model]\n\tB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\n\tC1[Release Buffers]\n\tC1 --> C2[Close Model]\n\tC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```\nTo infer multiple data, repeat `Inference` process.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\n#### Executing Model\nThe execution phase of model involves copying the input layer, executing the model, and copying the output layer.\nFor more information on copying data to the input layer and comparing data with the output layer, refer to Section Processing Input and Outputs section.\n1. EnnExecuteModel:\n\tThis function executes the model using the specified model ID.\n\t(example):\n\t```cpp\n\tenn::api::EnnExecuteModel(model_id)\n\t```\n#### Deinitializing Framework\nAfter executing the model, deinitialize the framework to release resources.\n1. EnnReleaseBuffers:\n\tThis function releases the allocated buffers.\n\t(example):\n\t```cpp\n\tenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n\t```\n2. EnnCloseModel:\n\tThis function closes the specified model.\n\t(example):\n\t```cpp\n\tenn::api::EnnCloseModel(model_id)\n\t```\n3. EnnDeinitialize:\n\tThis function deinitializes the ENN framework.\n\t(example):\n\t```cpp\n\tenn::api::EnnDeinitialize()\n\t```",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc548\uc804\uc5d0 \ub300\ud55c \uc0bc\uc131\uc758 \ud655\uace0\ud55c \uc758\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ucc28\uc138\ub300 \uc624\ud1a0 SoC\uc758 \uc138\uc774\ud504\ud2f0 \uc544\uc77c\ub79c\ub4dc(safety island)\ub294 ASIL-D\ub97c \uc900\uc218\ud558\uba70, \ud638\uc2a4\ud2b8 CPU\uc640 \ubcc4\ub3c4\ub85c \uc791\ub3d9\ud558\uc5ec \ub2e4\ub978 SoC\uc758 \uc0c1\ud0dc\ub97c \ubaa8\ub2c8\ud130\ub9c1\ud560 \uc608\uc815\uc774\ub2e4. \uc774\uc678\uc758 \ubd80\ubd84\uc740 ASPICE, ISO 26262 \ubc0f FMEA\uc640 \uac19\uc740 \uc790\ub3d9\ucc28 \ud45c\uc900\uc744 \ud1b5\ud574 ASIL-B\ub97c \uc900\uc218\ud558\ub3c4\ub85d \uc81c\uc791\ub418\uc5c8\ub2e4.\n\uc0ac\uc774\ubc84 \ubcf4\uc548 \ub610\ud55c \uc810\uc810 \ub354 \uc911\uc694\ud55c \uc694\uc18c\ub85c \ub5a0\uc624\ub974\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0, \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uc554\ud638\ud654 \uc5d4\uc9c4\uc774 \ub0b4\uc7a5\ub41c \uae30\ubcf8 \ubcf4\uc548 \ud504\ub85c\uc138\uc11c\uc640 StrongBOX \ud558\ub4dc\uc6e8\uc5b4 \ube14\ub85d\uc744 \uac16\ucd94\uace0 \uc788\ub2e4. \ub610\ud55c \uc0bc\uc131\uc740 \uc0ac\uc6a9\uc790 \uc815\ubcf4\ub97c \ubcf4\ud638\ud558\ub294 \uc790\uccb4 \uc6b4\uc601\uccb4\uc81c\uc778 \uc5d1\uc2dc\ub178\uc2a4TEE\ub97c \uac1c\ubc1c\ud588\uc73c\uba70 \uc774\ubbf8 EAL2 \uc778\uc99d\uc744 \ubc1b\uc558",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "URL: news/40ed73bb-4b81-47ed-b2ea-96181bdaa61b/detail\nCategory: News\nTitle: \uc774\uc81c\ub294 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc\u2019\ub85c! \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \ucd08\uc5f0\uacb0 \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 \uae30\uc220\nContent: <br>\n\uace7\uc774\uc5b4 GPU \uc124\uacc4 \ud611\uc5c5 \uc18c\uac1c\uc640 \ub354\ubd88\uc5b4 \u2018\ucc28\ubcc4\ud654\ub41c GPU\uc640 NPU \uae30\uc220\u2019, \u2018\ubbf8\ub798 \uae30\uc220 \ub3d9\ud5a5\u2019, \u2018\uc774\ubbf8\uc9c0\uc13c\uc11c\uc758 \uc9c4\ud654\uc640 \uae30\uc220\ub825\u2019 \ub4f1\uc758 \uac15\uc5f0\uc774 \uc774\uc5b4\uc84c\ub2e4. \uc5ec\uae30\uc5d0 \u2018\uc0dd\uc131\ud615 AI \ubc0f \ub300\ud615\uc5b8\uc5b4\ubaa8\ub378(LLM)\uc758 \ucd5c\uadfc \ub3d9\ud5a5\uc774 \ucef4\ud4e8\ud305 \ud50c\ub7ab\ud3fc\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\u2019\uc5d0 \ub300\ud55c \ud328\ub110 \ud1a0\ub860\uc744 \ub354\ud574, \uad00\ub828 \uc5c5\uacc4\uc640 \uc774\ud574 \uad00\uacc4\uc790\ub4e4\uc774 \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4\uc5d0 \ub300\ud574 \ud55c\uce35 \ub354 \uc2ec\ub3c4 \uae4a\uac8c \uc774\ud574\ud560 \uc218 \uc788\ub294 \uc790\ub9ac\ub97c \ub9c8\ub828\ud588\ub2e4.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38 \ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc2dc\uc2a4\ud15cLSI \uc0ac\uc5c5\ubd80 \ubc15\uc6a9\uc778 \uc0ac\uc7a5\uc774 \ubc1c\ud45c\ub97c \ud558\uace0 \uc788\ub294 \ubaa8\uc2b5**\n_* *_\n_* *_\n\n**\u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38\ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \ubc1c\ud45c\ub97c \ub4e3\ub294 \uccad\uc911\ub4e4\uc758 \ubaa8\uc2b5**\n_* *_\n_* *_\n\n**\u25b210\uc6d4 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \uc0bc\uc131\uc804\uc790 DS\ubd80\ubb38 \ubbf8\uc8fc\ucd1d\uad04(DSA)\uc5d0\uc11c \uc5f4\ub9b0 \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019 \ud604\uc7a5",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "URL: news/40ed73bb-4b81-47ed-b2ea-96181bdaa61b/detail\nCategory: News\nTitle: \uc774\uc81c\ub294 \u2018\uc2dc\uc2a4\ud15cLSI \ud734\uba38\ub178\uc774\ub4dc\u2019\ub85c! \u2018\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\u2019\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \ucd08\uc5f0\uacb0 \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 \uae30\uc220\nContent: \u2018System LSI Humanoid is around the corner\u2019\ub77c\ub294 \uc8fc\uc81c\ub85c \ud0a4\ub178\ud2b8 \uc5f0\uc124\uc5d0 \ub098\uc120 \uc2dc\uc2a4\ud15cLSI\uc0ac\uc5c5\ubd80 \ubc15\uc6a9\uc778 \uc0ac\uc7a5\uc740 \ucd08\uc9c0\ub2a5\ud654(Hyper-Intelligence), \ucd08\uc5f0\uacb0\uc131(Hyper-Connectivity), \ucd08\ub370\uc774\ud130(Hyper-Data)\ub97c \uac00\ub2a5\ud558\uac8c \ud560 \ubbf8\ub798 \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc194\ub8e8\uc158\uacfc \ud568\uaed8 \uadf8 \uc911\uc694\uc131\uc5d0 \ub300\ud574 \uac15\uc870\ud588\ub2e4.\n<br>\n\ubc15\uc6a9\uc778 \uc0ac\uc7a5\uc740 \uc5f0\uc124\uc5d0\uc11c \u201c\uc0dd\uc131\ud615 AI\ub294 \uc11c\ube44\uc2a4 \ucd9c\uc2dc \ub450 \ub2ec \ub9cc\uc5d0 1\uc5b5 \uba85\uc758 \uc0ac\uc6a9\uc790\ub97c \uc5bb\uc744 \ub9cc\ud07c \uc911\uc694\ud55c \uae30\uc220 \ud2b8\ub79c\ub4dc\ub85c \uc790\ub9ac \uc7a1\uc558\ub2e4\u201d\uba70, \u201c\uc0bc\uc131\uc804\uc790 \ubc18\ub3c4\uccb4\ub294 \ucd5c\uace0\uc758 \uc131\ub2a5\uacfc \uc194\ub8e8\uc158\uc744 \uad6c\ud604\ud558\uae30 \uc704\ud574 \u2018\uc5d1\uc2dc\ub178\uc2a4 2400\u2019\uacfc \uac19\uc740 \ucd5c\uc2e0 SoC, \ube44\uc9c0\uc0c1\ub124\ud2b8\uc6cc\ud06c \ud1b5\uc2e0\uae30\uc220, \uc778\uac04\uc758 \uc624\uac10\uacfc \ube44\uc2b7\ud55c \uae30\ub2a5\uc744 \uac00\uc9c4 \u2018\uc778\uac04\ud615 \ubc18\ub3c4\uccb4\u2019 \uad6c\ud604 \ub4f1\uc744 \ud1b5\ud574 \uc774\ubcf4\ub2e4 \ud55c \ub2e8\uacc4 \ubc1c\uc804\ub41c \u2018\uc120\ud589\uc801 AI\u2019 \uc2dc\ub300\ub85c \ub354 \ub098\uc740 \ubbf8\ub798\ub97c \ucc3d\uc870\ud560 \uac83\uc774\ub2e4\u201d\ub77c\ub294 \uacc4\ud68d\uc744 \ubc1d\ud614\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc800\uc804\ub825 \ubc0f \uc800\uc9c0\uc5f0\uc73c\ub85c \uc0c8\ub85c\uc6b4 \uac00\ub2a5\uc131\uc744 \uc5f4\ub2e4**\n\uc774\uc5d0 \ub530\ub77c \uc2dc\uc2a4\ud15c\uc740 \uc6d0\uc2dc \ub370\uc774\ud130\uc5d0\uc11c \uc0c1\ud669 \uc815\ubcf4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc0dd\uc131\ud560 \uc218 \uc788\uc73c\uba70, \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc774\ub97c \ud65c\uc6a9\ud558\uc5ec \ub3d9\uc791\uc744 \uc870\uc815\ud560 \uc218 \uc788\ub2e4. \ubaa8\ubc14\uc77c \uce74\uba54\ub77c\uac00 \uae30\uc874 \ub514\uc9c0\ud138 \uce74\uba54\ub77c\ub97c \ub530\ub77c\uc7a1\uc744 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \uc801\uc751\ud615 \uae30\uc220\uc774 \uc8fc\ubaa9\uc744 \ubc1b\uace0 \uc788\ub2e4. \uc0bc\uc131\uc758 \uc804\uc6a9 \uc2dc\uc2a4\ud15c\uc740 \ub85c\uceec \ubaa8\uc158 \ucd94\uc815 \ubc0f \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560\uc744 \ud1b5\ud574 \uc774\ubbf8\uc9c0 \ubc0f \ube44\ub514\uc624 \ud488\uc9c8\uc744 \uac1c\uc120\ud558\ub294 \uc0c1\ud669 \uc778\uc2dd \ucc98\ub9ac \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\uba70, \uc77c\ubc18 \ud504\ub85c\uc138\uc11c\ubcf4\ub2e4 \ucd5c\ub300 5\ubc30 \uc791\uc740 \ud06c\uae30\ub85c \uc774\ub97c \uc218\ud589\ud55c\ub2e4. \uc0bc\uc131\uc758 \ucd5c\uc801\ud654\ub41c \uc2dc\uc2a4\ud15c\uc744 \ud1b5\ud574 \uc0c1\uc2dc \uc791\ub3d9 \uce74\uba54\ub77c\ub294 \uc774\uc81c \ucd5c\uc18c 30uW\uc758 \uc804\ub825\uc744 \uc18c\ube44\ud558\ubbc0\ub85c \ubc30\ud130\ub9ac \uc7ac\ucda9\uc804 \uc5c6\uc774\ub3c4 \uc77c\uc8fc\uc77c \ub0b4\ub0b4 \ube44\ub514\uc624\ub97c \uc2a4\ud2b8\ub9ac\ubc0d\ud560 \uc218 \uc788\ub2e4."
            ]
        ],
        "answer": "Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
        "ground_truth": "Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions."
    },
    {
        "question": "I would like to know the device specifications provided by 1st AI Challenger.",
        "contexts": [
            [
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _* *_\n**\u25a1 5\uc77c (\ud604\uc9c0\uc2dc\uac04), \uc751\uc6a9\ucc98\ubcc4 \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \ud604\ud669\u00b7\ube44\uc804 \ubc1c\ud45c**\n - \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\uc220 \uac1c\ubc1c \uad00\ub828 \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\n\n**\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d, \uc0dd\uc131\ud615 AI \uae30\uc220 \ud0d1\uc7ac\ud55c '\uc5d1\uc2dc\ub178\uc2a4 2400' \uacf5\uac1c**\n - \uc804\uc791 \ub300\ube44 CPU \uc131\ub2a5 1.7\ubc30, AI \uc131\ub2a5 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\n - \uc5d1\uc2dc\ub178\uc2a4 2400 \uae30\ubc18 \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \uac8c\uc784 \ud504\ub9ac\ubdf0 \uc601\uc0c1 \uacf5\uac1c\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n - \uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4 \uacf5\uac1c\u2026\ucd5c\ub300 4\ubc30 \uc90c\uc5d0\ub3c4 \ucd08\uace0\ud654\uc9c8, AI \ud2b8\ub798\ud0b9 \uae30\uc220 \uc801\uc6a9\n - \ucc28\uc138\ub300 \ucc28\ub7c9\uc6a9 \uc194\ub8e8\uc158, \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c(NTN) \uc601\uc0c1 \uc2dc\uc5f0",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\uac00 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \ubbf8\uc8fc\ucd1d\uad04(Device Solutions America office)\uc5d0\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023'\uc744 \uac1c\ucd5c\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \ucd08\uc9c0\ub2a5\ud654(Hyper-Intelligence)\u00b7\ucd08\uc5f0\uacb0\uc131(Hyper-Connected)\u00b7\ucd08\ub370\uc774\ud130(Hyper-Data)\ub97c \uac00\ub2a5\ud558\uac8c \ud560 \uc8fc\uc694 \uc751\uc6a9\ucc98\ubcc4 \ucd5c\uc2e0 \ubc18\ub3c4\uccb4 \uc124\uacc4 \ud604\ud669\uacfc \ube44\uc804\uc744 \uacf5\uc720\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \uace0\uac1d\uc0ac\uc640 \ud30c\ud2b8\ub108\uc0ac \uad00\uacc4\uc790 300\uc5ec\uba85\uc774 \ucc38\uc11d\ud55c \uac00\uc6b4\ub370, \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \ubd84\uc57c \uae00\ub85c\ubc8c \uc804\ubb38\uac00, \uc11d\ud559\ub4e4\uacfc \ud568\uaed8 \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM, Large Language Model) \uae30\uc220\uc5d0 \ub300\ud55c \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\ub97c \uc9c4\ud589\ud588\ub2e4.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-2\nContent:\n| DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| AVGPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\n| MAXPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\nFor `AVGPOOL` and `MAXPOOL`, when the kernel size is larger than the supported size, it is split into multiple pooling layers as the origin.",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-1\nContent:\n| Operator_Name | Kernel Size | Padding | Strides | Dilation | Boost Mode |\n| ------------------------------ | ----------- | -------- | -------- | --------- | ---------- |\n| CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | {1 ~ 27} |\n| DECONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DECONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc911\uc559 \ucef4\ud4e8\ud305\uc5d0 \ub300\ud55c \uc218\uc694\uac00 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ud558\ub4dc\uc6e8\uc5b4\uc640 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \ud655\uc7a5\uc131\uc774 \ub354\uc6b1 \uc911\uc694\ud574\uc9c8 \uac83\uc774\ub2e4. \ub530\ub77c\uc11c \uc0bc\uc131\uc758 \ud575\uc2ec \uc194\ub8e8\uc158 \uc911 \ud558\ub098\ub294 \uc18c\ud504\ud2b8\uc6e8\uc5b4\ub97c \uc218\uc815\ud558\uc9c0 \uc54a\uace0\ub3c4 \ucef4\ud4e8\ud305 \uc6a9\ub7c9\uc744 \ub450 \ubc30\ub85c \ub298\ub9ac\uae30 \uc704\ud574 \ub450 SoC \uac04\uc758 \uc9c1\uc811 \uc5f0\uacb0(die-to-die)\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. \ub610\ud55c \uc624\ud1a0 SoC\ub294 OEM\uc758 \uc2dc\uc2a4\ud15c\uc5d0 \ub530\ub77c \ud328\ud0a4\uc9c0 \uac04 \uba40\ud2f0 SoC \uc5f0\uacb0\uc744 \uc704\ud574 PCIe \ub610\ub294 \uc774\ub354\ub137 \uc0ac\uc6a9\uc744 \uc9c0\uc6d0\ud55c\ub2e4.\n_*AI \ud1b5\ud569, \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc815\uc758 \ucc28\ub7c9\uc758 \ubbf8\ub798\ub97c \uc704\ud55c \uc900\ube44 *_\n\uc55e\uc73c\ub85c \uc6b4\uc804\uc790\ub294 \ucd5c\ub300 150\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uc2e4\uc2dc\uac04\uc73c\ub85c \uc791\ub3d9\ud560 \uc218 \uc788\ub294 \uc0bc\uc131\uc758 \uc804\uc6a9 AI \uac00\uc18d\uae30\ub97c \ud1b5\ud574 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0 \uae30\ubc18\ud55c AI \ube44\uc11c\uc640 \uc6d0\ud65c\ud558\uac8c \uc0c1\ud638 \uc791\uc6a9\ud560 \uc218 \uc788\uac8c \ub420 \uac83\uc774\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uba40\ud2f0\ubbf8\ub514\uc5b4\uc758 \uae30\uc220\uc758 \ubc1c\uc804\uc73c\ub85c \ud734\uba38\ub178\uc774\ub4dc\uac00 \uc778\uac04\ucc98\ub7fc \uc790\uc5f0\uc2a4\ub7fd\uac8c \uac10\uc9c0\ud558\uace0, \uc778\uc9c0\ud558\uace0, \ud589\ub3d9\ud558\ub294 \ubbf8\ub798\uc5d0 \ub354 \uac00\uae4c\uc6cc\uc9c0\uace0 \uc788\ub2e4. \ud64d \uc0c1\ubb34\ub294 \"\uc774\ub7ec\ud55c \ubbf8\ub798\ub97c \uc2e4\ud604\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc544\uc9c1 \ud604\uc2e4\uacfc\uc758 \uae30\uc220\uc801\uc778 \uaca9\ucc28\ub97c \uc881\ud600\uc57c \ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac \uc2dc\uc2a4\ud15c \uae30\uc220\ub85c \ubab0\uc785\uac10 \ub118\uce58\uace0 \uc778\ud130\ub799\ud2f0\ube0c\ud55c \ubbf8\ub798\ub97c \uad6c\ud604\ud560 \uac83\u201d\uc774\ub77c\uace0 \ud655\uc2e0\ud588\ub2e4.\n\n1 MobileBERT: \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc758 \ubaa8\ubc14\uc77c \ubc84\uc804\n2 AP: Application Processor\n3 \ucef4\ud4e8\ud305\uc5d0\uc11c \uc0c1\ud669 \uc778\uc9c0\ub294 \uc2dc\uc2a4\ud15c/\uad6c\uc131 \uc694\uc18c\uac00 \ud658\uacbd\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc218\uc9d1\ud558\uace0 \uadf8\uc5d0 \ub530\ub77c \ub3d9\uc791\uc744 \uc870\uc815\ud558\ub294 \uae30\ub2a5\uc744 \ub9d0\ud568\n ",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc544\ud0a4\ud14d\ucc98 \ubcc0\uacbd\uc758 \uacb0\uacfc\ub85c MobileBERT1 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc740 \uc774\uc804 \uc138\ub300\uc5d0 \ube44\ud574 3\ubc30 \ud5a5\uc0c1\ub418\uc5c8\ub2e4\n**\ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc704\ud55c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC**\n\uc790\ub3d9\ucc28\ub294 \ud604\uc7ac \uc804\ud654\uac00 \uc2a4\ub9c8\ud2b8\ud3f0\uc73c\ub85c \uc9c4\ud654\ud55c \uac83\uacfc \uc720\uc0ac\ud558\uac8c \uae09\uaca9\ud558\uac8c \ubcc0\ud654\ud558\uace0 \uc788\ub2e4. \uc2e4\uc81c\ub85c \ud604\ub300\uc758 \uc790\ub3d9\ucc28\ub294 \ub2e8\uc21c\ud55c \uc774\ub3d9 \uc218\ub2e8\uc744 \ub118\uc5b4 \uc0dd\uc131\ud615 AI\uc640 \uac19\uc740 \uad11\ubc94\uc704\ud55c \ucef4\ud4e8\ud305 \uae30\ub2a5\uc758 \uc9d1\ud569\uccb4\uac00 \ub418\uc5c8\ub2e4.\n\uc774\ub7ec\ud55c \ud604\uc2e4\uc744 \ubc30\uacbd\uc73c\ub85c \uc0bc\uc131\uc804\uc790 AP2 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uac1c\ubc1c\ud300 \ubc29\uc9c0\ud6c8 \uc0c1\ubb34\ub294 \uc0bc\uc131 \uc624\ud1a0 SoC \uc194\ub8e8\uc158\uacfc \ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ub300\ud55c \uc138\uc158\uc744 \uc9c4\ud589\ud588\ub2e4.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \ud5a5\ud6c4 \ubcf4\uc548 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \ubaa8\ub4e0 \uac1c\ubc1c\uc740 ISO 21434 \ud45c\uc900\uc744 \ub530\ub97c \uc608\uc815\uc774\ub2e4.\n**\ud55c \uce35 \ub192\uc544\uc9c4 \ud655\uc7a5\uc131**\n\ud655\uc7a5\uc131 \uce21\uba74\uc5d0\uc11c \uc0bc\uc131\uc740 \uac01 \uc790\ub3d9\ucc28 \ub3c4\uba54\uc778\uc758 \uc99d\uac00\ud558\ub294 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc694\uad6c \uc0ac\ud56d\uc744 \ucda9\uc871\ud558\uae30 \uc704\ud574 \ub3c5\uc790\uc801\uc73c\ub85c Type-1 \ud558\uc774\ud37c\ubc14\uc774\uc800\ub97c \uac1c\ubc1c\ud588\ub2e4. \uc774 \ud558\uc774\ud37c\ubc14\uc774\uc800\ub294 \uc131\ub2a5 \uc800\ud558 \uc5c6\uc774 \ub2e4\uc591\ud55c OS\ub97c \uac00\uc0c1\ud654 \ud560 \uc218 \uc788\uc73c\uba70 \uc5c5\uacc4 \ud45c\uc900 API\uc778 VirtIO\ub97c \uc9c0\uc6d0\ud55c\ub2e4. \ub610\ud55c \ud2b9\uc815 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc5d0 \uad6d\ud55c\ub418\uc9c0 \uc54a\uace0 \uc11c\ub4dc\ud30c\ud2f0 \ud558\uc774\ud37c\ubc14\uc774\uc800\ub97c \uc9c0\uc6d0\ud55c\ub2e4.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\ub2a5\uc744 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub3c4\uc785\ud558\uae30 \uc704\ud55c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 IP\uac1c\ubc1c \ubc29\ud5a5\uc131\uc744 \uc18c\uac1c\ud588\ub2e4.\n**\ucd5c\uc801\ud654\ub41c 4\ud074\ub7ec\uc2a4\ud130 \uad6c\uc870\uc758 CPU**\n\uc138 \uac1c\uc758 \uc8fc\uc694 IP \uc911 \uccab \ubc88\uc9f8\ub294 CPU\ub85c, \uae30\uc874\uc5d0\ub294 \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud558\uc774\uc5d4\ub4dc(high-end) \ud074\ub7ec\uc2a4\ud130, \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ub85c\uc6b0\uc5d4\ub4dc(low-end) \ud074\ub7ec\uc2a4\ud130, \uc911\uac04 \ud074\ub7ec\uc2a4\ud130 \ub4f1 \uc138 \uac1c\uc758 \ud074\ub7ec\uc2a4\ud130\ub85c \uad6c\uc131\ub418\uc5c8\ub2e4. \uc0bc\uc131\uc740 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 CPU \uc6b4\uc601\uc744 \uc704\ud574 \uc911\uac04 \ud074\ub7ec\uc2a4\ud130\ub97c \ubbf8\ub4dc \ud558\uc774(mid-high)\uc640 \ubbf8\ub4dc \ub85c\uc6b0(mid-low)\uc758 \ub450 \uac00\uc9c0 \uacc4\uce35\uc73c\ub85c \ub098\ub204\uace0, \ubbf8\ub4dc \ud558\uc774 \uacc4\uce35\uc740 \uac8c\uc784\uacfc \uac19\uc740 \ucef4\ud4e8\ud305 \uc9d1\uc57d\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud588",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub610\ud55c \uc790\ub3d9\ucc28 \uae30\uc220\uc774 \ub354\uc6b1 \ubc1c\uc804\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub294 \uc0c1\ud669\uc5d0\uc11c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uac01 \uc601\uc5ed\uc758 \uc5ec\ub7ec \uc774\uae30\uc885 OS\ub97c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ud560 \uc218 \uc788\uc5b4 \ud68d\uae30\uc801\uc778 \ubbf8\ub798\ub97c \uc900\ube44\ud560 \uc218 \uc788\ub2e4. \ubc29 \uc0c1\ubb34\ub294 \"\uc0bc\uc131 \uc624\ud1a0 SoC\uc758 \uc911\uc559 \ucef4\ud4e8\ud305 \uae30\ub2a5 \ub355\ubd84\uc5d0 \ud55c\ub54c \uc0c1\uc0c1 \uc18d\uc5d0\ub9cc \uc874\uc7ac\ud588\ub358 \uac83\ub4e4\uc774 \uace7 \ud604\uc2e4\uc774 \ub420 \uac83\"\uc774\ub77c\uba70 \ubbf8\ub798\uc5d0 \ub300\ud55c \ubc1d\uc740 \uc804\ub9dd\uc744 \ub4dc\ub7ec\ub0c8\ub2e4.\n**\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158**\n\ub354\uc6b1 \uac15\ub825\ud55c SoC\uc640 \uc13c\uc11c \uae30\ub2a5\uc774 \uac1c\ubc1c\ub428\uc5d0 \ub530\ub77c \ucd5c\uc2e0 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc810\uc810 \ub354 \uc2a4\ub9c8\ud2b8\ud574\uc9c0\uace0 \ub192\uc740 \uc5f0\uc0b0 \ub2a5\ub825\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc694\uac74\uc744 \ucda9\uc871\ud558\uace0 \uc778\uac04\uc758 \ud589\ub3d9\uc744 \uc644\ubcbd\ud558\uac8c \ubaa8\ubc29\ud55c\ub2e4\ub294 \uad81\uadf9\uc801\uc778 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 \uc0bc\uc131\uc740 \uc800\uc9c0\uc5f0\uacfc \uc800\uc804\ub825\uc744 \ub450 \uac00\uc9c0 \ud575\uc2ec \ubc1c\uc804 \ubd84\uc57c\ub85c \uc120\uc815\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uac1c\ubc1c\ud300 \ud64d\uae30\uc900 \uc0c1\ubb34\ub294 \u201c\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\u201d\uc774\ub77c\ub294 \uc81c\ubaa9\uc758 \uc138\uc158\uc5d0\uc11c \uc774 \ub450 \uac00\uc9c0 \ud575\uc2ec \ubd84\uc57c\ub97c \uc5b4\ub5bb\uac8c \ubc1c\uc804\uc2dc\ucf1c\uc654\ub294\uc9c0 \uc124\uba85\ud588\ub2e4.\n**\uc0c1\ud669 \uc778\uc9c0 \ucef4\ud4e8\ud305**\n\ub2e4\uc591\ud55c \uae30\ub2a5\uc774 \ud0d1\uc7ac\ub41c \ucd5c\uc2e0 \ub514\ubc14\uc774\uc2a4 \uc911 \uc0c1\ub2f9\uc218\ub294 \uc0c1\ud669 \uc778\uc9c0 \ucef4\ud4e8\ud3053\uc73c\ub85c \uad6c\ud604\ub418\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud55c\ub2e4. \uc0c1\ud669 \uc778\uc9c0\ub294 \uc801\uc678\uc120 \uc13c\uc2f1\uc5d0\uc11c \uc2dc\uc791\ub418\uc5c8\uc9c0\ub9cc, \ucd5c\uc2e0 \uc13c\uc11c\ub294 \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 \uc0c8\ub85c\uc6b4 \uac10\uac01 \ub370\uc774\ud130\uc758 \uc138\uacc4\ub97c \uc5f4\uc5b4 \uc8fc\uc5c8",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.segmentation.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"deeplabv3.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_H = INPUT_SIZE_H\n const val OUTPUT_SIZE_W = INPUT_SIZE_W\n const val OUTPUT_SIZE_C = 21\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\ub294 2\uc5b5 \ud654\uc18c \uc774\ubbf8\uc9c0\uc13c\uc11c \uae30\ubc18 \ucd08\uace0\ud574\uc0c1\ub3c4 \ud2b9\uc218 \uc90c \uae30\uc220\uc778 '\uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4(Zoom Anyplace)'\ub97c \ucc98\uc74c \uacf5\uac1c\ud588\ub2e4. \uc774 \uae30\uc220\uc740 \uc6c0\uc9c1\uc774\ub294 \uc0ac\ubb3c\uc5d0 \ub300\ud574 \ud480\uc2a4\ud06c\ub9b0\uacfc \ucd5c\ub300 4\ubc30 \ud074\ub85c\uc988\uc5c5 \uc7a5\uba74\uae4c\uc9c0 \ud654\uc9c8 \uc800\ud558\uc5c6\uc774 \ub3d9\uc2dc\uc5d0 \ucd2c\uc601\ud560 \uc218 \uc788\uace0, \ud074\ub85c\uc988\uc5c5\uc2dc AI \uae30\uc220\ub85c \uc0ac\ubb3c\uc744 \uc790\ub3d9 \ucd94\uc801\ud560 \uc218 \uc788\uc5b4 \ubaa8\ubc14\uc77c \uc0ac\uc6a9\uc790\uc5d0\uac8c \uc644\uc804\ud788 \uc0c8\ub85c\uc6b4 \uce74\uba54\ub77c \uc90c \uacbd\ud5d8\uc744 \uc81c\uacf5\ud55c\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 2025\ub144 \uc591\uc0b0 \uc608\uc815\uc778 \ucc28\uc138\ub300 \ud504\ub9ac\ubbf8\uc5c4 \uc778\ud3ec\ud14c\uc778\uba3c\ud2b8(IVI, In-Vehicle Infotainment)\uc6a9 \ud504\ub85c\uc138\uc11c '\uc5d1\uc2dc\ub178\uc2a4 \uc624\ud1a0 V920' \uad6c\ub3d9 \uc601\uc0c1\uc744 \uacf5\uac1c\ud588\ub2e4."
            ]
        ],
        "answer": "The 1st AI Challenger provided the Exynos 2200 chipset, consisting of a CPU configuration with Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, along with the Samsung Xclipse 920 GPU. Additionally, it's equipped with a Dual-core NPU and DSP for AI operations. The camera capabilities include support for up to 200MP in single-camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
        "ground_truth": "The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz."
    },
    {
        "question": "I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.",
        "contexts": [
            [
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I want to know the main summary of Samsung Exynos 2200.\nAnswer: Samsung Electronics introduces the Exynos 2200, a premium mobile processor featuring the Samsung Xclipse GPU based on AMD RDNA 2 architecture, crafted using a 4-nanometer EUV process. This processor enhances mobile gaming, social media, and photography, offering the first mobile hardware-accelerated ray tracing for realistic lighting and variable rate shading for smoother gameplay. It boasts an upgraded neural processing unit (NPU), Arm's latest Armv9 CPU cores for superior performance and security, and a fast 5G modem compliant with 3GPP Release 16. With integrated Secure Element (iSE) and robust encryption, it ensures data security. The processor's ISP supports up to 200MP resolution, AI-integrated camera functionalities for professional-quality images, an advanced codec for up to 8K video, and a display solution with HDR10+ and up to 144Hz refresh rates, ensuring a comprehensive and immersive visual experience.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "Question: I want to know what types of smartphones were mass-produced based on the Exynos 2200.\nAnswer: Smartphones mass-produced based on the Exynos 2200 chipset include Samsung Galaxy S22 Ultra, Samsung Galaxy S22+, and Samsung Galaxy S22.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\ub2a5\uc744 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub3c4\uc785\ud558\uae30 \uc704\ud55c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 IP\uac1c\ubc1c \ubc29\ud5a5\uc131\uc744 \uc18c\uac1c\ud588\ub2e4.\n**\ucd5c\uc801\ud654\ub41c 4\ud074\ub7ec\uc2a4\ud130 \uad6c\uc870\uc758 CPU**\n\uc138 \uac1c\uc758 \uc8fc\uc694 IP \uc911 \uccab \ubc88\uc9f8\ub294 CPU\ub85c, \uae30\uc874\uc5d0\ub294 \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud558\uc774\uc5d4\ub4dc(high-end) \ud074\ub7ec\uc2a4\ud130, \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ub85c\uc6b0\uc5d4\ub4dc(low-end) \ud074\ub7ec\uc2a4\ud130, \uc911\uac04 \ud074\ub7ec\uc2a4\ud130 \ub4f1 \uc138 \uac1c\uc758 \ud074\ub7ec\uc2a4\ud130\ub85c \uad6c\uc131\ub418\uc5c8\ub2e4. \uc0bc\uc131\uc740 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 CPU \uc6b4\uc601\uc744 \uc704\ud574 \uc911\uac04 \ud074\ub7ec\uc2a4\ud130\ub97c \ubbf8\ub4dc \ud558\uc774(mid-high)\uc640 \ubbf8\ub4dc \ub85c\uc6b0(mid-low)\uc758 \ub450 \uac00\uc9c0 \uacc4\uce35\uc73c\ub85c \ub098\ub204\uace0, \ubbf8\ub4dc \ud558\uc774 \uacc4\uce35\uc740 \uac8c\uc784\uacfc \uac19\uc740 \ucef4\ud4e8\ud305 \uc9d1\uc57d\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud588",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub610\ud55c \uc790\ub3d9\ucc28 \uae30\uc220\uc774 \ub354\uc6b1 \ubc1c\uc804\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub294 \uc0c1\ud669\uc5d0\uc11c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uac01 \uc601\uc5ed\uc758 \uc5ec\ub7ec \uc774\uae30\uc885 OS\ub97c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ud560 \uc218 \uc788\uc5b4 \ud68d\uae30\uc801\uc778 \ubbf8\ub798\ub97c \uc900\ube44\ud560 \uc218 \uc788\ub2e4. \ubc29 \uc0c1\ubb34\ub294 \"\uc0bc\uc131 \uc624\ud1a0 SoC\uc758 \uc911\uc559 \ucef4\ud4e8\ud305 \uae30\ub2a5 \ub355\ubd84\uc5d0 \ud55c\ub54c \uc0c1\uc0c1 \uc18d\uc5d0\ub9cc \uc874\uc7ac\ud588\ub358 \uac83\ub4e4\uc774 \uace7 \ud604\uc2e4\uc774 \ub420 \uac83\"\uc774\ub77c\uba70 \ubbf8\ub798\uc5d0 \ub300\ud55c \ubc1d\uc740 \uc804\ub9dd\uc744 \ub4dc\ub7ec\ub0c8\ub2e4.\n**\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158**\n\ub354\uc6b1 \uac15\ub825\ud55c SoC\uc640 \uc13c\uc11c \uae30\ub2a5\uc774 \uac1c\ubc1c\ub428\uc5d0 \ub530\ub77c \ucd5c\uc2e0 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc810\uc810 \ub354 \uc2a4\ub9c8\ud2b8\ud574\uc9c0\uace0 \ub192\uc740 \uc5f0\uc0b0 \ub2a5\ub825\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _* *_\n**\u25a1 5\uc77c (\ud604\uc9c0\uc2dc\uac04), \uc751\uc6a9\ucc98\ubcc4 \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \ud604\ud669\u00b7\ube44\uc804 \ubc1c\ud45c**\n - \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\uc220 \uac1c\ubc1c \uad00\ub828 \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\n\n**\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d, \uc0dd\uc131\ud615 AI \uae30\uc220 \ud0d1\uc7ac\ud55c '\uc5d1\uc2dc\ub178\uc2a4 2400' \uacf5\uac1c**\n - \uc804\uc791 \ub300\ube44 CPU \uc131\ub2a5 1.7\ubc30, AI \uc131\ub2a5 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\n - \uc5d1\uc2dc\ub178\uc2a4 2400 \uae30\ubc18 \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \uac8c\uc784 \ud504\ub9ac\ubdf0 \uc601\uc0c1 \uacf5\uac1c\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n - \uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4 \uacf5\uac1c\u2026\ucd5c\ub300 4\ubc30 \uc90c\uc5d0\ub3c4 \ucd08\uace0\ud654\uc9c8, AI \ud2b8\ub798\ud0b9 \uae30\uc220 \uc801\uc6a9\n - \ucc28\uc138\ub300 \ucc28\ub7c9\uc6a9 \uc194\ub8e8\uc158, \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c(NTN) \uc601\uc0c1 \uc2dc\uc5f0",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc544\ud0a4\ud14d\ucc98 \ubcc0\uacbd\uc758 \uacb0\uacfc\ub85c MobileBERT1 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc740 \uc774\uc804 \uc138\ub300\uc5d0 \ube44\ud574 3\ubc30 \ud5a5\uc0c1\ub418\uc5c8\ub2e4\n**\ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc704\ud55c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC**\n\uc790\ub3d9\ucc28\ub294 \ud604\uc7ac \uc804\ud654\uac00 \uc2a4\ub9c8\ud2b8\ud3f0\uc73c\ub85c \uc9c4\ud654\ud55c \uac83\uacfc \uc720\uc0ac\ud558\uac8c \uae09\uaca9\ud558\uac8c \ubcc0\ud654\ud558\uace0 \uc788\ub2e4. \uc2e4\uc81c\ub85c \ud604\ub300\uc758 \uc790\ub3d9\ucc28\ub294 \ub2e8\uc21c\ud55c \uc774\ub3d9 \uc218\ub2e8\uc744 \ub118\uc5b4 \uc0dd\uc131\ud615 AI\uc640 \uac19\uc740 \uad11\ubc94\uc704\ud55c \ucef4\ud4e8\ud305 \uae30\ub2a5\uc758 \uc9d1\ud569\uccb4\uac00 \ub418\uc5c8\ub2e4.\n\uc774\ub7ec\ud55c \ud604\uc2e4\uc744 \ubc30\uacbd\uc73c\ub85c \uc0bc\uc131\uc804\uc790 AP2 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uac1c\ubc1c\ud300 \ubc29\uc9c0\ud6c8 \uc0c1\ubb34\ub294 \uc0bc\uc131 \uc624\ud1a0 SoC \uc194\ub8e8\uc158\uacfc \ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ub300\ud55c \uc138\uc158\uc744 \uc9c4\ud589\ud588\ub2e4.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _*\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d\uacfc \uc0dd\uc131\ud615 AI \uae30\uc220\ub85c \uc0ac\uc6a9\uc790 \uacbd\ud5d8 \uadf9\ub300\ud654\ud55c \uc5d1\uc2dc\ub178\uc2a4 2400 *_\n_* *_\n\uc0bc\uc131\uc804\uc790\ub294 \uc774\ubc88 \ud589\uc0ac\uc5d0\uc11c AMD(Advanced Micro Devices)\uc758 \ucd5c\uc2e0 \uc544\ud0a4\ud14d\uccd0 RDNA3 \uae30\ubc18 \uc5d1\uc2a4\ud074\ub9bd\uc2a4 940(Xclipse 940) \uadf8\ub798\ud53d \ucc98\ub9ac\uc7a5\uce58(GPU)\ub97c \ud0d1\uc7ac\ud55c \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \ud504\ub85c\uc138\uc11c '\uc5d1\uc2dc\ub178\uc2a4(Exynos) 2400'\uc744 \uacf5\uac1c\ud588\ub2e4.\n_* *_\n'\uc5d1\uc2dc\ub178\uc2a4 2400'\uc740 \uc804\uc791\uc778 '\uc5d1\uc2dc\ub178\uc2a4 2200' \ub300\ube44 CPU \uc131\ub2a5\uc740 1.7\ubc30, AI \uc131\ub2a5\uc740 \uc9c0\ub09c 2\ub144\uac04 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\ub410\ub2e4.\n\n\uc0bc\uc131\uc804\uc790\ub294 \uc774 \uc81c\ud488\uc5d0 \ub354\uc6b1 \ud5a5\uc0c1\ub41c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(Ray Tracing)\uacfc \ud568\uaed8 \uae00\ub85c\ubc8c \uc77c\ub8e8\ubbf8\ub124\uc774\uc158(Global Illumination), \ub9ac\ud50c\ub809\uc158/\uc250\ub3c4\uc6b0 \ub80c\ub354\ub9c1(Shadow Rendering) \ub4f1 \ub2e4\uc591\ud55c \ucca8\ub2e8 \uadf8\ub798\ud53d \uae30\uc220\uc744 \ud0d1\uc7ac\ud574 \uace0\uc131\ub2a5 \uac8c\uc784\uc744 \uc990\uae30\ub294 \uc720\uc800\ub4e4\uc5d0\uac8c \ucd5c\uace0\uc758 \uc0ac\uc6a9\uc790 \uacbd\ud5d8(User Experience)\ub97c \uc81c\uacf5\ud560 \uacc4\ud68d\uc774\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uc774 \uac1c\ub150\uc740 \uc13c\uc11c\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\uace0, \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uace0, \ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790\uc5d0\uac8c \ub354 \ub098\uc740 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud558\ub294 \uc138 \ub2e8\uacc4\ub85c \uc138\ubd84\ud654\ud560 \uc218 \uc788\ub2e4.\n\ubaa8\ub4e0 \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub824\uba74 \ub9ce\uc740 \uc5f0\uc0b0\uc774 \ud544\uc694\ud558\ubbc0\ub85c \uc804\ub825 \uc18c\ube44\uac00 \uc99d\uac00\ud55c\ub2e4. \ubc14\ub85c \uc774 \ubd80\ubd84\uc5d0\uc11c \uc0bc\uc131\uc758 \uae30\uc220\uc801 \ud601\uc2e0\uc774 \ube5b\uc744 \ubc1c\ud55c\ub2e4. \uc0bc\uc131\uc740 \uc804\uc6a9 \ub3c4\uba54\uc778\ubcc4 \uc2dc\uc2a4\ud15c\uc744 \ud3ec\ud568\ud558\ub294 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\ub97c \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub294 \ub354 \uac04\ub2e8\ud558\uba74\uc11c \ud2b9\uc218\ud55c \uc5f0\uc0b0 \ubd80\ud558\ub97c \ucc98\ub9ac\ud558\uae30 \ub54c\ubb38\uc5d0 \uae30\ubcf8\uc801\uc73c\ub85c \ud6a8\uc728\uc131\uc774 \ub192\ub2e4. \uc774 \uc2dc\uc2a4\ud15c\uc740 \uc804\ub825 \uc18c\ube44\uac00 \uc801\uace0 \ucc98\ub9ac \uc9c0\uc5f0 \uc2dc\uac04\uc774 \uc9e7\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uacfc\uc911\ud55c \uac8c\uc784 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc804\ub825\uacfc \uc131\ub2a5\uc744 \ub354\uc6b1 \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 CPU \uc804\ub825 \ubd80\ubd84\uc744 \uc904\uc774\uace0 \ub354 \ub9ce\uc740 \uc804\ub825\uc744 CPU \ucf54\uc5b4\uc5d0\uc11c GPU\ub85c \uc7ac\ud560\ub2f9\ud558\uc5ec \ubaa8\ubc14\uc77c \uc7a5\uce58\uc5d0\uc11c \ucd5c\uace0 \uc218\uc900\uc758 \uac8c\uc784 \uadf8\ub798\ud53d\uc744 \uc9c0\uc6d0\ud560 \uc218 \uc788\ub3c4\ub85d \ucef4\ud4e8\ud305 \uc131\ub2a5\uc744 \ub192\uc600\ub2e4.\n**\ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub418\ub294 \ud48d\ubd80\ud55c \uadf8\ub798\ud53d \uacbd\ud5d8**\n\uc0bc\uc131\uc740 \ud504\ub9ac\ubbf8\uc5c4\ubd80\ud130 \uc800\uac00\ud615\uc5d0 \uc774\ub974\ub294 \ub113\uc740 \ubc94\uc704\uc758 \ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0 \ucf58\uc194 \uc218\uc900\uc758 \uac8c\uc784\uc744 \uc81c\uacf5\ud55c\ub2e4\ub294 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 AMD\uc758 RDNATM \uc544\ud0a4\ud14d\ucc98\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 Xclipse GPU\ub97c \uac1c\ubc1c\ud574\uc654\uace0, \uc774\ub97c \ud1b5\ud574 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(ray tracing) \uae30\ub2a5\uc744 \ucd5c\ucd08\ub85c \ub3c4\uc785\ud588\ub2e4",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc0dd\uc131\ud615 \uc778\uacf5\uc9c0\ub2a5(AI) \ubc0f \uac01\uc885 \ucca8\ub2e8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ub4f1\uc7a5\uc73c\ub85c \uc778\ud574 \uc2dc\uc7a5\uc5d0\uc11c\ub294 \ud6a8\uc728\uc131, \ud655\uc7a5\uc131, \uc800\uc804\ub825\uc744 \uac16\ucd98 \ub354\uc6b1 \uc2a4\ub9c8\ud2b8\ud558\uace0 \uac15\ub825\ud55c \ucef4\ud4e8\ud305\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4. \uc774\uc5d0 \ub530\ub77c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\uc758 \uc8fc\uc694 \ud14c\ub9c8 \uc911 \ud558\ub098\uc778 \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\ub97c \uc8fc\uc81c\ub85c \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220, \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158, \uc0bc\uc131 \uc624\ud1a0 SoC(System-on-Chips) \ub4f1 \uc138 \uac1c\uc758 \uac15\uc5f0\uc774 \uc9c4\ud589\ub418\uc5c8\ub2e4.\n**\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220**\n\uccab\ubc88\uc9f8 \uc138\uc158\uc778 \"\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220\"\uc740 \uc0bc\uc131 \uc624\uc2a4\ud2f4 \uc5f0\uad6c\uc13c\ud130 \ubca0\ub2c8 \uce74\ud2f0\ube44\uc548(Benny Katibian) \ubd80\uc0ac\uc7a5\uc774 \uc9c4\ud589\ud588\ub2e4",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc5d1\uc2dc\ub178\uc2a4 \ub808\ud37c\ub7f0\uc2a4 \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub41c \ubaa8\ubc14\uc77c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5 \ub370\ubaa8 \uc601\uc0c1\uc744 \ud1b5\ud574 \uadf8\ub9bc\uc790, \ubc18\uc0ac \ub4f1\uacfc \uac19\uc740 \uc5ec\ub7ec \uac1c\uc758 \uc138\ubd80 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5\ub4e4\uc774 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.\n**\uc0dd\uc131\ud615 AI \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 NPU**\n\uc0bc\uc131\uc758 \ucd5c\uc2e0 NPU \uc194\ub8e8\uc158\uc740 \uba54\ubaa8\ub9ac \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcc0\uacbd\ud558\uace0 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \ube44\uc120\ud615 \uc5f0\uc0b0\uc758 \ud65c\uc6a9\ub3c4\ub97c \ud06c\uac8c \ub192\uc77c \uac83\uc774\ub2e4",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## The ENN SDK\nThe ENN SDK tool facilitates the conversion of TensorFlow Lite neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the Samsung Exynos hardware to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of the Sample Native Program\nThe sample native program accepts a model file and raw input data files as parameters.\nIf golden output data files are provided, the program compares these files and displays the result. If no file is provided, the program generates a raw output file.\n### Model and Data Used\nIn this sample native program, a converted NNC file and raw input/output file available in the Github Repository are used.\n### Using Native Program\nTo execute the sample native program, refer to README file.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc911\uc559 \ucef4\ud4e8\ud305\uc5d0 \ub300\ud55c \uc218\uc694\uac00 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ud558\ub4dc\uc6e8\uc5b4\uc640 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \ud655\uc7a5\uc131\uc774 \ub354\uc6b1 \uc911\uc694\ud574\uc9c8 \uac83\uc774\ub2e4. \ub530\ub77c\uc11c \uc0bc\uc131\uc758 \ud575\uc2ec \uc194\ub8e8\uc158 \uc911 \ud558\ub098\ub294 \uc18c\ud504\ud2b8\uc6e8\uc5b4\ub97c \uc218\uc815\ud558\uc9c0 \uc54a\uace0\ub3c4 \ucef4\ud4e8\ud305 \uc6a9\ub7c9\uc744 \ub450 \ubc30\ub85c \ub298\ub9ac\uae30 \uc704\ud574 \ub450 SoC \uac04\uc758 \uc9c1\uc811 \uc5f0\uacb0(die-to-die)\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. \ub610\ud55c \uc624\ud1a0 SoC\ub294 OEM\uc758 \uc2dc\uc2a4\ud15c\uc5d0 \ub530\ub77c \ud328\ud0a4\uc9c0 \uac04 \uba40\ud2f0 SoC \uc5f0\uacb0\uc744 \uc704\ud574 PCIe \ub610\ub294 \uc774\ub354\ub137 \uc0ac\uc6a9\uc744 \uc9c0\uc6d0\ud55c\ub2e4.\n_*AI \ud1b5\ud569, \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc815\uc758 \ucc28\ub7c9\uc758 \ubbf8\ub798\ub97c \uc704\ud55c \uc900\ube44 *_\n\uc55e\uc73c\ub85c \uc6b4\uc804\uc790\ub294 \ucd5c\ub300 150\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uc2e4\uc2dc\uac04\uc73c\ub85c \uc791\ub3d9\ud560 \uc218 \uc788\ub294 \uc0bc\uc131\uc758 \uc804\uc6a9 AI \uac00\uc18d\uae30\ub97c \ud1b5\ud574 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0 \uae30\ubc18\ud55c AI \ube44\uc11c\uc640 \uc6d0\ud65c\ud558\uac8c \uc0c1\ud638 \uc791\uc6a9\ud560 \uc218 \uc788\uac8c \ub420 \uac83\uc774\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc548\uc804\uc5d0 \ub300\ud55c \uc0bc\uc131\uc758 \ud655\uace0\ud55c \uc758\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ucc28\uc138\ub300 \uc624\ud1a0 SoC\uc758 \uc138\uc774\ud504\ud2f0 \uc544\uc77c\ub79c\ub4dc(safety island)\ub294 ASIL-D\ub97c \uc900\uc218\ud558\uba70, \ud638\uc2a4\ud2b8 CPU\uc640 \ubcc4\ub3c4\ub85c \uc791\ub3d9\ud558\uc5ec \ub2e4\ub978 SoC\uc758 \uc0c1\ud0dc\ub97c \ubaa8\ub2c8\ud130\ub9c1\ud560 \uc608\uc815\uc774\ub2e4. \uc774\uc678\uc758 \ubd80\ubd84\uc740 ASPICE, ISO 26262 \ubc0f FMEA\uc640 \uac19\uc740 \uc790\ub3d9\ucc28 \ud45c\uc900\uc744 \ud1b5\ud574 ASIL-B\ub97c \uc900\uc218\ud558\ub3c4\ub85d \uc81c\uc791\ub418\uc5c8\ub2e4.\n\uc0ac\uc774\ubc84 \ubcf4\uc548 \ub610\ud55c \uc810\uc810 \ub354 \uc911\uc694\ud55c \uc694\uc18c\ub85c \ub5a0\uc624\ub974\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0, \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uc554\ud638\ud654 \uc5d4\uc9c4\uc774 \ub0b4\uc7a5\ub41c \uae30\ubcf8 \ubcf4\uc548 \ud504\ub85c\uc138\uc11c\uc640 StrongBOX \ud558\ub4dc\uc6e8\uc5b4 \ube14\ub85d\uc744 \uac16\ucd94\uace0 \uc788\ub2e4. \ub610\ud55c \uc0bc\uc131\uc740 \uc0ac\uc6a9\uc790 \uc815\ubcf4\ub97c \ubcf4\ud638\ud558\ub294 \uc790\uccb4 \uc6b4\uc601\uccb4\uc81c\uc778 \uc5d1\uc2dc\ub178\uc2a4TEE\ub97c \uac1c\ubc1c\ud588\uc73c\uba70 \uc774\ubbf8 EAL2 \uc778\uc99d\uc744 \ubc1b\uc558",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc800\uc804\ub825 \ubc0f \uc800\uc9c0\uc5f0\uc73c\ub85c \uc0c8\ub85c\uc6b4 \uac00\ub2a5\uc131\uc744 \uc5f4\ub2e4**\n\uc774\uc5d0 \ub530\ub77c \uc2dc\uc2a4\ud15c\uc740 \uc6d0\uc2dc \ub370\uc774\ud130\uc5d0\uc11c \uc0c1\ud669 \uc815\ubcf4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc0dd\uc131\ud560 \uc218 \uc788\uc73c\uba70, \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc774\ub97c \ud65c\uc6a9\ud558\uc5ec \ub3d9\uc791\uc744 \uc870\uc815\ud560 \uc218 \uc788\ub2e4. \ubaa8\ubc14\uc77c \uce74\uba54\ub77c\uac00 \uae30\uc874 \ub514\uc9c0\ud138 \uce74\uba54\ub77c\ub97c \ub530\ub77c\uc7a1\uc744 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \uc801\uc751\ud615 \uae30\uc220\uc774 \uc8fc\ubaa9\uc744 \ubc1b\uace0 \uc788\ub2e4. \uc0bc\uc131\uc758 \uc804\uc6a9 \uc2dc\uc2a4\ud15c\uc740 \ub85c\uceec \ubaa8\uc158 \ucd94\uc815 \ubc0f \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560\uc744 \ud1b5\ud574 \uc774\ubbf8\uc9c0 \ubc0f \ube44\ub514\uc624 \ud488\uc9c8\uc744 \uac1c\uc120\ud558\ub294 \uc0c1\ud669 \uc778\uc2dd \ucc98\ub9ac \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\uba70, \uc77c\ubc18 \ud504\ub85c\uc138\uc11c\ubcf4\ub2e4 \ucd5c\ub300 5\ubc30 \uc791\uc740 \ud06c\uae30\ub85c \uc774\ub97c \uc218\ud589\ud55c\ub2e4. \uc0bc\uc131\uc758 \ucd5c\uc801\ud654\ub41c \uc2dc\uc2a4\ud15c\uc744 \ud1b5\ud574 \uc0c1\uc2dc \uc791\ub3d9 \uce74\uba54\ub77c\ub294 \uc774\uc81c \ucd5c\uc18c 30uW\uc758 \uc804\ub825\uc744 \uc18c\ube44\ud558\ubbc0\ub85c \ubc30\ud130\ub9ac \uc7ac\ucda9\uc804 \uc5c6\uc774\ub3c4 \uc77c\uc8fc\uc77c \ub0b4\ub0b4 \ube44\ub514\uc624\ub97c \uc2a4\ud2b8\ub9ac\ubc0d\ud560 \uc218 \uc788\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc790\ub3d9\ucc28 \uc544\ud0a4\ud14d\ucc98\ub97c \uc911\uc559 \ucef4\ud4e8\ud305\uc73c\ub85c \uc804\ud658**\n\ubc29 \uc0c1\ubb34\ub294 \ucc28\ub7c9\uc774 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c \ub3c4\uba54\uc778 \uc911\uc559 \uc9d1\uc911\uc2dd \uc2dc\uc2a4\ud15c\uc73c\ub85c, \uadf8\ub9ac\uace0 \uc774\uc81c\ub294 \ud1b5\ud569 \uc911\uc559 \ucef4\ud4e8\ud305\uc73c\ub85c \ubc1c\uc804\ud574 \uc628 \uacfc\uc815\uc5d0 \ub300\ud574 \uc124\uba85\ud588\ub2e4. \uc774\ub7ec\ud55c \ubcc0\ud654\ub85c \uc778\ud574 \ud6a8\uc728\uc131\uc774 \ub192\uc544\uc9c0\uace0 \uac04\uc18c\ud654\ub418\uc5c8\uc9c0\ub9cc \uace0\ub3c4\ub85c \uc5f0\uacb0\ub41c \uc544\ud0a4\ud14d\ucc98\uc758 \ud2b9\uc131\uc73c\ub85c \uc778\ud574 \uc548\uc804\uacfc \uac19\uc740 \uc601\uc5ed\uc5d0 \ub300\ud55c \uc0c8\ub85c\uc6b4 \uae30\uc220\uc801 \uace0\ub824\uc0ac\ud56d\uc774 \ubc1c\uc0dd\ud588\ub2e4. \uc774\uc5d0 \uc644\ubcbd\ud558\uac8c \ub300\uc751\ud558\uae30 \uc704\ud574 \ucc28\uc138\ub300 \uc0bc\uc131 \uc624\ud1a0 SoC\ub294 \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc9c0\ud5a5\ud558\uba70 \uc548\uc804\uc131, \ubcf4\uc548, \ud655\uc7a5\uc131\uc744 \uac15\ud654\ud55c \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud558\uace0 \uc788\ub2e4.\n_*\uac15\ud654\ub41c \uc548\uc804 \ubc0f \ubcf4\uc548 *_",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required."
            ]
        ],
        "answer": "The Exynos 2200 brings significant advantages to the reference device provided by First AI Challenger managers:\n\n1. **Console-Quality Graphics**: The chipset features the Samsung Xclipse GPU, based on AMD RDNA 2 architecture, offering a new era of mobile gaming with realistic lighting through hardware-accelerated ray tracing and smoother gameplay with variable rate shading.\n\n2. **Enhanced AI Capabilities**: The processor is equipped with an upgraded neural processing unit (NPU), suitable for a wide range of AI applications, leveraging a Dual-core NPU and DSP for improved AI performance and efficiency.\n\n3. **Superior Performance and Security**: The Exynos 2200 incorporates Arm's latest Armv9 CPU cores, a fast 5G modem compliant with 3GPP Release 16, an integrated Secure Element (iSE), and robust encryption to ensure data security.\n\n4. **Advanced Camera and Video Capabilities**: The chipset supports high-resolution cameras up to 200MP, 8K video decoding and encoding, as well as diverse video codecs such as HEVC(H.265), VP9, and AV1 formats.\n\n5. **Immersive Display Experience**: It provides support for high refresh rate displays, including 4K/WQUXGA at 120Hz and QHD+ at 144Hz, enhancing the visual experience for users.\n\nThese advantages make the Exynos 2200 an excellent chipset for driving technological innovation and offering a cutting-edge experience for gaming, AI applications, photography, and display quality.",
        "ground_truth": "Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over."
    },
    {
        "question": "I want to know what types of smartphones were mass-produced based on the Exynos 2200.",
        "contexts": [
            [
                "Question: I want to know what types of smartphones were mass-produced based on the Exynos 2200.\nAnswer: Smartphones mass-produced based on the Exynos 2200 chipset include Samsung Galaxy S22 Ultra, Samsung Galaxy S22+, and Samsung Galaxy S22.",
                "Question: I want to know the main summary of Samsung Exynos 2200.\nAnswer: Samsung Electronics introduces the Exynos 2200, a premium mobile processor featuring the Samsung Xclipse GPU based on AMD RDNA 2 architecture, crafted using a 4-nanometer EUV process. This processor enhances mobile gaming, social media, and photography, offering the first mobile hardware-accelerated ray tracing for realistic lighting and variable rate shading for smoother gameplay. It boasts an upgraded neural processing unit (NPU), Arm's latest Armv9 CPU cores for superior performance and security, and a fast 5G modem compliant with 3GPP Release 16. With integrated Secure Element (iSE) and robust encryption, it ensures data security. The processor's ISP supports up to 200MP resolution, AI-integrated camera functionalities for professional-quality images, an advanced codec for up to 8K video, and a display solution with HDR10+ and up to 144Hz refresh rates, ensuring a comprehensive and immersive visual experience.",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _*\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d\uacfc \uc0dd\uc131\ud615 AI \uae30\uc220\ub85c \uc0ac\uc6a9\uc790 \uacbd\ud5d8 \uadf9\ub300\ud654\ud55c \uc5d1\uc2dc\ub178\uc2a4 2400 *_\n_* *_\n\uc0bc\uc131\uc804\uc790\ub294 \uc774\ubc88 \ud589\uc0ac\uc5d0\uc11c AMD(Advanced Micro Devices)\uc758 \ucd5c\uc2e0 \uc544\ud0a4\ud14d\uccd0 RDNA3 \uae30\ubc18 \uc5d1\uc2a4\ud074\ub9bd\uc2a4 940(Xclipse 940) \uadf8\ub798\ud53d \ucc98\ub9ac\uc7a5\uce58(GPU)\ub97c \ud0d1\uc7ac\ud55c \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \ud504\ub85c\uc138\uc11c '\uc5d1\uc2dc\ub178\uc2a4(Exynos) 2400'\uc744 \uacf5\uac1c\ud588\ub2e4.\n_* *_\n'\uc5d1\uc2dc\ub178\uc2a4 2400'\uc740 \uc804\uc791\uc778 '\uc5d1\uc2dc\ub178\uc2a4 2200' \ub300\ube44 CPU \uc131\ub2a5\uc740 1.7\ubc30, AI \uc131\ub2a5\uc740 \uc9c0\ub09c 2\ub144\uac04 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\ub410\ub2e4.\n\n\uc0bc\uc131\uc804\uc790\ub294 \uc774 \uc81c\ud488\uc5d0 \ub354\uc6b1 \ud5a5\uc0c1\ub41c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(Ray Tracing)\uacfc \ud568\uaed8 \uae00\ub85c\ubc8c \uc77c\ub8e8\ubbf8\ub124\uc774\uc158(Global Illumination), \ub9ac\ud50c\ub809\uc158/\uc250\ub3c4\uc6b0 \ub80c\ub354\ub9c1(Shadow Rendering) \ub4f1 \ub2e4\uc591\ud55c \ucca8\ub2e8 \uadf8\ub798\ud53d \uae30\uc220\uc744 \ud0d1\uc7ac\ud574 \uace0\uc131\ub2a5 \uac8c\uc784\uc744 \uc990\uae30\ub294 \uc720\uc800\ub4e4\uc5d0\uac8c \ucd5c\uace0\uc758 \uc0ac\uc6a9\uc790 \uacbd\ud5d8(User Experience)\ub97c \uc81c\uacf5\ud560 \uacc4\ud68d\uc774\ub2e4.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _* *_\n**\u25a1 5\uc77c (\ud604\uc9c0\uc2dc\uac04), \uc751\uc6a9\ucc98\ubcc4 \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \ud604\ud669\u00b7\ube44\uc804 \ubc1c\ud45c**\n - \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\uc220 \uac1c\ubc1c \uad00\ub828 \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\n\n**\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d, \uc0dd\uc131\ud615 AI \uae30\uc220 \ud0d1\uc7ac\ud55c '\uc5d1\uc2dc\ub178\uc2a4 2400' \uacf5\uac1c**\n - \uc804\uc791 \ub300\ube44 CPU \uc131\ub2a5 1.7\ubc30, AI \uc131\ub2a5 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\n - \uc5d1\uc2dc\ub178\uc2a4 2400 \uae30\ubc18 \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \uac8c\uc784 \ud504\ub9ac\ubdf0 \uc601\uc0c1 \uacf5\uac1c\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n - \uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4 \uacf5\uac1c\u2026\ucd5c\ub300 4\ubc30 \uc90c\uc5d0\ub3c4 \ucd08\uace0\ud654\uc9c8, AI \ud2b8\ub798\ud0b9 \uae30\uc220 \uc801\uc6a9\n - \ucc28\uc138\ub300 \ucc28\ub7c9\uc6a9 \uc194\ub8e8\uc158, \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c(NTN) \uc601\uc0c1 \uc2dc\uc5f0",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent:  \ub9ac\ud50c\ub809\uc158/\uc250\ub3c4\uc6b0 \ub80c\ub354\ub9c1(Reflection/Shadow Rendering): \ube5b\uc758 \ubc18\uc0ac\ud6a8\uacfc, \uadf8\ub9bc\uc790 \uacbd\uacc4\ub97c \ud604\uc2e4 \uc138\uacc4\uc640 \uc720\uc0ac\ud558\uac8c \uc790\uc5f0\uc2a4\ub7fd\uac8c \ud45c\ud604\ud558\ub294 \uae30\ubc95\n\n\ub610 \uc0bc\uc131\uc804\uc790\ub294 '\uc5d1\uc2dc\ub178\uc2a4 2400'\uc744 \ub808\ud37c\ub7f0\uc2a4 \uae30\uae30\uc5d0 \ud0d1\uc7ac\ud574 \ud5a5\ud6c4 \uc2a4\ub9c8\ud2b8\ud3f0\uc5d0 \uc801\uc6a9\ub420 \ubb38\uc790\ub97c \uc774\ubbf8\uc9c0\ub85c \ubcc0\ud658\ud558\ub294 \uc0c8\ub85c\uc6b4 \uc0dd\uc131\ud615 AI \uae30\uc220\ub3c4 \uc120\ubcf4\uc600\ub2e4.\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc0bc\uc131\uc804\uc790 \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n_* *_\n\uc0bc\uc131\uc804\uc790\ub294 \uc774\ub0a0 \uc5d1\uc2dc\ub178\uc2a4 \uc624\ud1a0(Exynos Auto)\u00b7\uc544\uc774\uc18c\uc140 \uc624\ud1a0(ISOCELL Auto)\u00b7\uc544\uc774\uc18c\uc140 \ube44\uc804(ISOCELL Vizion) \ub4f1 \ub2e4\uc591\ud55c \ucc28\uc138\ub300 \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc81c\ud488 \uae30\uc220\uc744 \uc2dc\uc5f0\ud574 \uace0\uac1d\uc0ac\uc640 \ud30c\ud2b8\ub108\uc0ac\ub4e4\ub85c\ubd80\ud130 \ud638\ud3c9\uc744 \ubc1b\uc558\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uacfc\uc911\ud55c \uac8c\uc784 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc804\ub825\uacfc \uc131\ub2a5\uc744 \ub354\uc6b1 \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 CPU \uc804\ub825 \ubd80\ubd84\uc744 \uc904\uc774\uace0 \ub354 \ub9ce\uc740 \uc804\ub825\uc744 CPU \ucf54\uc5b4\uc5d0\uc11c GPU\ub85c \uc7ac\ud560\ub2f9\ud558\uc5ec \ubaa8\ubc14\uc77c \uc7a5\uce58\uc5d0\uc11c \ucd5c\uace0 \uc218\uc900\uc758 \uac8c\uc784 \uadf8\ub798\ud53d\uc744 \uc9c0\uc6d0\ud560 \uc218 \uc788\ub3c4\ub85d \ucef4\ud4e8\ud305 \uc131\ub2a5\uc744 \ub192\uc600\ub2e4.\n**\ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub418\ub294 \ud48d\ubd80\ud55c \uadf8\ub798\ud53d \uacbd\ud5d8**\n\uc0bc\uc131\uc740 \ud504\ub9ac\ubbf8\uc5c4\ubd80\ud130 \uc800\uac00\ud615\uc5d0 \uc774\ub974\ub294 \ub113\uc740 \ubc94\uc704\uc758 \ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0 \ucf58\uc194 \uc218\uc900\uc758 \uac8c\uc784\uc744 \uc81c\uacf5\ud55c\ub2e4\ub294 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 AMD\uc758 RDNATM \uc544\ud0a4\ud14d\ucc98\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 Xclipse GPU\ub97c \uac1c\ubc1c\ud574\uc654\uace0, \uc774\ub97c \ud1b5\ud574 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(ray tracing) \uae30\ub2a5\uc744 \ucd5c\ucd08\ub85c \ub3c4\uc785\ud588\ub2e4",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc544\ud0a4\ud14d\ucc98 \ubcc0\uacbd\uc758 \uacb0\uacfc\ub85c MobileBERT1 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc740 \uc774\uc804 \uc138\ub300\uc5d0 \ube44\ud574 3\ubc30 \ud5a5\uc0c1\ub418\uc5c8\ub2e4\n**\ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc704\ud55c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC**\n\uc790\ub3d9\ucc28\ub294 \ud604\uc7ac \uc804\ud654\uac00 \uc2a4\ub9c8\ud2b8\ud3f0\uc73c\ub85c \uc9c4\ud654\ud55c \uac83\uacfc \uc720\uc0ac\ud558\uac8c \uae09\uaca9\ud558\uac8c \ubcc0\ud654\ud558\uace0 \uc788\ub2e4. \uc2e4\uc81c\ub85c \ud604\ub300\uc758 \uc790\ub3d9\ucc28\ub294 \ub2e8\uc21c\ud55c \uc774\ub3d9 \uc218\ub2e8\uc744 \ub118\uc5b4 \uc0dd\uc131\ud615 AI\uc640 \uac19\uc740 \uad11\ubc94\uc704\ud55c \ucef4\ud4e8\ud305 \uae30\ub2a5\uc758 \uc9d1\ud569\uccb4\uac00 \ub418\uc5c8\ub2e4.\n\uc774\ub7ec\ud55c \ud604\uc2e4\uc744 \ubc30\uacbd\uc73c\ub85c \uc0bc\uc131\uc804\uc790 AP2 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uac1c\ubc1c\ud300 \ubc29\uc9c0\ud6c8 \uc0c1\ubb34\ub294 \uc0bc\uc131 \uc624\ud1a0 SoC \uc194\ub8e8\uc158\uacfc \ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ub300\ud55c \uc138\uc158\uc744 \uc9c4\ud589\ud588\ub2e4.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\ub294 2\uc5b5 \ud654\uc18c \uc774\ubbf8\uc9c0\uc13c\uc11c \uae30\ubc18 \ucd08\uace0\ud574\uc0c1\ub3c4 \ud2b9\uc218 \uc90c \uae30\uc220\uc778 '\uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4(Zoom Anyplace)'\ub97c \ucc98\uc74c \uacf5\uac1c\ud588\ub2e4. \uc774 \uae30\uc220\uc740 \uc6c0\uc9c1\uc774\ub294 \uc0ac\ubb3c\uc5d0 \ub300\ud574 \ud480\uc2a4\ud06c\ub9b0\uacfc \ucd5c\ub300 4\ubc30 \ud074\ub85c\uc988\uc5c5 \uc7a5\uba74\uae4c\uc9c0 \ud654\uc9c8 \uc800\ud558\uc5c6\uc774 \ub3d9\uc2dc\uc5d0 \ucd2c\uc601\ud560 \uc218 \uc788\uace0, \ud074\ub85c\uc988\uc5c5\uc2dc AI \uae30\uc220\ub85c \uc0ac\ubb3c\uc744 \uc790\ub3d9 \ucd94\uc801\ud560 \uc218 \uc788\uc5b4 \ubaa8\ubc14\uc77c \uc0ac\uc6a9\uc790\uc5d0\uac8c \uc644\uc804\ud788 \uc0c8\ub85c\uc6b4 \uce74\uba54\ub77c \uc90c \uacbd\ud5d8\uc744 \uc81c\uacf5\ud55c\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 2025\ub144 \uc591\uc0b0 \uc608\uc815\uc778 \ucc28\uc138\ub300 \ud504\ub9ac\ubbf8\uc5c4 \uc778\ud3ec\ud14c\uc778\uba3c\ud2b8(IVI, In-Vehicle Infotainment)\uc6a9 \ud504\ub85c\uc138\uc11c '\uc5d1\uc2dc\ub178\uc2a4 \uc624\ud1a0 V920' \uad6c\ub3d9 \uc601\uc0c1\uc744 \uacf5\uac1c\ud588\ub2e4.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc774 \uc81c\ud488\uc740 Arm\uc758 \ucd5c\uc2e0 \uc804\uc7a5\uc6a9 \uc911\uc559\ucc98\ub9ac\uc7a5\uce58(CPU) \ucf54\uc5b4\ud14d\uc2a4-A78AE 10\uac1c\ub97c \uae30\ubc18\uc73c\ub85c \uc774\uc804 \uc81c\ud488 \ub300\ube44 1.7\ubc30 \uac15\ud654\ub41c CPU \uc131\ub2a5\uacfc \ucd5c\ub300 6\uac1c\uc758 \uace0\ud654\uc9c8 \ub514\uc2a4\ud50c\ub808\uc774\uc5d0 \ub3d9\uc2dc \uc5f0\uacb0 \uac00\ub2a5\ud55c \uba40\ud2f0 \ucee4\ub125\ud2f0\ube44\ud2f0 \uae30\ub2a5\uc73c\ub85c \uc2a4\ub9c8\ud2b8\ud558\uace0 \uc990\uac70\uc6b4 \ubaa8\ube4c\ub9ac\ud2f0 \uacbd\ud5d8\uc744 \uc81c\uacf5\ud55c\ub2e4.\n\ub610 \uc0bc\uc131\uc804\uc790\ub294 \ucc28\ub7c9\uc6a9 \uc774\ubbf8\uc9c0\uc13c\uc11c\ud5a5 '\uc544\uc774\uc18c\uc140 \uc624\ud1a0'\uc640 \uc0ac\ubb3c\uc758 \ube60\ub978 \uc6c0\uc9c1\uc784\uc744 \uc21c\uac04\uc801\uc73c\ub85c \uc815\ud655\ud558\uac8c \ud3ec\ucc29 \uac00\ub2a5\ud55c '\uc544\uc774\uc18c\uc140 \ube44\uc804' \uc81c\ud488\uc744 \ud1b5\ud574 \uc548\uc804 \uc8fc\ud589 \uae30\uc220\ub3c4 \uc120\ubcf4\uc600\ub2e4.\n\uc0bc\uc131\uc804\uc790\uac00 \uc774\ub0a0 \uacf5\uac1c\ud55c '\uc544\uc774\uc18c\uc140 \uc624\ud1a0 1H1'\uc740 \ub2e4\uc591\ud55c \uc8fc\ud589, \uc870\ub3c4 \ud658\uacbd\uc5d0\uc11c \ub3c4\ub85c\uc640 \uc0ac\ubb3c\uc744 \uc815\ud655\ud788 \ud310\ub2e8\ud560 \uc218 \uc788\ub294 120dB HDR\uc744 \uc9c0\uc6d0\ud558\uace0, \uc2e0\ud638\ub4f1\uc758 \uae5c\ube61\uc784 \ud604\uc0c1 \ub4f1 'LED \ud50c\ub9ac\ucee4(LED Flicker)'\ub97c \uc644\ud654\ud560 \uc218 \uc788\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub610\ud55c \uc790\ub3d9\ucc28 \uae30\uc220\uc774 \ub354\uc6b1 \ubc1c\uc804\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub294 \uc0c1\ud669\uc5d0\uc11c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uac01 \uc601\uc5ed\uc758 \uc5ec\ub7ec \uc774\uae30\uc885 OS\ub97c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ud560 \uc218 \uc788\uc5b4 \ud68d\uae30\uc801\uc778 \ubbf8\ub798\ub97c \uc900\ube44\ud560 \uc218 \uc788\ub2e4. \ubc29 \uc0c1\ubb34\ub294 \"\uc0bc\uc131 \uc624\ud1a0 SoC\uc758 \uc911\uc559 \ucef4\ud4e8\ud305 \uae30\ub2a5 \ub355\ubd84\uc5d0 \ud55c\ub54c \uc0c1\uc0c1 \uc18d\uc5d0\ub9cc \uc874\uc7ac\ud588\ub358 \uac83\ub4e4\uc774 \uace7 \ud604\uc2e4\uc774 \ub420 \uac83\"\uc774\ub77c\uba70 \ubbf8\ub798\uc5d0 \ub300\ud55c \ubc1d\uc740 \uc804\ub9dd\uc744 \ub4dc\ub7ec\ub0c8\ub2e4.\n**\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158**\n\ub354\uc6b1 \uac15\ub825\ud55c SoC\uc640 \uc13c\uc11c \uae30\ub2a5\uc774 \uac1c\ubc1c\ub428\uc5d0 \ub530\ub77c \ucd5c\uc2e0 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc810\uc810 \ub354 \uc2a4\ub9c8\ud2b8\ud574\uc9c0\uace0 \ub192\uc740 \uc5f0\uc0b0 \ub2a5\ub825\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\ub2a5\uc744 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub3c4\uc785\ud558\uae30 \uc704\ud55c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 IP\uac1c\ubc1c \ubc29\ud5a5\uc131\uc744 \uc18c\uac1c\ud588\ub2e4.\n**\ucd5c\uc801\ud654\ub41c 4\ud074\ub7ec\uc2a4\ud130 \uad6c\uc870\uc758 CPU**\n\uc138 \uac1c\uc758 \uc8fc\uc694 IP \uc911 \uccab \ubc88\uc9f8\ub294 CPU\ub85c, \uae30\uc874\uc5d0\ub294 \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud558\uc774\uc5d4\ub4dc(high-end) \ud074\ub7ec\uc2a4\ud130, \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ub85c\uc6b0\uc5d4\ub4dc(low-end) \ud074\ub7ec\uc2a4\ud130, \uc911\uac04 \ud074\ub7ec\uc2a4\ud130 \ub4f1 \uc138 \uac1c\uc758 \ud074\ub7ec\uc2a4\ud130\ub85c \uad6c\uc131\ub418\uc5c8\ub2e4. \uc0bc\uc131\uc740 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 CPU \uc6b4\uc601\uc744 \uc704\ud574 \uc911\uac04 \ud074\ub7ec\uc2a4\ud130\ub97c \ubbf8\ub4dc \ud558\uc774(mid-high)\uc640 \ubbf8\ub4dc \ub85c\uc6b0(mid-low)\uc758 \ub450 \uac00\uc9c0 \uacc4\uce35\uc73c\ub85c \ub098\ub204\uace0, \ubbf8\ub4dc \ud558\uc774 \uacc4\uce35\uc740 \uac8c\uc784\uacfc \uac19\uc740 \ucef4\ud4e8\ud305 \uc9d1\uc57d\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud588",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Converting NN Models with ENN SDK Service\nContent:\nIn this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.\nTo get started, you must be a member of the Exynos Developer Society.\n- If you are not a member of the Exynos Developer Society, sign up from here.\n- If you already have an account, log in to the Exynos Developer Society.\nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from here.\n2. Navigate to the ENN SDK service page and provide a title for your project.\n3. Then, upload the downloaded TFLite model\n4. Next, select hardware preferences.\n\t- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n\t- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n5. After confirming your selections, click **Convert** to convert the model.\n6. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n7. Click **NNC Download** to download the converted NNC model.\n\tYou can now integrate the NNC model into the desired application.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc0dd\uc131\ud615 \uc778\uacf5\uc9c0\ub2a5(AI) \ubc0f \uac01\uc885 \ucca8\ub2e8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ub4f1\uc7a5\uc73c\ub85c \uc778\ud574 \uc2dc\uc7a5\uc5d0\uc11c\ub294 \ud6a8\uc728\uc131, \ud655\uc7a5\uc131, \uc800\uc804\ub825\uc744 \uac16\ucd98 \ub354\uc6b1 \uc2a4\ub9c8\ud2b8\ud558\uace0 \uac15\ub825\ud55c \ucef4\ud4e8\ud305\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4. \uc774\uc5d0 \ub530\ub77c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\uc758 \uc8fc\uc694 \ud14c\ub9c8 \uc911 \ud558\ub098\uc778 \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\ub97c \uc8fc\uc81c\ub85c \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220, \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158, \uc0bc\uc131 \uc624\ud1a0 SoC(System-on-Chips) \ub4f1 \uc138 \uac1c\uc758 \uac15\uc5f0\uc774 \uc9c4\ud589\ub418\uc5c8\ub2e4.\n**\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220**\n\uccab\ubc88\uc9f8 \uc138\uc158\uc778 \"\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220\"\uc740 \uc0bc\uc131 \uc624\uc2a4\ud2f4 \uc5f0\uad6c\uc13c\ud130 \ubca0\ub2c8 \uce74\ud2f0\ube44\uc548(Benny Katibian) \ubd80\uc0ac\uc7a5\uc774 \uc9c4\ud589\ud588\ub2e4",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent:  \uc2a4\uce74\uc77c\ub85c\uc0ac(Skylo Technologies): \uc778\uacf5\uc704\uc131\uc744 \uc0ac\uc6a9\ud558\ub294 \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c (NTN, Non-Terrestrial Networks)\uc911 NTN NB-IoT \uae30\ub2a5\uc744 \ubcc4\ub3c4\uc758 IoT\uce69\uc774 \uc5c6\uc774 \uc9c0\uc6d0\ud558\ub294 \ucc28\uc138\ub300 5G \ubaa8\ub380\uc73c\ub85c \uc778\uacf5\uc704\uc131 \uc2e0\ud638 \uc218\uc2e0\uae30\ub97c \uc81c\uc791\n\uc774\uc678\uc5d0\ub3c4 \uc0bc\uc131\uc804\uc790\ub294 \ucd08\uad11\ub300\uc5ed(UWB, Ultra-Wideband) \uae30\uc220\uc744 \ud65c\uc6a9\ud55c '\uc5d1\uc2dc\ub178\uc2a4 \ucee4\ub125\ud2b8(Exynos Connect) U100\u00b72\uc5b5 \ucd08\uace0\ud654\uc18c \uc774\ubbf8\uc9c0\uc13c\uc11c '\uc544\uc774\uc18c\uc140 HP2'\u00b7QD OLED \ud654\uc9c8\uc744 \uc704\ud55c \ub514\uc2a4\ud50c\ub808\uc774IC\u00b7IoT \ubcf4\uc548 \uc194\ub8e8\uc158\u00b7\ubb34\uc120\ucda9\uc804\ud5a5 \uc804\ub825\uad00\ub9acIC(PMIC)\u00b7\uc2a4\ub9c8\ud2b8 \ud5ec\uc2a4 \ud504\ub85c\uc138\uc11c \ub4f1 \ub2e4\uc591\ud55c \ucc28\uc138\ub300 \ubc18\ub3c4\uccb4 \uc194\ub8e8\uc158\uc744 \uc18c\uac1c\ud588\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc5d1\uc2dc\ub178\uc2a4 \ub808\ud37c\ub7f0\uc2a4 \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub41c \ubaa8\ubc14\uc77c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5 \ub370\ubaa8 \uc601\uc0c1\uc744 \ud1b5\ud574 \uadf8\ub9bc\uc790, \ubc18\uc0ac \ub4f1\uacfc \uac19\uc740 \uc5ec\ub7ec \uac1c\uc758 \uc138\ubd80 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5\ub4e4\uc774 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.\n**\uc0dd\uc131\ud615 AI \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 NPU**\n\uc0bc\uc131\uc758 \ucd5c\uc2e0 NPU \uc194\ub8e8\uc158\uc740 \uba54\ubaa8\ub9ac \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcc0\uacbd\ud558\uace0 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \ube44\uc120\ud615 \uc5f0\uc0b0\uc758 \ud65c\uc6a9\ub3c4\ub97c \ud06c\uac8c \ub192\uc77c \uac83\uc774\ub2e4",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc800\uc804\ub825 \ubc0f \uc800\uc9c0\uc5f0\uc73c\ub85c \uc0c8\ub85c\uc6b4 \uac00\ub2a5\uc131\uc744 \uc5f4\ub2e4**\n\uc774\uc5d0 \ub530\ub77c \uc2dc\uc2a4\ud15c\uc740 \uc6d0\uc2dc \ub370\uc774\ud130\uc5d0\uc11c \uc0c1\ud669 \uc815\ubcf4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc0dd\uc131\ud560 \uc218 \uc788\uc73c\uba70, \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc774\ub97c \ud65c\uc6a9\ud558\uc5ec \ub3d9\uc791\uc744 \uc870\uc815\ud560 \uc218 \uc788\ub2e4. \ubaa8\ubc14\uc77c \uce74\uba54\ub77c\uac00 \uae30\uc874 \ub514\uc9c0\ud138 \uce74\uba54\ub77c\ub97c \ub530\ub77c\uc7a1\uc744 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \uc801\uc751\ud615 \uae30\uc220\uc774 \uc8fc\ubaa9\uc744 \ubc1b\uace0 \uc788\ub2e4. \uc0bc\uc131\uc758 \uc804\uc6a9 \uc2dc\uc2a4\ud15c\uc740 \ub85c\uceec \ubaa8\uc158 \ucd94\uc815 \ubc0f \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560\uc744 \ud1b5\ud574 \uc774\ubbf8\uc9c0 \ubc0f \ube44\ub514\uc624 \ud488\uc9c8\uc744 \uac1c\uc120\ud558\ub294 \uc0c1\ud669 \uc778\uc2dd \ucc98\ub9ac \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\uba70, \uc77c\ubc18 \ud504\ub85c\uc138\uc11c\ubcf4\ub2e4 \ucd5c\ub300 5\ubc30 \uc791\uc740 \ud06c\uae30\ub85c \uc774\ub97c \uc218\ud589\ud55c\ub2e4. \uc0bc\uc131\uc758 \ucd5c\uc801\ud654\ub41c \uc2dc\uc2a4\ud15c\uc744 \ud1b5\ud574 \uc0c1\uc2dc \uc791\ub3d9 \uce74\uba54\ub77c\ub294 \uc774\uc81c \ucd5c\uc18c 30uW\uc758 \uc804\ub825\uc744 \uc18c\ube44\ud558\ubbc0\ub85c \ubc30\ud130\ub9ac \uc7ac\ucda9\uc804 \uc5c6\uc774\ub3c4 \uc77c\uc8fc\uc77c \ub0b4\ub0b4 \ube44\ub514\uc624\ub97c \uc2a4\ud2b8\ub9ac\ubc0d\ud560 \uc218 \uc788\ub2e4.",
                "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc790\ub3d9\ucc28 \uc544\ud0a4\ud14d\ucc98\ub97c \uc911\uc559 \ucef4\ud4e8\ud305\uc73c\ub85c \uc804\ud658**\n\ubc29 \uc0c1\ubb34\ub294 \ucc28\ub7c9\uc774 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c \ub3c4\uba54\uc778 \uc911\uc559 \uc9d1\uc911\uc2dd \uc2dc\uc2a4\ud15c\uc73c\ub85c, \uadf8\ub9ac\uace0 \uc774\uc81c\ub294 \ud1b5\ud569 \uc911\uc559 \ucef4\ud4e8\ud305\uc73c\ub85c \ubc1c\uc804\ud574 \uc628 \uacfc\uc815\uc5d0 \ub300\ud574 \uc124\uba85\ud588\ub2e4. \uc774\ub7ec\ud55c \ubcc0\ud654\ub85c \uc778\ud574 \ud6a8\uc728\uc131\uc774 \ub192\uc544\uc9c0\uace0 \uac04\uc18c\ud654\ub418\uc5c8\uc9c0\ub9cc \uace0\ub3c4\ub85c \uc5f0\uacb0\ub41c \uc544\ud0a4\ud14d\ucc98\uc758 \ud2b9\uc131\uc73c\ub85c \uc778\ud574 \uc548\uc804\uacfc \uac19\uc740 \uc601\uc5ed\uc5d0 \ub300\ud55c \uc0c8\ub85c\uc6b4 \uae30\uc220\uc801 \uace0\ub824\uc0ac\ud56d\uc774 \ubc1c\uc0dd\ud588\ub2e4. \uc774\uc5d0 \uc644\ubcbd\ud558\uac8c \ub300\uc751\ud558\uae30 \uc704\ud574 \ucc28\uc138\ub300 \uc0bc\uc131 \uc624\ud1a0 SoC\ub294 \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc9c0\ud5a5\ud558\uba70 \uc548\uc804\uc131, \ubcf4\uc548, \ud655\uc7a5\uc131\uc744 \uac15\ud654\ud55c \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud558\uace0 \uc788\ub2e4.\n_*\uac15\ud654\ub41c \uc548\uc804 \ubc0f \ubcf4\uc548 *_",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## The ENN SDK\nThe ENN SDK tool facilitates the conversion of TensorFlow Lite neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the Samsung Exynos hardware to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of the Sample Native Program\nThe sample native program accepts a model file and raw input data files as parameters.\nIf golden output data files are provided, the program compares these files and displays the result. If no file is provided, the program generates a raw output file.\n### Model and Data Used\nIn this sample native program, a converted NNC file and raw input/output file available in the Github Repository are used.\n### Using Native Program\nTo execute the sample native program, refer to README file.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\uac00 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \ubbf8\uc8fc\ucd1d\uad04(Device Solutions America office)\uc5d0\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023'\uc744 \uac1c\ucd5c\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \ucd08\uc9c0\ub2a5\ud654(Hyper-Intelligence)\u00b7\ucd08\uc5f0\uacb0\uc131(Hyper-Connected)\u00b7\ucd08\ub370\uc774\ud130(Hyper-Data)\ub97c \uac00\ub2a5\ud558\uac8c \ud560 \uc8fc\uc694 \uc751\uc6a9\ucc98\ubcc4 \ucd5c\uc2e0 \ubc18\ub3c4\uccb4 \uc124\uacc4 \ud604\ud669\uacfc \ube44\uc804\uc744 \uacf5\uc720\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \uace0\uac1d\uc0ac\uc640 \ud30c\ud2b8\ub108\uc0ac \uad00\uacc4\uc790 300\uc5ec\uba85\uc774 \ucc38\uc11d\ud55c \uac00\uc6b4\ub370, \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \ubd84\uc57c \uae00\ub85c\ubc8c \uc804\ubb38\uac00, \uc11d\ud559\ub4e4\uacfc \ud568\uaed8 \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM, Large Language Model) \uae30\uc220\uc5d0 \ub300\ud55c \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\ub97c \uc9c4\ud589\ud588\ub2e4.",
                "Section: Compiling Using NDK\\Compiling Using NDK\\Creating the Makefile\nContent:\nThe Makefile is a crucial component in the build process.\nIt instructs the compiler on how to build the program.\nThe Makefile for this project is divided into two parts such as `Android.mk` and `Application.mk`.\n### Android.mk\nThe `Android.mk` file defines the module and its properties.\n(example):\n```cmake\nLOCAL_PATH := $(call my-dir)\ninclude $(CLEAR_VARS)\nLOCAL_MODULE := enn_public_api_ndk_v1\nLOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so\ninclude $(PREBUILT_SHARED_LIBRARY)\ninclude $(CLEAR_VARS)\nLOCAL_MODULE := enn_nnc_model_tester\nLOCAL_C_INCLUDES += \\\n\t${LOCAL_PATH} \\\n\t${LOCAL_PATH}/include\nLOCAL_LDLIBS := -llog\nLOCAL_CFLAGS += -Wall -std=c++14 -O3\nLOCAL_CPPFLAGS += -fexceptions -frtti\nLOCAL_SRC_FILES := enn_nnc_model_tester.cpp\nLOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1\ninclude $(BUILD_EXECUTABLE)\n```\n### Application.mk:\nThe `Application.mk` file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used.\n(example):\n```cmake\nAPP_ABI := arm64-v8a\nAPP_STL := c++_static\n```",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc548\uc804\uc5d0 \ub300\ud55c \uc0bc\uc131\uc758 \ud655\uace0\ud55c \uc758\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ucc28\uc138\ub300 \uc624\ud1a0 SoC\uc758 \uc138\uc774\ud504\ud2f0 \uc544\uc77c\ub79c\ub4dc(safety island)\ub294 ASIL-D\ub97c \uc900\uc218\ud558\uba70, \ud638\uc2a4\ud2b8 CPU\uc640 \ubcc4\ub3c4\ub85c \uc791\ub3d9\ud558\uc5ec \ub2e4\ub978 SoC\uc758 \uc0c1\ud0dc\ub97c \ubaa8\ub2c8\ud130\ub9c1\ud560 \uc608\uc815\uc774\ub2e4. \uc774\uc678\uc758 \ubd80\ubd84\uc740 ASPICE, ISO 26262 \ubc0f FMEA\uc640 \uac19\uc740 \uc790\ub3d9\ucc28 \ud45c\uc900\uc744 \ud1b5\ud574 ASIL-B\ub97c \uc900\uc218\ud558\ub3c4\ub85d \uc81c\uc791\ub418\uc5c8\ub2e4.\n\uc0ac\uc774\ubc84 \ubcf4\uc548 \ub610\ud55c \uc810\uc810 \ub354 \uc911\uc694\ud55c \uc694\uc18c\ub85c \ub5a0\uc624\ub974\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0, \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uc554\ud638\ud654 \uc5d4\uc9c4\uc774 \ub0b4\uc7a5\ub41c \uae30\ubcf8 \ubcf4\uc548 \ud504\ub85c\uc138\uc11c\uc640 StrongBOX \ud558\ub4dc\uc6e8\uc5b4 \ube14\ub85d\uc744 \uac16\ucd94\uace0 \uc788\ub2e4. \ub610\ud55c \uc0bc\uc131\uc740 \uc0ac\uc6a9\uc790 \uc815\ubcf4\ub97c \ubcf4\ud638\ud558\ub294 \uc790\uccb4 \uc6b4\uc601\uccb4\uc81c\uc778 \uc5d1\uc2dc\ub178\uc2a4TEE\ub97c \uac1c\ubc1c\ud588\uc73c\uba70 \uc774\ubbf8 EAL2 \uc778\uc99d\uc744 \ubc1b\uc558",
                "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc911\uc559 \ucef4\ud4e8\ud305\uc5d0 \ub300\ud55c \uc218\uc694\uac00 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ud558\ub4dc\uc6e8\uc5b4\uc640 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \ud655\uc7a5\uc131\uc774 \ub354\uc6b1 \uc911\uc694\ud574\uc9c8 \uac83\uc774\ub2e4. \ub530\ub77c\uc11c \uc0bc\uc131\uc758 \ud575\uc2ec \uc194\ub8e8\uc158 \uc911 \ud558\ub098\ub294 \uc18c\ud504\ud2b8\uc6e8\uc5b4\ub97c \uc218\uc815\ud558\uc9c0 \uc54a\uace0\ub3c4 \ucef4\ud4e8\ud305 \uc6a9\ub7c9\uc744 \ub450 \ubc30\ub85c \ub298\ub9ac\uae30 \uc704\ud574 \ub450 SoC \uac04\uc758 \uc9c1\uc811 \uc5f0\uacb0(die-to-die)\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. \ub610\ud55c \uc624\ud1a0 SoC\ub294 OEM\uc758 \uc2dc\uc2a4\ud15c\uc5d0 \ub530\ub77c \ud328\ud0a4\uc9c0 \uac04 \uba40\ud2f0 SoC \uc5f0\uacb0\uc744 \uc704\ud574 PCIe \ub610\ub294 \uc774\ub354\ub137 \uc0ac\uc6a9\uc744 \uc9c0\uc6d0\ud55c\ub2e4.\n_*AI \ud1b5\ud569, \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc815\uc758 \ucc28\ub7c9\uc758 \ubbf8\ub798\ub97c \uc704\ud55c \uc900\ube44 *_\n\uc55e\uc73c\ub85c \uc6b4\uc804\uc790\ub294 \ucd5c\ub300 150\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uc2e4\uc2dc\uac04\uc73c\ub85c \uc791\ub3d9\ud560 \uc218 \uc788\ub294 \uc0bc\uc131\uc758 \uc804\uc6a9 AI \uac00\uc18d\uae30\ub97c \ud1b5\ud574 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0 \uae30\ubc18\ud55c AI \ube44\uc11c\uc640 \uc6d0\ud65c\ud558\uac8c \uc0c1\ud638 \uc791\uc6a9\ud560 \uc218 \uc788\uac8c \ub420 \uac83\uc774\ub2e4.",
                "Section: Quick Start Guide\\Quick Start Guide\\Basic Workflow\nContent:\nFollowing figure illustrates the three steps for converting and executing an NN model:\n```mermaid\nflowchart LR\n\tsubgraph \"ENN SDK Service\"\n\t\tdirection LR\n\t\tconvert(\"Convert The Model\")\n\tend\n\tsubgraph \"ENN Framework\"\n\t\tdirection LR\n\t\texecute(\"Execute The Model\")\n\tend\n\tmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```\n### Model Conversion\nTo convert TensorFlow Lite models, ENN SDK provides an online conversion tool through the Samsung Exynos Developer Society.\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.\nFor more information on the process of converting NN models, refer to Converting NN Models with ENN SDK Service.\n### Model Execution\nTo execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe ENN SDK provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.\nFor more information on the process of executing NN models, refer to Executing Models Using Native Program and Executing Models Using Android Application.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uc774 \uac1c\ub150\uc740 \uc13c\uc11c\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\uace0, \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uace0, \ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790\uc5d0\uac8c \ub354 \ub098\uc740 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud558\ub294 \uc138 \ub2e8\uacc4\ub85c \uc138\ubd84\ud654\ud560 \uc218 \uc788\ub2e4.\n\ubaa8\ub4e0 \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub824\uba74 \ub9ce\uc740 \uc5f0\uc0b0\uc774 \ud544\uc694\ud558\ubbc0\ub85c \uc804\ub825 \uc18c\ube44\uac00 \uc99d\uac00\ud55c\ub2e4. \ubc14\ub85c \uc774 \ubd80\ubd84\uc5d0\uc11c \uc0bc\uc131\uc758 \uae30\uc220\uc801 \ud601\uc2e0\uc774 \ube5b\uc744 \ubc1c\ud55c\ub2e4. \uc0bc\uc131\uc740 \uc804\uc6a9 \ub3c4\uba54\uc778\ubcc4 \uc2dc\uc2a4\ud15c\uc744 \ud3ec\ud568\ud558\ub294 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\ub97c \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub294 \ub354 \uac04\ub2e8\ud558\uba74\uc11c \ud2b9\uc218\ud55c \uc5f0\uc0b0 \ubd80\ud558\ub97c \ucc98\ub9ac\ud558\uae30 \ub54c\ubb38\uc5d0 \uae30\ubcf8\uc801\uc73c\ub85c \ud6a8\uc728\uc131\uc774 \ub192\ub2e4. \uc774 \uc2dc\uc2a4\ud15c\uc740 \uc804\ub825 \uc18c\ube44\uac00 \uc801\uace0 \ucc98\ub9ac \uc9c0\uc5f0 \uc2dc\uac04\uc774 \uc9e7\ub2e4."
            ]
        ],
        "answer": "Smartphones mass-produced based on the Exynos 2200 chipset include Samsung Galaxy S22 Ultra, Samsung Galaxy S22+, and Samsung Galaxy S22.",
        "ground_truth": "Smartphones mass-produced based on the Exynos 2200 chipset include Samsung Galaxy S22 Ultra, Samsung Galaxy S22+, and Samsung Galaxy S22."
    },
    {
        "question": "I want to know the main summary of Samsung Exynos 2200.",
        "contexts": [
            [
                "Question: I want to know the main summary of Samsung Exynos 2200.\nAnswer: Samsung Electronics introduces the Exynos 2200, a premium mobile processor featuring the Samsung Xclipse GPU based on AMD RDNA 2 architecture, crafted using a 4-nanometer EUV process. This processor enhances mobile gaming, social media, and photography, offering the first mobile hardware-accelerated ray tracing for realistic lighting and variable rate shading for smoother gameplay. It boasts an upgraded neural processing unit (NPU), Arm's latest Armv9 CPU cores for superior performance and security, and a fast 5G modem compliant with 3GPP Release 16. With integrated Secure Element (iSE) and robust encryption, it ensures data security. The processor's ISP supports up to 200MP resolution, AI-integrated camera functionalities for professional-quality images, an advanced codec for up to 8K video, and a display solution with HDR10+ and up to 144Hz refresh rates, ensuring a comprehensive and immersive visual experience.",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I want to know what types of smartphones were mass-produced based on the Exynos 2200.\nAnswer: Smartphones mass-produced based on the Exynos 2200 chipset include Samsung Galaxy S22 Ultra, Samsung Galaxy S22+, and Samsung Galaxy S22.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## The ENN SDK\nThe ENN SDK tool facilitates the conversion of TensorFlow Lite neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the Samsung Exynos hardware to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of the Sample Native Program\nThe sample native program accepts a model file and raw input data files as parameters.\nIf golden output data files are provided, the program compares these files and displays the result. If no file is provided, the program generates a raw output file.\n### Model and Data Used\nIn this sample native program, a converted NNC file and raw input/output file available in the Github Repository are used.\n### Using Native Program\nTo execute the sample native program, refer to README file.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _*\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d\uacfc \uc0dd\uc131\ud615 AI \uae30\uc220\ub85c \uc0ac\uc6a9\uc790 \uacbd\ud5d8 \uadf9\ub300\ud654\ud55c \uc5d1\uc2dc\ub178\uc2a4 2400 *_\n_* *_\n\uc0bc\uc131\uc804\uc790\ub294 \uc774\ubc88 \ud589\uc0ac\uc5d0\uc11c AMD(Advanced Micro Devices)\uc758 \ucd5c\uc2e0 \uc544\ud0a4\ud14d\uccd0 RDNA3 \uae30\ubc18 \uc5d1\uc2a4\ud074\ub9bd\uc2a4 940(Xclipse 940) \uadf8\ub798\ud53d \ucc98\ub9ac\uc7a5\uce58(GPU)\ub97c \ud0d1\uc7ac\ud55c \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \ud504\ub85c\uc138\uc11c '\uc5d1\uc2dc\ub178\uc2a4(Exynos) 2400'\uc744 \uacf5\uac1c\ud588\ub2e4.\n_* *_\n'\uc5d1\uc2dc\ub178\uc2a4 2400'\uc740 \uc804\uc791\uc778 '\uc5d1\uc2dc\ub178\uc2a4 2200' \ub300\ube44 CPU \uc131\ub2a5\uc740 1.7\ubc30, AI \uc131\ub2a5\uc740 \uc9c0\ub09c 2\ub144\uac04 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\ub410\ub2e4.\n\n\uc0bc\uc131\uc804\uc790\ub294 \uc774 \uc81c\ud488\uc5d0 \ub354\uc6b1 \ud5a5\uc0c1\ub41c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(Ray Tracing)\uacfc \ud568\uaed8 \uae00\ub85c\ubc8c \uc77c\ub8e8\ubbf8\ub124\uc774\uc158(Global Illumination), \ub9ac\ud50c\ub809\uc158/\uc250\ub3c4\uc6b0 \ub80c\ub354\ub9c1(Shadow Rendering) \ub4f1 \ub2e4\uc591\ud55c \ucca8\ub2e8 \uadf8\ub798\ud53d \uae30\uc220\uc744 \ud0d1\uc7ac\ud574 \uace0\uc131\ub2a5 \uac8c\uc784\uc744 \uc990\uae30\ub294 \uc720\uc800\ub4e4\uc5d0\uac8c \ucd5c\uace0\uc758 \uc0ac\uc6a9\uc790 \uacbd\ud5d8(User Experience)\ub97c \uc81c\uacf5\ud560 \uacc4\ud68d\uc774\ub2e4.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _* *_\n**\u25a1 5\uc77c (\ud604\uc9c0\uc2dc\uac04), \uc751\uc6a9\ucc98\ubcc4 \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \ud604\ud669\u00b7\ube44\uc804 \ubc1c\ud45c**\n - \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\uc220 \uac1c\ubc1c \uad00\ub828 \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\n\n**\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d, \uc0dd\uc131\ud615 AI \uae30\uc220 \ud0d1\uc7ac\ud55c '\uc5d1\uc2dc\ub178\uc2a4 2400' \uacf5\uac1c**\n - \uc804\uc791 \ub300\ube44 CPU \uc131\ub2a5 1.7\ubc30, AI \uc131\ub2a5 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\n - \uc5d1\uc2dc\ub178\uc2a4 2400 \uae30\ubc18 \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \uac8c\uc784 \ud504\ub9ac\ubdf0 \uc601\uc0c1 \uacf5\uac1c\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n - \uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4 \uacf5\uac1c\u2026\ucd5c\ub300 4\ubc30 \uc90c\uc5d0\ub3c4 \ucd08\uace0\ud654\uc9c8, AI \ud2b8\ub798\ud0b9 \uae30\uc220 \uc801\uc6a9\n - \ucc28\uc138\ub300 \ucc28\ub7c9\uc6a9 \uc194\ub8e8\uc158, \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c(NTN) \uc601\uc0c1 \uc2dc\uc5f0",
                "Section: Quick Start Guide\\Quick Start Guide\\Basic Workflow\nContent:\nFollowing figure illustrates the three steps for converting and executing an NN model:\n```mermaid\nflowchart LR\n\tsubgraph \"ENN SDK Service\"\n\t\tdirection LR\n\t\tconvert(\"Convert The Model\")\n\tend\n\tsubgraph \"ENN Framework\"\n\t\tdirection LR\n\t\texecute(\"Execute The Model\")\n\tend\n\tmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```\n### Model Conversion\nTo convert TensorFlow Lite models, ENN SDK provides an online conversion tool through the Samsung Exynos Developer Society.\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.\nFor more information on the process of converting NN models, refer to Converting NN Models with ENN SDK Service.\n### Model Execution\nTo execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe ENN SDK provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.\nFor more information on the process of executing NN models, refer to Executing Models Using Native Program and Executing Models Using Android Application.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc0dd\uc131\ud615 \uc778\uacf5\uc9c0\ub2a5(AI) \ubc0f \uac01\uc885 \ucca8\ub2e8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ub4f1\uc7a5\uc73c\ub85c \uc778\ud574 \uc2dc\uc7a5\uc5d0\uc11c\ub294 \ud6a8\uc728\uc131, \ud655\uc7a5\uc131, \uc800\uc804\ub825\uc744 \uac16\ucd98 \ub354\uc6b1 \uc2a4\ub9c8\ud2b8\ud558\uace0 \uac15\ub825\ud55c \ucef4\ud4e8\ud305\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4. \uc774\uc5d0 \ub530\ub77c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\uc758 \uc8fc\uc694 \ud14c\ub9c8 \uc911 \ud558\ub098\uc778 \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\ub97c \uc8fc\uc81c\ub85c \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220, \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158, \uc0bc\uc131 \uc624\ud1a0 SoC(System-on-Chips) \ub4f1 \uc138 \uac1c\uc758 \uac15\uc5f0\uc774 \uc9c4\ud589\ub418\uc5c8\ub2e4.\n**\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220**\n\uccab\ubc88\uc9f8 \uc138\uc158\uc778 \"\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220\"\uc740 \uc0bc\uc131 \uc624\uc2a4\ud2f4 \uc5f0\uad6c\uc13c\ud130 \ubca0\ub2c8 \uce74\ud2f0\ube44\uc548(Benny Katibian) \ubd80\uc0ac\uc7a5\uc774 \uc9c4\ud589\ud588\ub2e4",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub610\ud55c \uc790\ub3d9\ucc28 \uae30\uc220\uc774 \ub354\uc6b1 \ubc1c\uc804\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub294 \uc0c1\ud669\uc5d0\uc11c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uac01 \uc601\uc5ed\uc758 \uc5ec\ub7ec \uc774\uae30\uc885 OS\ub97c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ud560 \uc218 \uc788\uc5b4 \ud68d\uae30\uc801\uc778 \ubbf8\ub798\ub97c \uc900\ube44\ud560 \uc218 \uc788\ub2e4. \ubc29 \uc0c1\ubb34\ub294 \"\uc0bc\uc131 \uc624\ud1a0 SoC\uc758 \uc911\uc559 \ucef4\ud4e8\ud305 \uae30\ub2a5 \ub355\ubd84\uc5d0 \ud55c\ub54c \uc0c1\uc0c1 \uc18d\uc5d0\ub9cc \uc874\uc7ac\ud588\ub358 \uac83\ub4e4\uc774 \uace7 \ud604\uc2e4\uc774 \ub420 \uac83\"\uc774\ub77c\uba70 \ubbf8\ub798\uc5d0 \ub300\ud55c \ubc1d\uc740 \uc804\ub9dd\uc744 \ub4dc\ub7ec\ub0c8\ub2e4.\n**\ud604\uc2e4 \uc138\uacc4\uc758 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158**\n\ub354\uc6b1 \uac15\ub825\ud55c SoC\uc640 \uc13c\uc11c \uae30\ub2a5\uc774 \uac1c\ubc1c\ub428\uc5d0 \ub530\ub77c \ucd5c\uc2e0 \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc810\uc810 \ub354 \uc2a4\ub9c8\ud2b8\ud574\uc9c0\uace0 \ub192\uc740 \uc5f0\uc0b0 \ub2a5\ub825\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent:  \uc2a4\uce74\uc77c\ub85c\uc0ac(Skylo Technologies): \uc778\uacf5\uc704\uc131\uc744 \uc0ac\uc6a9\ud558\ub294 \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c (NTN, Non-Terrestrial Networks)\uc911 NTN NB-IoT \uae30\ub2a5\uc744 \ubcc4\ub3c4\uc758 IoT\uce69\uc774 \uc5c6\uc774 \uc9c0\uc6d0\ud558\ub294 \ucc28\uc138\ub300 5G \ubaa8\ub380\uc73c\ub85c \uc778\uacf5\uc704\uc131 \uc2e0\ud638 \uc218\uc2e0\uae30\ub97c \uc81c\uc791\n\uc774\uc678\uc5d0\ub3c4 \uc0bc\uc131\uc804\uc790\ub294 \ucd08\uad11\ub300\uc5ed(UWB, Ultra-Wideband) \uae30\uc220\uc744 \ud65c\uc6a9\ud55c '\uc5d1\uc2dc\ub178\uc2a4 \ucee4\ub125\ud2b8(Exynos Connect) U100\u00b72\uc5b5 \ucd08\uace0\ud654\uc18c \uc774\ubbf8\uc9c0\uc13c\uc11c '\uc544\uc774\uc18c\uc140 HP2'\u00b7QD OLED \ud654\uc9c8\uc744 \uc704\ud55c \ub514\uc2a4\ud50c\ub808\uc774IC\u00b7IoT \ubcf4\uc548 \uc194\ub8e8\uc158\u00b7\ubb34\uc120\ucda9\uc804\ud5a5 \uc804\ub825\uad00\ub9acIC(PMIC)\u00b7\uc2a4\ub9c8\ud2b8 \ud5ec\uc2a4 \ud504\ub85c\uc138\uc11c \ub4f1 \ub2e4\uc591\ud55c \ucc28\uc138\ub300 \ubc18\ub3c4\uccb4 \uc194\ub8e8\uc158\uc744 \uc18c\uac1c\ud588\ub2e4.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent:  \ub9ac\ud50c\ub809\uc158/\uc250\ub3c4\uc6b0 \ub80c\ub354\ub9c1(Reflection/Shadow Rendering): \ube5b\uc758 \ubc18\uc0ac\ud6a8\uacfc, \uadf8\ub9bc\uc790 \uacbd\uacc4\ub97c \ud604\uc2e4 \uc138\uacc4\uc640 \uc720\uc0ac\ud558\uac8c \uc790\uc5f0\uc2a4\ub7fd\uac8c \ud45c\ud604\ud558\ub294 \uae30\ubc95\n\n\ub610 \uc0bc\uc131\uc804\uc790\ub294 '\uc5d1\uc2dc\ub178\uc2a4 2400'\uc744 \ub808\ud37c\ub7f0\uc2a4 \uae30\uae30\uc5d0 \ud0d1\uc7ac\ud574 \ud5a5\ud6c4 \uc2a4\ub9c8\ud2b8\ud3f0\uc5d0 \uc801\uc6a9\ub420 \ubb38\uc790\ub97c \uc774\ubbf8\uc9c0\ub85c \ubcc0\ud658\ud558\ub294 \uc0c8\ub85c\uc6b4 \uc0dd\uc131\ud615 AI \uae30\uc220\ub3c4 \uc120\ubcf4\uc600\ub2e4.\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc0bc\uc131\uc804\uc790 \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n_* *_\n\uc0bc\uc131\uc804\uc790\ub294 \uc774\ub0a0 \uc5d1\uc2dc\ub178\uc2a4 \uc624\ud1a0(Exynos Auto)\u00b7\uc544\uc774\uc18c\uc140 \uc624\ud1a0(ISOCELL Auto)\u00b7\uc544\uc774\uc18c\uc140 \ube44\uc804(ISOCELL Vizion) \ub4f1 \ub2e4\uc591\ud55c \ucc28\uc138\ub300 \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc81c\ud488 \uae30\uc220\uc744 \uc2dc\uc5f0\ud574 \uace0\uac1d\uc0ac\uc640 \ud30c\ud2b8\ub108\uc0ac\ub4e4\ub85c\ubd80\ud130 \ud638\ud3c9\uc744 \ubc1b\uc558\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uc774 \uac1c\ub150\uc740 \uc13c\uc11c\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\uace0, \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uace0, \ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790\uc5d0\uac8c \ub354 \ub098\uc740 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud558\ub294 \uc138 \ub2e8\uacc4\ub85c \uc138\ubd84\ud654\ud560 \uc218 \uc788\ub2e4.\n\ubaa8\ub4e0 \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub824\uba74 \ub9ce\uc740 \uc5f0\uc0b0\uc774 \ud544\uc694\ud558\ubbc0\ub85c \uc804\ub825 \uc18c\ube44\uac00 \uc99d\uac00\ud55c\ub2e4. \ubc14\ub85c \uc774 \ubd80\ubd84\uc5d0\uc11c \uc0bc\uc131\uc758 \uae30\uc220\uc801 \ud601\uc2e0\uc774 \ube5b\uc744 \ubc1c\ud55c\ub2e4. \uc0bc\uc131\uc740 \uc804\uc6a9 \ub3c4\uba54\uc778\ubcc4 \uc2dc\uc2a4\ud15c\uc744 \ud3ec\ud568\ud558\ub294 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\ub97c \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub294 \ub354 \uac04\ub2e8\ud558\uba74\uc11c \ud2b9\uc218\ud55c \uc5f0\uc0b0 \ubd80\ud558\ub97c \ucc98\ub9ac\ud558\uae30 \ub54c\ubb38\uc5d0 \uae30\ubcf8\uc801\uc73c\ub85c \ud6a8\uc728\uc131\uc774 \ub192\ub2e4. \uc774 \uc2dc\uc2a4\ud15c\uc740 \uc804\ub825 \uc18c\ube44\uac00 \uc801\uace0 \ucc98\ub9ac \uc9c0\uc5f0 \uc2dc\uac04\uc774 \uc9e7\ub2e4.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\uac00 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \ubbf8\uc8fc\ucd1d\uad04(Device Solutions America office)\uc5d0\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023'\uc744 \uac1c\ucd5c\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \ucd08\uc9c0\ub2a5\ud654(Hyper-Intelligence)\u00b7\ucd08\uc5f0\uacb0\uc131(Hyper-Connected)\u00b7\ucd08\ub370\uc774\ud130(Hyper-Data)\ub97c \uac00\ub2a5\ud558\uac8c \ud560 \uc8fc\uc694 \uc751\uc6a9\ucc98\ubcc4 \ucd5c\uc2e0 \ubc18\ub3c4\uccb4 \uc124\uacc4 \ud604\ud669\uacfc \ube44\uc804\uc744 \uacf5\uc720\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \uace0\uac1d\uc0ac\uc640 \ud30c\ud2b8\ub108\uc0ac \uad00\uacc4\uc790 300\uc5ec\uba85\uc774 \ucc38\uc11d\ud55c \uac00\uc6b4\ub370, \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \ubd84\uc57c \uae00\ub85c\ubc8c \uc804\ubb38\uac00, \uc11d\ud559\ub4e4\uacfc \ud568\uaed8 \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM, Large Language Model) \uae30\uc220\uc5d0 \ub300\ud55c \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\ub97c \uc9c4\ud589\ud588\ub2e4.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Converting NN Models with ENN SDK Service\nContent:\nIn this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.\nTo get started, you must be a member of the Exynos Developer Society.\n- If you are not a member of the Exynos Developer Society, sign up from here.\n- If you already have an account, log in to the Exynos Developer Society.\nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from here.\n2. Navigate to the ENN SDK service page and provide a title for your project.\n3. Then, upload the downloaded TFLite model\n4. Next, select hardware preferences.\n\t- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n\t- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n5. After confirming your selections, click **Convert** to convert the model.\n6. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n7. Click **NNC Download** to download the converted NNC model.\n\tYou can now integrate the NNC model into the desired application.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\ub2a5\uc744 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub3c4\uc785\ud558\uae30 \uc704\ud55c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 IP\uac1c\ubc1c \ubc29\ud5a5\uc131\uc744 \uc18c\uac1c\ud588\ub2e4.\n**\ucd5c\uc801\ud654\ub41c 4\ud074\ub7ec\uc2a4\ud130 \uad6c\uc870\uc758 CPU**\n\uc138 \uac1c\uc758 \uc8fc\uc694 IP \uc911 \uccab \ubc88\uc9f8\ub294 CPU\ub85c, \uae30\uc874\uc5d0\ub294 \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud558\uc774\uc5d4\ub4dc(high-end) \ud074\ub7ec\uc2a4\ud130, \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ub85c\uc6b0\uc5d4\ub4dc(low-end) \ud074\ub7ec\uc2a4\ud130, \uc911\uac04 \ud074\ub7ec\uc2a4\ud130 \ub4f1 \uc138 \uac1c\uc758 \ud074\ub7ec\uc2a4\ud130\ub85c \uad6c\uc131\ub418\uc5c8\ub2e4. \uc0bc\uc131\uc740 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 CPU \uc6b4\uc601\uc744 \uc704\ud574 \uc911\uac04 \ud074\ub7ec\uc2a4\ud130\ub97c \ubbf8\ub4dc \ud558\uc774(mid-high)\uc640 \ubbf8\ub4dc \ub85c\uc6b0(mid-low)\uc758 \ub450 \uac00\uc9c0 \uacc4\uce35\uc73c\ub85c \ub098\ub204\uace0, \ubbf8\ub4dc \ud558\uc774 \uacc4\uce35\uc740 \uac8c\uc784\uacfc \uac19\uc740 \ucef4\ud4e8\ud305 \uc9d1\uc57d\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud588",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\ub294 2\uc5b5 \ud654\uc18c \uc774\ubbf8\uc9c0\uc13c\uc11c \uae30\ubc18 \ucd08\uace0\ud574\uc0c1\ub3c4 \ud2b9\uc218 \uc90c \uae30\uc220\uc778 '\uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4(Zoom Anyplace)'\ub97c \ucc98\uc74c \uacf5\uac1c\ud588\ub2e4. \uc774 \uae30\uc220\uc740 \uc6c0\uc9c1\uc774\ub294 \uc0ac\ubb3c\uc5d0 \ub300\ud574 \ud480\uc2a4\ud06c\ub9b0\uacfc \ucd5c\ub300 4\ubc30 \ud074\ub85c\uc988\uc5c5 \uc7a5\uba74\uae4c\uc9c0 \ud654\uc9c8 \uc800\ud558\uc5c6\uc774 \ub3d9\uc2dc\uc5d0 \ucd2c\uc601\ud560 \uc218 \uc788\uace0, \ud074\ub85c\uc988\uc5c5\uc2dc AI \uae30\uc220\ub85c \uc0ac\ubb3c\uc744 \uc790\ub3d9 \ucd94\uc801\ud560 \uc218 \uc788\uc5b4 \ubaa8\ubc14\uc77c \uc0ac\uc6a9\uc790\uc5d0\uac8c \uc644\uc804\ud788 \uc0c8\ub85c\uc6b4 \uce74\uba54\ub77c \uc90c \uacbd\ud5d8\uc744 \uc81c\uacf5\ud55c\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 2025\ub144 \uc591\uc0b0 \uc608\uc815\uc778 \ucc28\uc138\ub300 \ud504\ub9ac\ubbf8\uc5c4 \uc778\ud3ec\ud14c\uc778\uba3c\ud2b8(IVI, In-Vehicle Infotainment)\uc6a9 \ud504\ub85c\uc138\uc11c '\uc5d1\uc2dc\ub178\uc2a4 \uc624\ud1a0 V920' \uad6c\ub3d9 \uc601\uc0c1\uc744 \uacf5\uac1c\ud588\ub2e4.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc800\uc804\ub825 \ubc0f \uc800\uc9c0\uc5f0\uc73c\ub85c \uc0c8\ub85c\uc6b4 \uac00\ub2a5\uc131\uc744 \uc5f4\ub2e4**\n\uc774\uc5d0 \ub530\ub77c \uc2dc\uc2a4\ud15c\uc740 \uc6d0\uc2dc \ub370\uc774\ud130\uc5d0\uc11c \uc0c1\ud669 \uc815\ubcf4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc0dd\uc131\ud560 \uc218 \uc788\uc73c\uba70, \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc774\ub97c \ud65c\uc6a9\ud558\uc5ec \ub3d9\uc791\uc744 \uc870\uc815\ud560 \uc218 \uc788\ub2e4. \ubaa8\ubc14\uc77c \uce74\uba54\ub77c\uac00 \uae30\uc874 \ub514\uc9c0\ud138 \uce74\uba54\ub77c\ub97c \ub530\ub77c\uc7a1\uc744 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \uc801\uc751\ud615 \uae30\uc220\uc774 \uc8fc\ubaa9\uc744 \ubc1b\uace0 \uc788\ub2e4. \uc0bc\uc131\uc758 \uc804\uc6a9 \uc2dc\uc2a4\ud15c\uc740 \ub85c\uceec \ubaa8\uc158 \ucd94\uc815 \ubc0f \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560\uc744 \ud1b5\ud574 \uc774\ubbf8\uc9c0 \ubc0f \ube44\ub514\uc624 \ud488\uc9c8\uc744 \uac1c\uc120\ud558\ub294 \uc0c1\ud669 \uc778\uc2dd \ucc98\ub9ac \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\uba70, \uc77c\ubc18 \ud504\ub85c\uc138\uc11c\ubcf4\ub2e4 \ucd5c\ub300 5\ubc30 \uc791\uc740 \ud06c\uae30\ub85c \uc774\ub97c \uc218\ud589\ud55c\ub2e4. \uc0bc\uc131\uc758 \ucd5c\uc801\ud654\ub41c \uc2dc\uc2a4\ud15c\uc744 \ud1b5\ud574 \uc0c1\uc2dc \uc791\ub3d9 \uce74\uba54\ub77c\ub294 \uc774\uc81c \ucd5c\uc18c 30uW\uc758 \uc804\ub825\uc744 \uc18c\ube44\ud558\ubbc0\ub85c \ubc30\ud130\ub9ac \uc7ac\ucda9\uc804 \uc5c6\uc774\ub3c4 \uc77c\uc8fc\uc77c \ub0b4\ub0b4 \ube44\ub514\uc624\ub97c \uc2a4\ud2b8\ub9ac\ubc0d\ud560 \uc218 \uc788\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc548\uc804\uc5d0 \ub300\ud55c \uc0bc\uc131\uc758 \ud655\uace0\ud55c \uc758\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ucc28\uc138\ub300 \uc624\ud1a0 SoC\uc758 \uc138\uc774\ud504\ud2f0 \uc544\uc77c\ub79c\ub4dc(safety island)\ub294 ASIL-D\ub97c \uc900\uc218\ud558\uba70, \ud638\uc2a4\ud2b8 CPU\uc640 \ubcc4\ub3c4\ub85c \uc791\ub3d9\ud558\uc5ec \ub2e4\ub978 SoC\uc758 \uc0c1\ud0dc\ub97c \ubaa8\ub2c8\ud130\ub9c1\ud560 \uc608\uc815\uc774\ub2e4. \uc774\uc678\uc758 \ubd80\ubd84\uc740 ASPICE, ISO 26262 \ubc0f FMEA\uc640 \uac19\uc740 \uc790\ub3d9\ucc28 \ud45c\uc900\uc744 \ud1b5\ud574 ASIL-B\ub97c \uc900\uc218\ud558\ub3c4\ub85d \uc81c\uc791\ub418\uc5c8\ub2e4.\n\uc0ac\uc774\ubc84 \ubcf4\uc548 \ub610\ud55c \uc810\uc810 \ub354 \uc911\uc694\ud55c \uc694\uc18c\ub85c \ub5a0\uc624\ub974\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0, \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uc554\ud638\ud654 \uc5d4\uc9c4\uc774 \ub0b4\uc7a5\ub41c \uae30\ubcf8 \ubcf4\uc548 \ud504\ub85c\uc138\uc11c\uc640 StrongBOX \ud558\ub4dc\uc6e8\uc5b4 \ube14\ub85d\uc744 \uac16\ucd94\uace0 \uc788\ub2e4. \ub610\ud55c \uc0bc\uc131\uc740 \uc0ac\uc6a9\uc790 \uc815\ubcf4\ub97c \ubcf4\ud638\ud558\ub294 \uc790\uccb4 \uc6b4\uc601\uccb4\uc81c\uc778 \uc5d1\uc2dc\ub178\uc2a4TEE\ub97c \uac1c\ubc1c\ud588\uc73c\uba70 \uc774\ubbf8 EAL2 \uc778\uc99d\uc744 \ubc1b\uc558",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
                "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc790\ub3d9\ucc28 \uc544\ud0a4\ud14d\ucc98\ub97c \uc911\uc559 \ucef4\ud4e8\ud305\uc73c\ub85c \uc804\ud658**\n\ubc29 \uc0c1\ubb34\ub294 \ucc28\ub7c9\uc774 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c \ub3c4\uba54\uc778 \uc911\uc559 \uc9d1\uc911\uc2dd \uc2dc\uc2a4\ud15c\uc73c\ub85c, \uadf8\ub9ac\uace0 \uc774\uc81c\ub294 \ud1b5\ud569 \uc911\uc559 \ucef4\ud4e8\ud305\uc73c\ub85c \ubc1c\uc804\ud574 \uc628 \uacfc\uc815\uc5d0 \ub300\ud574 \uc124\uba85\ud588\ub2e4. \uc774\ub7ec\ud55c \ubcc0\ud654\ub85c \uc778\ud574 \ud6a8\uc728\uc131\uc774 \ub192\uc544\uc9c0\uace0 \uac04\uc18c\ud654\ub418\uc5c8\uc9c0\ub9cc \uace0\ub3c4\ub85c \uc5f0\uacb0\ub41c \uc544\ud0a4\ud14d\ucc98\uc758 \ud2b9\uc131\uc73c\ub85c \uc778\ud574 \uc548\uc804\uacfc \uac19\uc740 \uc601\uc5ed\uc5d0 \ub300\ud55c \uc0c8\ub85c\uc6b4 \uae30\uc220\uc801 \uace0\ub824\uc0ac\ud56d\uc774 \ubc1c\uc0dd\ud588\ub2e4. \uc774\uc5d0 \uc644\ubcbd\ud558\uac8c \ub300\uc751\ud558\uae30 \uc704\ud574 \ucc28\uc138\ub300 \uc0bc\uc131 \uc624\ud1a0 SoC\ub294 \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc9c0\ud5a5\ud558\uba70 \uc548\uc804\uc131, \ubcf4\uc548, \ud655\uc7a5\uc131\uc744 \uac15\ud654\ud55c \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud558\uace0 \uc788\ub2e4.\n_*\uac15\ud654\ub41c \uc548\uc804 \ubc0f \ubcf4\uc548 *_",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc774 \uc81c\ud488\uc740 Arm\uc758 \ucd5c\uc2e0 \uc804\uc7a5\uc6a9 \uc911\uc559\ucc98\ub9ac\uc7a5\uce58(CPU) \ucf54\uc5b4\ud14d\uc2a4-A78AE 10\uac1c\ub97c \uae30\ubc18\uc73c\ub85c \uc774\uc804 \uc81c\ud488 \ub300\ube44 1.7\ubc30 \uac15\ud654\ub41c CPU \uc131\ub2a5\uacfc \ucd5c\ub300 6\uac1c\uc758 \uace0\ud654\uc9c8 \ub514\uc2a4\ud50c\ub808\uc774\uc5d0 \ub3d9\uc2dc \uc5f0\uacb0 \uac00\ub2a5\ud55c \uba40\ud2f0 \ucee4\ub125\ud2f0\ube44\ud2f0 \uae30\ub2a5\uc73c\ub85c \uc2a4\ub9c8\ud2b8\ud558\uace0 \uc990\uac70\uc6b4 \ubaa8\ube4c\ub9ac\ud2f0 \uacbd\ud5d8\uc744 \uc81c\uacf5\ud55c\ub2e4.\n\ub610 \uc0bc\uc131\uc804\uc790\ub294 \ucc28\ub7c9\uc6a9 \uc774\ubbf8\uc9c0\uc13c\uc11c\ud5a5 '\uc544\uc774\uc18c\uc140 \uc624\ud1a0'\uc640 \uc0ac\ubb3c\uc758 \ube60\ub978 \uc6c0\uc9c1\uc784\uc744 \uc21c\uac04\uc801\uc73c\ub85c \uc815\ud655\ud558\uac8c \ud3ec\ucc29 \uac00\ub2a5\ud55c '\uc544\uc774\uc18c\uc140 \ube44\uc804' \uc81c\ud488\uc744 \ud1b5\ud574 \uc548\uc804 \uc8fc\ud589 \uae30\uc220\ub3c4 \uc120\ubcf4\uc600\ub2e4.\n\uc0bc\uc131\uc804\uc790\uac00 \uc774\ub0a0 \uacf5\uac1c\ud55c '\uc544\uc774\uc18c\uc140 \uc624\ud1a0 1H1'\uc740 \ub2e4\uc591\ud55c \uc8fc\ud589, \uc870\ub3c4 \ud658\uacbd\uc5d0\uc11c \ub3c4\ub85c\uc640 \uc0ac\ubb3c\uc744 \uc815\ud655\ud788 \ud310\ub2e8\ud560 \uc218 \uc788\ub294 120dB HDR\uc744 \uc9c0\uc6d0\ud558\uace0, \uc2e0\ud638\ub4f1\uc758 \uae5c\ube61\uc784 \ud604\uc0c1 \ub4f1 'LED \ud50c\ub9ac\ucee4(LED Flicker)'\ub97c \uc644\ud654\ud560 \uc218 \uc788\ub2e4.",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\ub294 \uc6b4\uc804\uc790\uc5d0\uac8c \ucd5c\uace0\uc758 \ubaa8\ube4c\ub9ac\ud2f0 \uacbd\ud5d8\uc744 \uc81c\uacf5\ud558\ub294 \ucc28\uc138\ub300 \ucc28\ub7c9\uc6a9 \ud575\uc2ec \ubc18\ub3c4\uccb4\ub97c \ud1b5\ud574 \uc804\uc7a5 \uae30\uc220 \uacbd\uc7c1\ub825\uc744 \uac15\ud654\ud574 \ub098\uac08 \uacc4\ud68d\uc774\ub2e4.\n\ud55c\ud3b8, \uc0bc\uc131\uc804\uc790\ub294 \uc774\ub0a0 \ud589\uc0ac\uc5d0\uc11c \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c(NTN, Non-Terrestrial Network) \uc0ac\uc5c5\uc790 '\uc2a4\uce74\uc77c\ub85c \uc77c\ub809\ud2b8\ub85c\ub2c9\uc2a4(Skylo Electronics)'\uc640 \ud568\uaed8 \ucc28\uc138\ub300 5G \ubaa8\ub380\uc744 \ud1b5\ud574 \ubaa8\ubc14\uc77c \uae30\uae30\uc640 \uc778\uacf5\uc704\uc131\uc744 5G\ub85c \uc5f0\uacb0\ud558\ub294 \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c \ud1b5\uc2e0\uc744 \uc120\ubcf4\uc774\uba70, \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc0bc\uc131\uc804\uc790\uc758 \ubb34\uc120 \ud1b5\uc2e0 \uae30\uc220 \ub9ac\ub354\uc2ed\uc744 \uac15\uc870\ud588\ub2e4.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc5d1\uc2dc\ub178\uc2a4 \ub808\ud37c\ub7f0\uc2a4 \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub41c \ubaa8\ubc14\uc77c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5 \ub370\ubaa8 \uc601\uc0c1\uc744 \ud1b5\ud574 \uadf8\ub9bc\uc790, \ubc18\uc0ac \ub4f1\uacfc \uac19\uc740 \uc5ec\ub7ec \uac1c\uc758 \uc138\ubd80 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5\ub4e4\uc774 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.\n**\uc0dd\uc131\ud615 AI \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 NPU**\n\uc0bc\uc131\uc758 \ucd5c\uc2e0 NPU \uc194\ub8e8\uc158\uc740 \uba54\ubaa8\ub9ac \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcc0\uacbd\ud558\uace0 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \ube44\uc120\ud615 \uc5f0\uc0b0\uc758 \ud65c\uc6a9\ub3c4\ub97c \ud06c\uac8c \ub192\uc77c \uac83\uc774\ub2e4",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uacfc\uc911\ud55c \uac8c\uc784 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc804\ub825\uacfc \uc131\ub2a5\uc744 \ub354\uc6b1 \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 CPU \uc804\ub825 \ubd80\ubd84\uc744 \uc904\uc774\uace0 \ub354 \ub9ce\uc740 \uc804\ub825\uc744 CPU \ucf54\uc5b4\uc5d0\uc11c GPU\ub85c \uc7ac\ud560\ub2f9\ud558\uc5ec \ubaa8\ubc14\uc77c \uc7a5\uce58\uc5d0\uc11c \ucd5c\uace0 \uc218\uc900\uc758 \uac8c\uc784 \uadf8\ub798\ud53d\uc744 \uc9c0\uc6d0\ud560 \uc218 \uc788\ub3c4\ub85d \ucef4\ud4e8\ud305 \uc131\ub2a5\uc744 \ub192\uc600\ub2e4.\n**\ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub418\ub294 \ud48d\ubd80\ud55c \uadf8\ub798\ud53d \uacbd\ud5d8**\n\uc0bc\uc131\uc740 \ud504\ub9ac\ubbf8\uc5c4\ubd80\ud130 \uc800\uac00\ud615\uc5d0 \uc774\ub974\ub294 \ub113\uc740 \ubc94\uc704\uc758 \ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0 \ucf58\uc194 \uc218\uc900\uc758 \uac8c\uc784\uc744 \uc81c\uacf5\ud55c\ub2e4\ub294 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 AMD\uc758 RDNATM \uc544\ud0a4\ud14d\ucc98\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 Xclipse GPU\ub97c \uac1c\ubc1c\ud574\uc654\uace0, \uc774\ub97c \ud1b5\ud574 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(ray tracing) \uae30\ub2a5\uc744 \ucd5c\ucd08\ub85c \ub3c4\uc785\ud588\ub2e4",
                "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs."
            ]
        ],
        "answer": "Samsung Electronics introduces the Exynos 2200, a premium mobile processor featuring the Samsung Xclipse GPU based on AMD RDNA 2 architecture, crafted using a 4-nanometer EUV process. This processor enhances mobile gaming, social media, and photography, offering the first mobile hardware-accelerated ray tracing for realistic lighting and variable rate shading for smoother gameplay. It boasts an upgraded neural processing unit (NPU), Arm's latest Armv9 CPU cores for superior performance and security, and a fast 5G modem compliant with 3GPP Release 16. With integrated Secure Element (iSE) and robust encryption, it ensures data security. The processor's ISP supports up to 200MP resolution, AI-integrated camera functionalities for professional-quality images, an advanced codec for up to 8K video, and a display solution with HDR10+ and up to 144Hz refresh rates, ensuring a comprehensive and immersive visual experience.",
        "ground_truth": "Samsung Electronics introduces the Exynos 2200, a premium mobile processor featuring the Samsung Xclipse GPU based on AMD RDNA 2 architecture, crafted using a 4-nanometer EUV process. This processor enhances mobile gaming, social media, and photography, offering the first mobile hardware-accelerated ray tracing for realistic lighting and variable rate shading for smoother gameplay. It boasts an upgraded neural processing unit (NPU), Arm's latest Armv9 CPU cores for superior performance and security, and a fast 5G modem compliant with 3GPP Release 16. With integrated Secure Element (iSE) and robust encryption, it ensures data security. The processor's ISP supports up to 200MP resolution, AI-integrated camera functionalities for professional-quality images, an advanced codec for up to 8K video, and a display solution with HDR10+ and up to 144Hz refresh rates, ensuring a comprehensive and immersive visual experience."
    },
    {
        "question": "NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?",
        "contexts": [
            [
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc5d1\uc2dc\ub178\uc2a4 \ub808\ud37c\ub7f0\uc2a4 \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub41c \ubaa8\ubc14\uc77c \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5 \ub370\ubaa8 \uc601\uc0c1\uc744 \ud1b5\ud574 \uadf8\ub9bc\uc790, \ubc18\uc0ac \ub4f1\uacfc \uac19\uc740 \uc5ec\ub7ec \uac1c\uc758 \uc138\ubd80 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1 \uae30\ub2a5\ub4e4\uc774 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.\n**\uc0dd\uc131\ud615 AI \uc2dc\ub300\ub97c \uc704\ud55c \ucca8\ub2e8 NPU**\n\uc0bc\uc131\uc758 \ucd5c\uc2e0 NPU \uc194\ub8e8\uc158\uc740 \uba54\ubaa8\ub9ac \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcc0\uacbd\ud558\uace0 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \ube44\uc120\ud615 \uc5f0\uc0b0\uc758 \ud65c\uc6a9\ub3c4\ub97c \ud06c\uac8c \ub192\uc77c \uac83\uc774\ub2e4",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\nContent:\nThe data such as the model file and input data are required for executing the program.\nThe model can be directly loaded by the framework using its path.\nHowever, the input data must be copied to memory.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\ub2a5\uc744 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub3c4\uc785\ud558\uae30 \uc704\ud55c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 IP\uac1c\ubc1c \ubc29\ud5a5\uc131\uc744 \uc18c\uac1c\ud588\ub2e4.\n**\ucd5c\uc801\ud654\ub41c 4\ud074\ub7ec\uc2a4\ud130 \uad6c\uc870\uc758 CPU**\n\uc138 \uac1c\uc758 \uc8fc\uc694 IP \uc911 \uccab \ubc88\uc9f8\ub294 CPU\ub85c, \uae30\uc874\uc5d0\ub294 \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud558\uc774\uc5d4\ub4dc(high-end) \ud074\ub7ec\uc2a4\ud130, \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ub85c\uc6b0\uc5d4\ub4dc(low-end) \ud074\ub7ec\uc2a4\ud130, \uc911\uac04 \ud074\ub7ec\uc2a4\ud130 \ub4f1 \uc138 \uac1c\uc758 \ud074\ub7ec\uc2a4\ud130\ub85c \uad6c\uc131\ub418\uc5c8\ub2e4. \uc0bc\uc131\uc740 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 CPU \uc6b4\uc601\uc744 \uc704\ud574 \uc911\uac04 \ud074\ub7ec\uc2a4\ud130\ub97c \ubbf8\ub4dc \ud558\uc774(mid-high)\uc640 \ubbf8\ub4dc \ub85c\uc6b0(mid-low)\uc758 \ub450 \uac00\uc9c0 \uacc4\uce35\uc73c\ub85c \ub098\ub204\uace0, \ubbf8\ub4dc \ud558\uc774 \uacc4\uce35\uc740 \uac8c\uc784\uacfc \uac19\uc740 \ucef4\ud4e8\ud305 \uc9d1\uc57d\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud588",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc911\uc559 \ucef4\ud4e8\ud305\uc5d0 \ub300\ud55c \uc218\uc694\uac00 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ud558\ub4dc\uc6e8\uc5b4\uc640 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \ud655\uc7a5\uc131\uc774 \ub354\uc6b1 \uc911\uc694\ud574\uc9c8 \uac83\uc774\ub2e4. \ub530\ub77c\uc11c \uc0bc\uc131\uc758 \ud575\uc2ec \uc194\ub8e8\uc158 \uc911 \ud558\ub098\ub294 \uc18c\ud504\ud2b8\uc6e8\uc5b4\ub97c \uc218\uc815\ud558\uc9c0 \uc54a\uace0\ub3c4 \ucef4\ud4e8\ud305 \uc6a9\ub7c9\uc744 \ub450 \ubc30\ub85c \ub298\ub9ac\uae30 \uc704\ud574 \ub450 SoC \uac04\uc758 \uc9c1\uc811 \uc5f0\uacb0(die-to-die)\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. \ub610\ud55c \uc624\ud1a0 SoC\ub294 OEM\uc758 \uc2dc\uc2a4\ud15c\uc5d0 \ub530\ub77c \ud328\ud0a4\uc9c0 \uac04 \uba40\ud2f0 SoC \uc5f0\uacb0\uc744 \uc704\ud574 PCIe \ub610\ub294 \uc774\ub354\ub137 \uc0ac\uc6a9\uc744 \uc9c0\uc6d0\ud55c\ub2e4.\n_*AI \ud1b5\ud569, \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc815\uc758 \ucc28\ub7c9\uc758 \ubbf8\ub798\ub97c \uc704\ud55c \uc900\ube44 *_\n\uc55e\uc73c\ub85c \uc6b4\uc804\uc790\ub294 \ucd5c\ub300 150\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uc2e4\uc2dc\uac04\uc73c\ub85c \uc791\ub3d9\ud560 \uc218 \uc788\ub294 \uc0bc\uc131\uc758 \uc804\uc6a9 AI \uac00\uc18d\uae30\ub97c \ud1b5\ud574 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0 \uae30\ubc18\ud55c AI \ube44\uc11c\uc640 \uc6d0\ud65c\ud558\uac8c \uc0c1\ud638 \uc791\uc6a9\ud560 \uc218 \uc788\uac8c \ub420 \uac83\uc774\ub2e4.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\nExecuting NN models on the ENN framework comprises of three steps such as initializing the framework, inferring the models, and deinitializing the framework.\nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.\n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\n\tA1[Initialize]\n\tA1 --> A2[Open Model]\n\tA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\n\tB1[Copy Input Layer]\n\tB1 --> B2[Execute Model]\n\tB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\n\tC1[Release Buffers]\n\tC1 --> C2[Close Model]\n\tC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```\nTo infer multiple data, repeat `Inference`.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uacfc\uc911\ud55c \uac8c\uc784 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc804\ub825\uacfc \uc131\ub2a5\uc744 \ub354\uc6b1 \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 CPU \uc804\ub825 \ubd80\ubd84\uc744 \uc904\uc774\uace0 \ub354 \ub9ce\uc740 \uc804\ub825\uc744 CPU \ucf54\uc5b4\uc5d0\uc11c GPU\ub85c \uc7ac\ud560\ub2f9\ud558\uc5ec \ubaa8\ubc14\uc77c \uc7a5\uce58\uc5d0\uc11c \ucd5c\uace0 \uc218\uc900\uc758 \uac8c\uc784 \uadf8\ub798\ud53d\uc744 \uc9c0\uc6d0\ud560 \uc218 \uc788\ub3c4\ub85d \ucef4\ud4e8\ud305 \uc131\ub2a5\uc744 \ub192\uc600\ub2e4.\n**\ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uad6c\ud604\ub418\ub294 \ud48d\ubd80\ud55c \uadf8\ub798\ud53d \uacbd\ud5d8**\n\uc0bc\uc131\uc740 \ud504\ub9ac\ubbf8\uc5c4\ubd80\ud130 \uc800\uac00\ud615\uc5d0 \uc774\ub974\ub294 \ub113\uc740 \ubc94\uc704\uc758 \ubaa8\ubc14\uc77c \ud50c\ub7ab\ud3fc\uc5d0 \ucf58\uc194 \uc218\uc900\uc758 \uac8c\uc784\uc744 \uc81c\uacf5\ud55c\ub2e4\ub294 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 AMD\uc758 RDNATM \uc544\ud0a4\ud14d\ucc98\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 Xclipse GPU\ub97c \uac1c\ubc1c\ud574\uc654\uace0, \uc774\ub97c \ud1b5\ud574 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub808\uc774 \ud2b8\ub808\uc774\uc2f1(ray tracing) \uae30\ub2a5\uc744 \ucd5c\ucd08\ub85c \ub3c4\uc785\ud588\ub2e4",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: _* *_\n**\u25a1 5\uc77c (\ud604\uc9c0\uc2dc\uac04), \uc751\uc6a9\ucc98\ubcc4 \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \ud604\ud669\u00b7\ube44\uc804 \ubc1c\ud45c**\n - \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\uc220 \uac1c\ubc1c \uad00\ub828 \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\n\n**\u25a1 \ucd5c\uc2e0 \uadf8\ub798\ud53d, \uc0dd\uc131\ud615 AI \uae30\uc220 \ud0d1\uc7ac\ud55c '\uc5d1\uc2dc\ub178\uc2a4 2400' \uacf5\uac1c**\n - \uc804\uc791 \ub300\ube44 CPU \uc131\ub2a5 1.7\ubc30, AI \uc131\ub2a5 14.7\ubc30 \ub300\ud3ed \ud5a5\uc0c1\n - \uc5d1\uc2dc\ub178\uc2a4 2400 \uae30\ubc18 \ucc28\uc138\ub300 \ubaa8\ubc14\uc77c \uac8c\uc784 \ud504\ub9ac\ubdf0 \uc601\uc0c1 \uacf5\uac1c\n\n**\u25a1 \ucd08\uc5f0\uacb0\uc2dc\ub300\ub97c \ud5a5\ud55c \uc2dc\uc2a4\ud15c\ubc18\ub3c4\uccb4 \uc124\uacc4 \uae30\uc220 \uacbd\uc7c1\ub825 \uc120\ubd2c**\n - \uc90c \uc560\ub2c8\ud50c\ub808\uc774\uc2a4 \uacf5\uac1c\u2026\ucd5c\ub300 4\ubc30 \uc90c\uc5d0\ub3c4 \ucd08\uace0\ud654\uc9c8, AI \ud2b8\ub798\ud0b9 \uae30\uc220 \uc801\uc6a9\n - \ucc28\uc138\ub300 \ucc28\ub7c9\uc6a9 \uc194\ub8e8\uc158, \ube44\uc9c0\uc0c1 \ub124\ud2b8\uc6cc\ud06c(NTN) \uc601\uc0c1 \uc2dc\uc5f0",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Channel Alignment\nMaintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources.\n| Architecture | Channel Alignment |\n| -- | -- |\n| Gen-4 | 32 |\n#### Bilinear Resize Parameters\nTo ensure optimal image resizing using bilinear interpolation, configure the following settings:\n- **Option A**:\n\t- Aligned corner: `False`\n\t- Half pixel centers: `False`\n\t- Performance: High speed\n- **Option B**:\n\t- Aligned corner: `True`\n\t- Half pixel centers: `False`\n\t- Compatibility: Gen-4 and later NPUs\n\t- Performance: Medium speed\n- **Option C**:\n\t- Aligned corner: `False`\n\t- Half pixel centers: `True`\n\t- Note: Requires workaround\n\t- Performance: Reduced speed",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Golden Data and Comparing it with Output (Optional)\nContent:\nAfter executing the model, load the golden binary data into memory and compare it with the data in the output memory location of model.\nThis optional step validates whether the output of model matches the expected results.\nUse the EnnAllocateAllBuffer function to obtain the output memory location.\n(example):\n```cpp\ntemplate <typename T>\nint compare_data(T *data1, T *data2, int size, T threshold) {\n\tint diff = 0;\n\tfor (int idx = 0; idx < size; idx++) {\n\t\tif (std::abs(data1[idx] - data2[idx]) > threshold) {\n\t\t\tdiff++;\n\t\t}\n\t}\n\treturn diff;\n}\n```",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Required Parameters\nContent:\nTo execute NNC models using the ENN framework, the following parameters are required:\n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector<string>|List of input file paths|\n|`goldens` (optional)|vector<string>|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-1\nContent:\n| Operator_Name | Kernel Size | Padding | Strides | Dilation | Boost Mode |\n| ------------------------------ | ----------- | -------- | -------- | --------- | ---------- |\n| CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | {1 ~ 27} |\n| DECONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DECONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc0dd\uc131\ud615 \uc778\uacf5\uc9c0\ub2a5(AI) \ubc0f \uac01\uc885 \ucca8\ub2e8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ub4f1\uc7a5\uc73c\ub85c \uc778\ud574 \uc2dc\uc7a5\uc5d0\uc11c\ub294 \ud6a8\uc728\uc131, \ud655\uc7a5\uc131, \uc800\uc804\ub825\uc744 \uac16\ucd98 \ub354\uc6b1 \uc2a4\ub9c8\ud2b8\ud558\uace0 \uac15\ub825\ud55c \ucef4\ud4e8\ud305\uc744 \uc694\uad6c\ud558\uace0 \uc788\ub2e4. \uc774\uc5d0 \ub530\ub77c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023\uc758 \uc8fc\uc694 \ud14c\ub9c8 \uc911 \ud558\ub098\uc778 \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\ub97c \uc8fc\uc81c\ub85c \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220, \uba40\ud2f0\ubbf8\ub514\uc5b4 \uc560\ud50c\ub9ac\ucf00\uc774\uc158, \uc0bc\uc131 \uc624\ud1a0 SoC(System-on-Chips) \ub4f1 \uc138 \uac1c\uc758 \uac15\uc5f0\uc774 \uc9c4\ud589\ub418\uc5c8\ub2e4.\n**\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220**\n\uccab\ubc88\uc9f8 \uc138\uc158\uc778 \"\uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\uc220\"\uc740 \uc0bc\uc131 \uc624\uc2a4\ud2f4 \uc5f0\uad6c\uc13c\ud130 \ubca0\ub2c8 \uce74\ud2f0\ube44\uc548(Benny Katibian) \ubd80\uc0ac\uc7a5\uc774 \uc9c4\ud589\ud588\ub2e4",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\Using Implemented Native Functions\nContent:\nTo execute the NN models with implemented native functions, perform the following steps.\n1. Preparing Execution of Model\n\t1. Initialize the framework using the `ennInitialize` function.\n\t2. Load the ML model into the framework using the `ennOpenModel` function.\n\t3. Allocate and commit the necessary buffers using the `ennAllocateAllBuffers` function.\n2. Executing a Model\n\t1. Set input data as a parameter.\n\t2. Call the `ennExecute` function.\n\t3. Get the execution result as a return.\n3. Deinitializing the Framework\n\t1. Release the allocated memory of the buffers with the `ennReleaseBuffers` function.\n\t2. Close the model and release other resources with the `ennCloseModel` function.\n\t3. Deinitialize the framework with the `ennDeinitialize` function.",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Input Files\nContent:\nTo provide the data to the model, copy input data to the specified memory location.\nUse the EnnAllocateAllBuffer function to obtain the input memory location.\n(example):\n```cpp\nFILE *f = fopen(filename, \"rb\");\nchar *dst = reinterpret_cast<char *>(buffer_set[idx]->va);\nsize_t size;\nfseek(f, 0, SEEK_END);\nsize = ftell(f);\nfseek(f, 0, SEEK_SET);\nfread(dst, 1, size, f);\nfclose(f);\n```",
                "URL: news/b75128fb-34c4-454e-bef1-8af6fa636c6e/detail\nCategory: News\nTitle: \uc0bc\uc131\uc804\uc790, \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15c LSI \ud14c\ud06c \ub370\uc774 2023' \uac1c\ucd5c\nContent: \uc0bc\uc131\uc804\uc790\uac00 5\uc77c(\ud604\uc9c0\uc2dc\uac04) \ubbf8\uad6d \uc2e4\ub9ac\ucf58\ubc38\ub9ac \ubbf8\uc8fc\ucd1d\uad04(Device Solutions America office)\uc5d0\uc11c '\uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023'\uc744 \uac1c\ucd5c\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \ucd08\uc9c0\ub2a5\ud654(Hyper-Intelligence)\u00b7\ucd08\uc5f0\uacb0\uc131(Hyper-Connected)\u00b7\ucd08\ub370\uc774\ud130(Hyper-Data)\ub97c \uac00\ub2a5\ud558\uac8c \ud560 \uc8fc\uc694 \uc751\uc6a9\ucc98\ubcc4 \ucd5c\uc2e0 \ubc18\ub3c4\uccb4 \uc124\uacc4 \ud604\ud669\uacfc \ube44\uc804\uc744 \uacf5\uc720\ud588\ub2e4.\n\uc0bc\uc131\uc804\uc790\ub294 \uace0\uac1d\uc0ac\uc640 \ud30c\ud2b8\ub108\uc0ac \uad00\uacc4\uc790 300\uc5ec\uba85\uc774 \ucc38\uc11d\ud55c \uac00\uc6b4\ub370, \uc2dc\uc2a4\ud15c \ubc18\ub3c4\uccb4 \uc124\uacc4 \ubd84\uc57c \uae00\ub85c\ubc8c \uc804\ubb38\uac00, \uc11d\ud559\ub4e4\uacfc \ud568\uaed8 \uc0dd\uc131\ud615 AI\u00b7\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM, Large Language Model) \uae30\uc220\uc5d0 \ub300\ud55c \uc2ec\ub3c4 \uae4a\uc740 \ub17c\uc758\ub97c \uc9c4\ud589\ud588\ub2e4.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\\Preparing Framework and Model\nContent:\nBefore executing ML models on the ENN framework, initialize the framework, load the model, and allocate the required buffers.\n1. EnnInitialize:\n\tThis function initializes the ENN Framework.\n\t(example):\n\t```cpp\n\tenn::api::EnnInitialize();\n\t```\n2. EnnOpenModel:\n\tThis function opens the specified model and returns a model ID.\n\t(example):\n\t```cpp\n\tEnnModelId model_id;\n\tenn::api::EnnOpenModel(model_name.c_str(), &model_id);\n\t```\n3. EnnAllocateAllBuffers:\n\tThis function allocates the required buffers for the model.\n\tIt also provides the number of input/output buffers (`buffer_info`), their locations, and sizes (`buffer_set`).\n\t(example):\n\t```cpp\n\tEnnBufferPtr *buffer_set;\n\tNumberOfBuffersInfo buffer_info;\n\tenn::api::EnnAllocateAllBuffers(model_id, &buffer_set, &buffer_info);\n\t```\nFollowing is the data structure of `EnnBufferPtr` and `NumberOfBuffersInfo`.\n```cpp\ntypedef struct _ennBuffer {\n\tvoid *va;\n\tuint32_t size;\n\tuint32_t offset;\n} EnnBuffer;\ntypedef EnnBuffer* EnnBufferPtr;\ntypedef struct _NumberOfBuffersInfo {\n\tuint32_t n_in_buf;\n\tuint32_t n_out_buf;\n} NumberOfBuffersInfo;\n```",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \ub2e4. \uc774 \uac1c\ub150\uc740 \uc13c\uc11c\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\uace0, \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uace0, \ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790\uc5d0\uac8c \ub354 \ub098\uc740 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud558\ub294 \uc138 \ub2e8\uacc4\ub85c \uc138\ubd84\ud654\ud560 \uc218 \uc788\ub2e4.\n\ubaa8\ub4e0 \uc6d0\uc2dc \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub824\uba74 \ub9ce\uc740 \uc5f0\uc0b0\uc774 \ud544\uc694\ud558\ubbc0\ub85c \uc804\ub825 \uc18c\ube44\uac00 \uc99d\uac00\ud55c\ub2e4. \ubc14\ub85c \uc774 \ubd80\ubd84\uc5d0\uc11c \uc0bc\uc131\uc758 \uae30\uc220\uc801 \ud601\uc2e0\uc774 \ube5b\uc744 \ubc1c\ud55c\ub2e4. \uc0bc\uc131\uc740 \uc804\uc6a9 \ub3c4\uba54\uc778\ubcc4 \uc2dc\uc2a4\ud15c\uc744 \ud3ec\ud568\ud558\ub294 \ubd84\uc0b0 \uc544\ud0a4\ud14d\ucc98\ub97c \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub294 \ub354 \uac04\ub2e8\ud558\uba74\uc11c \ud2b9\uc218\ud55c \uc5f0\uc0b0 \ubd80\ud558\ub97c \ucc98\ub9ac\ud558\uae30 \ub54c\ubb38\uc5d0 \uae30\ubcf8\uc801\uc73c\ub85c \ud6a8\uc728\uc131\uc774 \ub192\ub2e4. \uc774 \uc2dc\uc2a4\ud15c\uc740 \uc804\ub825 \uc18c\ube44\uac00 \uc801\uace0 \ucc98\ub9ac \uc9c0\uc5f0 \uc2dc\uac04\uc774 \uc9e7\ub2e4.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\n#### Executing Model\nThe execution phase of model involves copying the input layer, executing the model, and copying the output layer.\nFor more information on copying data to the input layer and comparing data with the output layer, refer to Section Processing Input and Outputs section.\n1. EnnExecuteModel:\n\tThis function executes the model using the specified model ID.\n\t(example):\n\t```cpp\n\tenn::api::EnnExecuteModel(model_id)\n\t```\n#### Deinitializing Framework\nAfter executing the model, deinitialize the framework to release resources.\n1. EnnReleaseBuffers:\n\tThis function releases the allocated buffers.\n\t(example):\n\t```cpp\n\tenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n\t```\n2. EnnCloseModel:\n\tThis function closes the specified model.\n\t(example):\n\t```cpp\n\tenn::api::EnnCloseModel(model_id)\n\t```\n3. EnnDeinitialize:\n\tThis function deinitializes the ENN framework.\n\t(example):\n\t```cpp\n\tenn::api::EnnDeinitialize()\n\t```",
                "Section: Function Implementation\\Implementing Function\\Preparing NNC Model\nContent:\nUse `copyNNCFromAssetsToInternalStorage` function to copy the NNC model file from the asset directory of app to its internal storage.\nIt is necessary to copy the NNC model file because the model file needs to be accessed from the internal storage when used by the ENN Framework.\n(example):\n```kotlin\nprivate fun copyNNCFromAssetsToInternalStorage(filename: String) {\n\ttry {\n\t\tval inputStream = context.assets.open(filename)\n\t\tval outputFile = File(context.filesDir, filename)\n\t\tval outputStream = FileOutputStream(outputFile)\n\t\tval buffer = ByteArray(2048)\n\t\tvar bytesRead: Int\n\t\twhile (inputStream.read(buffer).also { bytesRead = it } != -1) {\n\t\t\toutputStream.write(buffer, 0, bytesRead)\n\t\t}\n\t\tinputStream.close()\n\t\toutputStream.close()\n\t} catch (e: IOException) {\n\t\te.printStackTrace()\n\t}\n}\n```",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\nContent:\nThis section describes the steps for executing NN models on the ENN framework.\nFor more information on the ENN framework, refer to the documentation.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uc774\ub7ec\ud55c \uc544\ud0a4\ud14d\ucc98 \ubcc0\uacbd\uc758 \uacb0\uacfc\ub85c MobileBERT1 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc740 \uc774\uc804 \uc138\ub300\uc5d0 \ube44\ud574 3\ubc30 \ud5a5\uc0c1\ub418\uc5c8\ub2e4\n**\ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305\uc744 \uc704\ud55c \uc0bc\uc131\uc758 \uc624\ud1a0 SoC**\n\uc790\ub3d9\ucc28\ub294 \ud604\uc7ac \uc804\ud654\uac00 \uc2a4\ub9c8\ud2b8\ud3f0\uc73c\ub85c \uc9c4\ud654\ud55c \uac83\uacfc \uc720\uc0ac\ud558\uac8c \uae09\uaca9\ud558\uac8c \ubcc0\ud654\ud558\uace0 \uc788\ub2e4. \uc2e4\uc81c\ub85c \ud604\ub300\uc758 \uc790\ub3d9\ucc28\ub294 \ub2e8\uc21c\ud55c \uc774\ub3d9 \uc218\ub2e8\uc744 \ub118\uc5b4 \uc0dd\uc131\ud615 AI\uc640 \uac19\uc740 \uad11\ubc94\uc704\ud55c \ucef4\ud4e8\ud305 \uae30\ub2a5\uc758 \uc9d1\ud569\uccb4\uac00 \ub418\uc5c8\ub2e4.\n\uc774\ub7ec\ud55c \ud604\uc2e4\uc744 \ubc30\uacbd\uc73c\ub85c \uc0bc\uc131\uc804\uc790 AP2 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uac1c\ubc1c\ud300 \ubc29\uc9c0\ud6c8 \uc0c1\ubb34\ub294 \uc0bc\uc131 \uc624\ud1a0 SoC \uc194\ub8e8\uc158\uacfc \ud655\uc7a5 \uac00\ub2a5\ud55c \uc911\uc559 \ucef4\ud4e8\ud305 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ub300\ud55c \uc138\uc158\uc744 \uc9c4\ud589\ud588\ub2e4.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-2\nContent:\n| DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| AVGPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\n| MAXPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\nFor `AVGPOOL` and `MAXPOOL`, when the kernel size is larger than the supported size, it is split into multiple pooling layers as the origin."
            ]
        ],
        "answer": "Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
        "ground_truth": "Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup."
    },
    {
        "question": "I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?",
        "contexts": [
            [
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-2\nContent:\n| 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O |\n| 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O |\n| 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O |\n| 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O |\n| 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O |\n| 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O |\n| 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O |\n| 20 | DIV | DIV | | | O | O |\n| 21 | DYNAMIC_CONVOLUTION | - | O | | | |\n| 22 | DYNAMIC_FC | - | O | | | |\n| 23 | ELEMENTWISE_DIV | DIV | | O | O | O |\n| 24 | ELEMENTWISE_MUL | MUL | O | O | O | O |\n| 25 | ELEMENTWISE_SUB | SUB | O | O | O | O |\n| 26 | ELEMENTWISE_SUM | ADD | O | O | O | O |",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\nContent:\nThe data such as the model file and input data are required for executing the program.\nThe model can be directly loaded by the framework using its path.\nHowever, the input data must be copied to memory.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: Function Implementation\\Implementing Function\\Processing Data\\Processing Output Data-2\nContent:\n\t\t}\n\t\tDataType.FLOAT32 -> {\n\t\t\tval byteBuffer = ByteBuffer.wrap(modelOutput).order(ByteOrder.nativeOrder())\n\t\t\tval floatBuffer = byteBuffer.asFloatBuffer()\n\t\t\tval data = FloatArray(floatBuffer.remaining())\n\t\t\tfloatBuffer.get(data)\n\t\t\tdata.mapIndexed { index, value ->\n\t\t\t\tlabelList[index] to ((value\n\t\t\t\t\t\t- OUTPUT_CONVERSION_OFFSET)\n\t\t\t\t\t\t/ OUTPUT_CONVERSION_SCALE)\n\t\t\t}.filter { it.second >= threshold }.sortedByDescending { it.second }.toMap()\n\t\t}\n\t\telse -> {\n\t\t\tthrow IllegalArgumentException(\"Unsupported output data type: ${OUTPUT_DATA_TYPE}\")\n\t\t}\n\t}\n\treturn output\n}\n```",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-1\nContent:\n| Operator_Name | Kernel Size | Padding | Strides | Dilation | Boost Mode |\n| ------------------------------ | ----------- | -------- | -------- | --------- | ---------- |\n| CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | {1 ~ 27} |\n| DECONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DECONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-2\nContent:\n| DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| AVGPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\n| MAXPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\nFor `AVGPOOL` and `MAXPOOL`, when the kernel size is larger than the supported size, it is split into multiple pooling layers as the origin.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-2\nContent:\n| `ennReleaseBuffers` | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| `ennExecute` | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennMemcpyHostToDevice` | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| `ennMemcpyDeviceToHost` | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\List of Implemented Native Functions-1\nContent:\nFollowing table describes the implemented native functions for the sample application.\n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| `ennInitialize` | Initialize the ENN framework | - | - |\n| `ennDeinitialize` | Deinitialize the ENN framework | - | - |\n| `ennOpenModel` | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| `ennCloseModel` | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| `ennAllocateAllBuffers` | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-6\nContent:\n| 75 | SELECT | SELECT | | | | O |\n| 76 | SHIFT_CLAMP | - | O | O | | |\n| 77 | SIN | SIN | | | O | O |\n| 78 | SLICE | SLICE | O | O | O | O |\n| 79 | SOFTMAX | SOFTMAX | | O | O | O |\n| 80 | SPACE_TO_BATCH_ND | SPACE_TO_BATCH_ND | | | | O |\n| 81 | SPACE_TO_DEPTH | SPACE_TO_DEPTH | O | O | O | O |\n| 82 | SPLIT | SPLIT | O | | O | O |\n| 83 | SPLIT_V | SPLIT_V | O | | | O |\n| 84 | SQRT | SQRT | | O | O | O |\n| 85 | SQUARED_DIFFERENCE | SQUARED_DIFFERENCE | | | O | O |\n| 86 | SQUEEZE | SQUEEZE | | | O | O |\n| 87 | STRIDED_SLICE | STRIDED_SLICE | O | O | O | O |\n| 88 | SUB | SUB | | | O | O |\n| 89 | TANH | TANH | O | O | O | O |\n| 90 | TILE | TILE | O | | O | O |\n| 91 | TRANSPOSE | TRANSPOSE | O | O | O | O |",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-3\nContent:\n| 27 | EXP | EXP | | O | O | O |\n| 28 | UNSQUEEZE | EXPAND_DIMS | | | | O |\n| 29 | FLATTEN | - | O | O | O | O |\n| 30 | FLOOR | FLOOR | | O | O | O |\n| 31 | FLOOR_DIV | FLOOR_DIV | | O | | O |\n| 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O |\n| 33 | GATHER | GATHER | | | O | O |\n| 34 | GLOBAL_AVGPOOL | - | O | | O | O |\n| 35 | GLOBAL_MAXPOOL | - | O | O | O | O |\n| 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O |\n| 37 | HARD_SWISH | HARD_SWISH | O | O | O | O |\n| 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O |\n| 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O |\n| 40 | LOG | LOG | | | O | O |\n| 41 | LOGISTIC | LOGISTIC | O | O | O | O |\n| 42 | MAXIMUM | MAXIMUM | | O | O | O |",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-4\nContent:\n| 43 | MAXPOOL | MAX_POOL_2D | O | O | O | O |\n| 44 | MEAN | MEAN | | O | O | O |\n| 45 | MINIMUM | MINIMUM | | O | O | O |\n| 46 | MIRROR_PAD | MIRROR_PAD | O | O | O | O |\n| 47 | MUL | MUL | | | O | O |\n| 48 | NEG | NEG | O | O | O | O |\n| 49 | PACK | PACK | | | O | O |\n| 50 | PAD | PAD | O | O | O | O |\n| 51 | PADV2 | PADV2 | O | | O | O |\n| 52 | PERMUTE | TRANSPOSE | O | | O | O |\n| 53 | POW | POW | | | O | O |\n| 54 | PRELU | PRELU | O | O | O | O |\n| 55 | QUANTIZE | QUANTIZE | O | O | O | O |\n| 56 | REDUCE_MAX | REDUCE_MAX | O | O | O | O |\n| 57 | REDUCE_MEAN | REDUCE_MEAN | O | O | O | O |\n| 58 | REDUCE_MIN | REDUCE_MIN | O | O | O | O |\n| 59 | REDUCE_SUM | SUM | O | | O | O |",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions Documentation\nContent:\n#### function EnnExecuteModel\n```cpp\nEnnReturn EnnExecuteModel(\n\tconst EnnModelId model_id,\n\tconst int session_id =0\n)\n```\nRequest to service core to execute model with commited buffers.\n**Parameters**:\n- **model_id** [IN] model ID from load_model\n- **session_id** [IN] session ID\n**Return**: EnnReturn result, 0 is success\n**Note**: this function runs in block mode\n#### function EnnExecuteModelAsync\n```cpp\nEnnReturn EnnExecuteModelAsync(\n\tconst EnnModelId model_id,\n\tconst int session_id =0\n)\n```\nRequest to service core to execute model in background asynchronously.\n**Parameters**:\n- **model_id** [IN] model ID from load_model\n- **session_id** [IN] session ID\n**Return**: EnnReturn result, 0 is success ",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: Function Implementation\\Implementing Function\\Introduction\nContent:\nFollowing classes are required to create the sample application:\n- `executor`\n\t- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `fragments`\n\t- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n\t- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `enn_type`\n\t- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers\nFor additional information, refer to the Class Information.\n### Application Flow\nAfter the sample application is launched, you can select either the camera model or image mode.\nDepending on the choice, `CameraFragment` or `ImageFragment` is loaded.\nBoth fragments implement a listener interface from `ModelExecutor.kt`.\nThis interface enables the return of inference results to the fragments.\nWhen a fragment receives input, it invokes the `process` function in `ModelExecutor.kt`.\nAfter the inference process is completed, this `process` function triggers the `onResults` function of the listener.\nThe `onResults` function then uses these results to update the user interface of the fragment.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\Using Implemented Native Functions\nContent:\nTo execute the NN models with implemented native functions, perform the following steps.\n1. Preparing Execution of Model\n\t1. Initialize the framework using the `ennInitialize` function.\n\t2. Load the ML model into the framework using the `ennOpenModel` function.\n\t3. Allocate and commit the necessary buffers using the `ennAllocateAllBuffers` function.\n2. Executing a Model\n\t1. Set input data as a parameter.\n\t2. Call the `ennExecute` function.\n\t3. Get the execution result as a return.\n3. Deinitializing the Framework\n\t1. Release the allocated memory of the buffers with the `ennReleaseBuffers` function.\n\t2. Close the model and release other resources with the `ennCloseModel` function.\n\t3. Deinitialize the framework with the `ennDeinitialize` function.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: . \uce74\ud2f0\ube44\uc548 \ubd80\uc0ac\uc7a5\uc740 \uc628\ub514\ubc14\uc774\uc2a4 \ucef4\ud4e8\ud305 \uae30\ub2a5\uc744 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0 \ub3c4\uc785\ud558\uae30 \uc704\ud55c \uc0bc\uc131 \uc2dc\uc2a4\ud15cLSI\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 IP\uac1c\ubc1c \ubc29\ud5a5\uc131\uc744 \uc18c\uac1c\ud588\ub2e4.\n**\ucd5c\uc801\ud654\ub41c 4\ud074\ub7ec\uc2a4\ud130 \uad6c\uc870\uc758 CPU**\n\uc138 \uac1c\uc758 \uc8fc\uc694 IP \uc911 \uccab \ubc88\uc9f8\ub294 CPU\ub85c, \uae30\uc874\uc5d0\ub294 \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud558\uc774\uc5d4\ub4dc(high-end) \ud074\ub7ec\uc2a4\ud130, \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ub85c\uc6b0\uc5d4\ub4dc(low-end) \ud074\ub7ec\uc2a4\ud130, \uc911\uac04 \ud074\ub7ec\uc2a4\ud130 \ub4f1 \uc138 \uac1c\uc758 \ud074\ub7ec\uc2a4\ud130\ub85c \uad6c\uc131\ub418\uc5c8\ub2e4. \uc0bc\uc131\uc740 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 CPU \uc6b4\uc601\uc744 \uc704\ud574 \uc911\uac04 \ud074\ub7ec\uc2a4\ud130\ub97c \ubbf8\ub4dc \ud558\uc774(mid-high)\uc640 \ubbf8\ub4dc \ub85c\uc6b0(mid-low)\uc758 \ub450 \uac00\uc9c0 \uacc4\uce35\uc73c\ub85c \ub098\ub204\uace0, \ubbf8\ub4dc \ud558\uc774 \uacc4\uce35\uc740 \uac8c\uc784\uacfc \uac19\uc740 \ucef4\ud4e8\ud305 \uc9d1\uc57d\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc0ac\uc6a9\ud558\uae30\ub85c \uacb0\uc815\ud588",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-5\nContent:\n| 60 | RELU | RELU | O | O | O | O |\n| 61 | RELU_0_TO_1 | RELU_0_TO_1 | | O | O | O |\n| 62 | RELU6 | RELU6 | O | O | O | O |\n| 63 | RELUN | RELUN | | O | | |\n| 64 | RESHAPE | RESHAPE | O | O | O | O |\n| 65 | RESIZE_BILINEAR | RESIZE_BILINEAR | O | O | O | O |\n| 66 | RESIZE_BILINEAR_DS | RESIZE_BILINEAR | | O | | O |\n| 67 | RESIZE_NEAREST_NEIGHBOR | RESIZE_NEAREST_NEIGHBOR | O | O | O | O |\n| 68 | RESIZE_NEAREST_NEIGHBOR_DS | RESIZE_NEAREST_NEIGHBOR | | O | | O |\n| 69 | REVERSE | REVERSE_V2 | | | | O |\n| 70 | REVERSE_V2 | REVERSE_V2 | | | O | O |\n| 71 | ROUND | ROUND | | | O | O |\n| 72 | RSQRT | RSQRT | | O | O | O |\n| 73 | SCALE | - | O | O | O | O |\n| 74 | SCATTER_ND | SCATTER_ND | | | | O |",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-7\nContent:\n| 92 | UNPACK | UNPACK | | | O | O |\n| 93 | SHAPE | SHAPE | | | | O |\n| 94 | CLIP | - | O | O | | |\n| 95 | CAST | CAST | O | | O | O |\n| 96 | BATCH_TO_SPACE_ND | BATCH_TO_SPACE_ND | | | | O |\n| 97 | EQUAL | EQUAL | | | O | O |\n| 98 | NEQUAL | NOT_EQUAL | | | O | |\n| 99 | LESS | LESS | | | O | O |\n| 100 | GREATER | GREATER | | | O | O |\n| 101 | GREATER_EQUAL | GREATER_EQUAL | | | O | O |\n| 102 | LESS_EQUAL | LESS_EQUAL | | | O | O |\n| 103 | ADD_N | ADD_N | | | O | O |\n| 104 | TOPK_V2 | TOPK_V2 | | | | O |\n| 105 | LOG_SOFTMAX | LOG_SOFTMAX | | | O | O |\n| 106 | FLOOR_MOD | FLOOR_MOD | | | O | O |\n| 107 | SEGMENT_SUM | SEGMENT_SUM | | | O | O |\n| 108 | GELU | GELU | | | O | O |"
            ]
        ],
        "answer": "Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC) for the 1st AI Challenger. You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
        "ground_truth": "Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs."
    },
    {
        "question": "We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?",
        "contexts": [
            [
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n object ModelConstants {\n const val MODEL_NAME = \\\"float32_pose.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val HEATMAP_DATA_TYPE = DataType.FLOAT32\n\n const val HEATMAP_SIZE_W = 9\n const val HEATMAP_SIZE_H = 9\n const val HEATMAP_SIZE_C = 17\n\n val OFFSET_DATA_TYPE = DataType.FLOAT32\n\n const val OFFSET_SIZE_W = 9\n const val OFFSET_SIZE_H = 9\n const val OFFSET_SIZE_C = 34\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup."
            ]
        ],
        "answer": "OpenPOSE is a supported model category in the ENN SDK for the 1st AI Challenger. However, it's important to note that there are multiple versions and types of OpenPOSE. If you encounter any issues during application, feel free to reach out for assistance. On the other hand, KoBERT is unfortunately not supported in the 1st AI Challenger. If you have further questions or need additional guidance, please don't hesitate to ask!",
        "ground_truth": "The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported."
    },
    {
        "question": "I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?",
        "contexts": [
            [
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.objectdetection.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"yolov5s-new.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 640\n const val INPUT_SIZE_H = 640\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 25200\n const val OUTPUT_SIZE_H = 85\n\n const val LABEL_FILE = \\\"coco.txt\\\"\n }\n ```\n3. If the new model uses a different set of labels, add the corresponding label text file to the assets directory.\n4. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\nThis guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt outlines the steps for creating an application that utilizes the ENN SDK to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the ENN SDK in their Android applications.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU."
            ]
        ],
        "answer": "In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8) are not supported in the 1st AI Challenger. Additionally, the MMdetection library is not supported, and a manual implementation would be required for its usage. However, as part of the AI Challenger, OCR (Optical Character Recognition) projects are supported within the program's scope. If you have further inquiries or need additional support, please feel free to reach out for assistance.",
        "ground_truth": "In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities."
    },
    {
        "question": "I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?",
        "contexts": [
            [
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n object ModelConstants {\n const val MODEL_NAME = \\\"float32_pose.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val HEATMAP_DATA_TYPE = DataType.FLOAT32\n\n const val HEATMAP_SIZE_W = 9\n const val HEATMAP_SIZE_H = 9\n const val HEATMAP_SIZE_C = 17\n\n val OFFSET_DATA_TYPE = DataType.FLOAT32\n\n const val OFFSET_SIZE_W = 9\n const val OFFSET_SIZE_H = 9\n const val OFFSET_SIZE_C = 34\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\nExecuting NN models on the ENN framework comprises of three steps such as initializing the framework, inferring the models, and deinitializing the framework.\nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.\n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\n\tA1[Initialize]\n\tA1 --> A2[Open Model]\n\tA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\n\tB1[Copy Input Layer]\n\tB1 --> B2[Execute Model]\n\tB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\n\tC1[Release Buffers]\n\tC1 --> C2[Close Model]\n\tC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```\nTo infer multiple data, repeat `Inference`.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Required Parameters\nContent:\nTo execute NNC models using the ENN framework, the following parameters are required:\n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector<string>|List of input file paths|\n|`goldens` (optional)|vector<string>|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Section: ENN Framework\\ENN Framework\\Process of Inference with ENN Framework\\Using Implemented Native Functions\nContent:\nTo execute the NN models with implemented native functions, perform the following steps.\n1. Preparing Execution of Model\n\t1. Initialize the framework using the `ennInitialize` function.\n\t2. Load the ML model into the framework using the `ennOpenModel` function.\n\t3. Allocate and commit the necessary buffers using the `ennAllocateAllBuffers` function.\n2. Executing a Model\n\t1. Set input data as a parameter.\n\t2. Call the `ennExecute` function.\n\t3. Get the execution result as a return.\n3. Deinitializing the Framework\n\t1. Release the allocated memory of the buffers with the `ennReleaseBuffers` function.\n\t2. Close the model and release other resources with the `ennCloseModel` function.\n\t3. Deinitialize the framework with the `ennDeinitialize` function.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Golden Data and Comparing it with Output (Optional)\nContent:\nAfter executing the model, load the golden binary data into memory and compare it with the data in the output memory location of model.\nThis optional step validates whether the output of model matches the expected results.\nUse the EnnAllocateAllBuffer function to obtain the output memory location.\n(example):\n```cpp\ntemplate <typename T>\nint compare_data(T *data1, T *data2, int size, T threshold) {\n\tint diff = 0;\n\tfor (int idx = 0; idx < size; idx++) {\n\t\tif (std::abs(data1[idx] - data2[idx]) > threshold) {\n\t\t\tdiff++;\n\t\t}\n\t}\n\treturn diff;\n}\n```",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix."
            ]
        ],
        "answer": "Regarding your interest in using the NeRF (Neural Radiance Fields) model for image-based rendering, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
        "ground_truth": "Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have."
    },
    {
        "question": "How do I post an article in the Best Lab?",
        "contexts": [
            [
                "Question: How do I post an article in the Best Lab?\nAnswer: Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, up to six projects are featured, receiving special attention. There is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n object ModelConstants {\n const val MODEL_NAME = \\\"float32_pose.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val HEATMAP_DATA_TYPE = DataType.FLOAT32\n\n const val HEATMAP_SIZE_W = 9\n const val HEATMAP_SIZE_H = 9\n const val HEATMAP_SIZE_C = 17\n\n val OFFSET_DATA_TYPE = DataType.FLOAT32\n\n const val OFFSET_SIZE_W = 9\n const val OFFSET_SIZE_H = 9\n const val OFFSET_SIZE_C = 34\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#functionality\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: # Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n# Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n# Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from this repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.segmentation.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"deeplabv3.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_H = INPUT_SIZE_H\n const val OUTPUT_SIZE_W = INPUT_SIZE_W\n const val OUTPUT_SIZE_C = 21\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Copying Data to ERD Board\nContent:\nThe following commands copy the necessary files to the ERD board:\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/model.nnc /data/local/tmp/\nadb push example/input.bin /data/local/tmp/\nadb push example/golden.bin /data/local/tmp/\n```\nThese commands perform the following actions:\n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
                "Section: Writing Native Program\\Writing Native Program\\Introduction\\Program Flow\nContent:\nThis program receives the locations of the model file, input file, and output file as parameters.\nInitially, the program initializes the ENN SDK framework.\nSubsequently, it opens the model and allocates the required buffers.\nThen, it loads the input files into the specified buffer locations.\nAfter this, the model is executed.\nPost-execution, the program loads and compares the output with the output files that are provided.\nIf the output files are not available, it dumps the buffer data into a file.\nBefore the program terminates, it releases all the buffers, closes the model, and de-initializes the ENN SDK framework.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: Object Detection In Android:\n\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "URL: solution/AI/lab/6d6c649a-328c-4b45-853a-1023fed7846c/detail\nCategory: Lab - Support Models\nTitle: YOLOv5\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.objectdetection.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"yolov5s-new.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 640\n const val INPUT_SIZE_H = 640\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 25200\n const val OUTPUT_SIZE_H = 85\n\n const val LABEL_FILE = \\\"coco.txt\\\"\n }\n ```\n3. If the new model uses a different set of labels, add the corresponding label text file to the assets directory.\n4. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.imageclassification.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"inception_v4_quant.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.UINT8\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 299\n const val INPUT_SIZE_H = 299\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 1F\n const val INPUT_CONVERSION_OFFSET = 0F\n\n val OUTPUT_DATA_TYPE = DataType.UINT8\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n\n const val LABEL_FILE = \\\"labels1001.txt\\\"\n }\n ```\n3. If the new model uses a different set of labels, add the corresponding label text file to the assets directory.\n4. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Implementing Sample Application-1\nContent:\nImplementing sample application involves the following five steps:\n1. Starting Android project: Covers the basics of starting an Android project, from installing Android studio to connecting the ERD board to Android studio.\n2. Setting necessary UI: Provides essential information on Android UI view components.\n3. Function implementation: Explains the functions available in the sample application.\n4. ENN SDK service: Provides the step-by-step process for converting the TFLite model to NNC model.\n5. ENN framework: Explains the implementation of the ENN framework in the sample application.\nThe general workflow of writing and executing an Android application using the ENN SDK is discribed in the following flowchart.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Android Application\nContent:\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.\nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the Github repository.\n1. Download the samples by cloning the Github repository\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\t```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\n\tThe application is launched on the ERD board after the build.\nFor more information, refer to Getting Started With Android Samples.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n```kotlin\npackage com.samsung.depthestimation.data\n\nobject ModelConstants {\n const val MODEL_NAME = \\\"MiDaS_v2.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 256\n const val INPUT_SIZE_H = 256\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 256\n const val OUTPUT_SIZE_H = 256\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n}\n```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Section: Function Implementation\\Implementing Function\\Processing Data\\Processing Output Data-1\nContent:\nThe `postProcess` function processes the output from the neural network model.\nIt converts the quantized output values to percentile scores, matches the index with the label, filters items that exceed the score threshold, and sorts them in descending order.\nThe filtered items provide the final classification results.\n(example):\n```kotlin\nprivate fun postProcess(modelOutput: ByteArray): Map<String, Float> {\n\tval output = when (OUTPUT_DATA_TYPE) {\n\t\tDataType.UINT8 -> {\n\t\t\tmodelOutput.asUByteArray().mapIndexed { index, value ->\n\t\t\t\tlabelList[index] to dequantizedValues[((value.toInt()\n\t\t\t\t\t\t- OUTPUT_CONVERSION_OFFSET)\n\t\t\t\t\t\t/ OUTPUT_CONVERSION_SCALE).toInt()]\n\t\t\t}.filter { it.second >= threshold }.sortedByDescending { it.second }.toMap()",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: Using Adb to Execute\\Using Adb to Execute\nContent:\nThis section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board.",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Golden Data and Comparing it with Output (Optional)\nContent:\nAfter executing the model, load the golden binary data into memory and compare it with the data in the output memory location of model.\nThis optional step validates whether the output of model matches the expected results.\nUse the EnnAllocateAllBuffer function to obtain the output memory location.\n(example):\n```cpp\ntemplate <typename T>\nint compare_data(T *data1, T *data2, int size, T threshold) {\n\tint diff = 0;\n\tfor (int idx = 0; idx < size; idx++) {\n\t\tif (std::abs(data1[idx] - data2[idx]) > threshold) {\n\t\t\tdiff++;\n\t\t}\n\t}\n\treturn diff;\n}\n```",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## The ENN SDK\nThe ENN SDK tool facilitates the conversion of TensorFlow Lite neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the Samsung Exynos hardware to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of the Sample Native Program\nThe sample native program accepts a model file and raw input data files as parameters.\nIf golden output data files are provided, the program compares these files and displays the result. If no file is provided, the program generates a raw output file.\n### Model and Data Used\nIn this sample native program, a converted NNC file and raw input/output file available in the Github Repository are used.\n### Using Native Program\nTo execute the sample native program, refer to README file.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\nContent:\nThis section describes the steps for executing NN models on the ENN framework.\nFor more information on the ENN framework, refer to the documentation.",
                "Section: Changing the Model\\Changing the Model in an Android Sample Application\\Example: Switching to MobileNet V3\\Updating the Android Project-2\nContent:\n3. Adjust the `postProcess()` function for the output of the new model, which is a logits-vector instead of $[0,1]$ scores:\n\t```kotlin\n\tprivate fun postProcess(modelOutput: ByteArray): Map<String, Float> {\n\t\tval output = when (OUTPUT_DATA_TYPE) {\n\t\t\tDataType.FLOAT32 -> {\n\t\t\t\tval byteBuffer = ByteBuffer.wrap(modelOutput).order(ByteOrder.nativeOrder())\n\t\t\t\tval floatBuffer = byteBuffer.asFloatBuffer()\n\t\t\t\tval data = FloatArray(floatBuffer.remaining())\n\t\t\t\tfloatBuffer.get(data)\n\t\t\t\tdata.mapIndexed { index, value ->\n\t\t\t\t\tlabelList[index] to (1.0 / (1.0 + kotlin.math.exp(-value.toDouble()))).toFloat()\n\t\t\t\t}.filter { it.second >= threshold }.sortedByDescending { it.second }.toMap()\n\t\t\t}\n\t\t\telse -> {\n\t\t\t\tthrow IllegalArgumentException(\"Unsupported output data type: ${OUTPUT_DATA_TYPE}\")\n\t\t\t}\n\t\t}\n\t\treturn output\n\t}\n\t```",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n>adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n>```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\nExecuting NN models on the ENN framework comprises of three steps such as initializing the framework, inferring the models, and deinitializing the framework.\nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.\n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\n\tA1[Initialize]\n\tA1 --> A2[Open Model]\n\tA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\n\tB1[Copy Input Layer]\n\tB1 --> B2[Execute Model]\n\tB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\n\tC1[Release Buffers]\n\tC1 --> C2[Close Model]\n\tC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```\nTo infer multiple data, repeat `Inference`.",
                "Section: Writing Native Program\\Writing Native Program\\Appendix\\A. File Structure\nContent:\n- `example`: This directory contains an example model and its corresponding input files.\n- `jni`: This directory includes source code, build files, and libraries.\n\t- `include`: A subdirectory that contain the required header files.\n\t- `lib64`: A subdirectory that contain the shared library.\n\t- `*.mk`: Various build files.\n\t- `enn_nnc_model_tester.cpp`: Source code for the sample program.\n- `libs`: This directory contains the built program and required libraries.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Converting NN Models with ENN SDK Service\nContent:\nIn this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.\nTo get started, you must be a member of the Exynos Developer Society.\n- If you are not a member of the Exynos Developer Society, sign up from here.\n- If you already have an account, log in to the Exynos Developer Society.\nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from here.\n2. Navigate to the ENN SDK service page and provide a title for your project.\n3. Then, upload the downloaded TFLite model\n4. Next, select hardware preferences.\n\t- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n\t- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n5. After confirming your selections, click **Convert** to convert the model.\n6. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n7. Click **NNC Download** to download the converted NNC model.\n\tYou can now integrate the NNC model into the desired application.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#changing-the-model-in-an-android-sample-application\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: \t\tconst val INPUT_CONVERSION_SCALE = 256F\n\t\tconst val INPUT_CONVERSION_OFFSET = 0F\n\t\tval OUTPUT_DATA_TYPE = DataType.FLOAT32\n\t\tval OUTPUT_DATA_LAYER = LayerType.HWC\n\t\tconst val OUTPUT_SIZE_W = 600\n\t\tconst val OUTPUT_SIZE_H = 400\n\t\tconst val OUTPUT_SIZE_C = INPUT_SIZE_C\n\t\tconst val OUTPUT_CONVERSION_SCALE = 256F\n\t\tconst val OUTPUT_CONVERSION_OFFSET = 0F\n\t}\n\t```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores."
            ]
        ],
        "answer": "Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, there is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform. If you have a project you believe fits these criteria, I would recommend reaching out through the appropriate channels to inquire about the submission process.",
        "ground_truth": "Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, up to six projects are featured, receiving special attention. There is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform."
    },
    {
        "question": "How many projects can I create maximum in the SDK Service?",
        "contexts": [
            [
                "Question: How many projects can I create maximum in the SDK Service?\nAnswer: In the SDK Service, you are allowed to create a maximum of 5 tabs. If you attempt to create more than 5 tabs, a warning popup will appear to notify you of the limit. Please adjust your projects accordingly within this constraint.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: Starting Android Project\\Starting the Android Project\\Creating Android Project\nContent:\n1. To start a new project, click **New Project** or **+**.\n2. To select a template for project, select **Empty Views Activity** and click **Next**.\n3. To configure the project, input your preferred project name and choose a location on your computer to save it.\n4. Click **Finish** to create the project.\n### Version\nFollowing is the SDK version information for the sample application.\n| SDK Component | Version Number |\n| ---- | ---- |\n| Android minSdk | 31 |\n| Android targetSdk | 34 |\n| Android compileSdk | 34 |\n| CMake Minimum Version | 3.4.1 |\n| `com.android.application` Plugin | 8.0.2 |\n| `com.android.library` Plugin | 8.0.2 |\n| `org.jetbrains.kotlin.android` Plugin | 1.8.0 |\nFor more information, refer the following:\n- Module build gradle file\n- Project build gradle file",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Adding C++ to Module\nContent:\n1. Right click **Project** panel with **Android** option being selected.\n\t\n2. Select the **Add C++ to Module** option and click OK.\n\t",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
                "Question: In the SDK Service, I'm unable to download the converted log and NNC file.\nAnswer: You can download the converted log and NNC file for up to 7 days after their creation. After this period, they are deleted, and the download button is disabled. If you need to download them again, you'll have to re-upload the same model file and repeat the conversion under the same conditions.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: How do I post an article in the Best Lab?\nAnswer: Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, up to six projects are featured, receiving special attention. There is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Section: Starting Android Project\\Starting the Android Project\\Using ERD Board\\Linking Up the ERD Board\nContent:\n#### Configuring the ERD Board\nEnabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.\nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\n\tThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**.\n#### Connecting the ERD Board\n1. Connect the ERD board to your computer using a USB cable.\n2. In the pop-up that appears, select **Allow** to enable the USB debugging.\n3. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "URL: solution/AI/overview/4b4e09cf-76e7-4366-a5c8-bfbcd1b87881/detail\nCategory: Solution - AI Overview\nTitle: Exynos Developer Society for Ecosystem\nContent: Exynos open source ecosystem for on-device AI is designed to encourage everyone to quickly build their own exceptional experience using our solutions, tools and services.\nExynos Developer Society is also an education center that provides developers with an advanced training and professional support - all tailored to their needs.\nBy sharing your excitement with other enthusiasts on this platform, we are thrilled to expect you to achieve more than you could on your own.\nContributing to human society through technological innovation and social collaboration is a paramount value of the Exynos Developer Society.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: Starting Android Project\\Starting the Android Project\\Installing Android Studio\nContent:\n1. **To Download Android studio**, visit the official website and click **Download Android Studio**.\n2. **To install and set up Android Studio**, execute the downloaded file and follow the instructions of installation wizard.\n\t> For more information, such as requirement for each OS, refer to Install Android Studio from Android Developers.\n3. Add the path to the Platform Tools directory in environment variables\n\t- Windows\n\t\t1. Navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables.\n\t\t2. Select `Path` User variable, then select **Edit**.\n\t\t3. Select New and add `%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools` directory.\n\t\t4. Select **Ok** to close all the settings windows.\n\t\t\t\n\t- Linux (Ubuntu)\n\t\t1. Install Platform Tools by:\n\t\t\t```shell\n\t\t\tsudo apt-get install android-tools-adb android-tools-fastboot\n\t\t\t```",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: Compiling Using NDK\\Compiling Using NDK\nContent:\n## Build Using NDK\nAfter the Makefile is set up, the build process with NDK can be initiated.\n- Windows\n\t```shell\n\t%ANDROID_NDK_HOME%\ndk-build.cmd -C .\\jni\n\t```\n- Linux\n\t```shell\n\t$ANDROID_NDK_HOME/ndk-build -C ./jni\n\t```\nThis command instructs NDK to start the build process in the current directory.\n## Verifying the Build\nAfter the build process is complete, the compiled program can be verified by checking the `libs` directory:\n- Windows\n\t```shell\n\tdir libs\\arm64-v8a\\\n\t```\n- Linux\n\t```shell\n\tls libs/arm64-v8a/\n\t```\n\tThe compiled program (`enn_nnc_model_tester`) is visible in the output.\n## Troubleshooting\nIf you encounter any issues during the build process, ensure the following:\n- The `NDK_PROJECT_PATH` environment variable is correctly set.\n- The `ANDROID_NDK_HOME` environment variable points to the correct location of the NDK installation.\n- The paths in the `Android.mk` file are correct.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Implementing Sample Application-2\nContent:\n```mermaid\ngraph TB\nA[Start]\nsubgraph B[Starting Android Project]\nB1[Install Android Studio] --> B2[Create a New Android Project]\nB2 --> B3[Connect the ERD Board to Android Studio]\nend\nsubgraph C[Setting Necessary UI]\nC1[Set up Android UI View Components]\nend\nsubgraph D[Function Implementation]\nD1[Implement UI Functions]\nD1 --> D2[Implement Listener]\nD2 --> D3[Implement Data Processing Functions]\nend\nsubgraph E[ENNTools]\nE1[Use ENNTools to Convert the TFLite Model to the NNC Model]\nend\nsubgraph F[ENN Framework]\nF1[Implement the ENN Framework in the Sample Application]\nend\nG[End]\nA --> B\nB --> C\nC --> D\nD --> E\nE --> F\nF --> G\n```",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Section: Compiling Using NDK\\Compiling Using NDK\\Creating the Makefile\nContent:\nThe Makefile is a crucial component in the build process.\nIt instructs the compiler on how to build the program.\nThe Makefile for this project is divided into two parts such as `Android.mk` and `Application.mk`.\n### Android.mk\nThe `Android.mk` file defines the module and its properties.\n(example):\n```cmake\nLOCAL_PATH := $(call my-dir)\ninclude $(CLEAR_VARS)\nLOCAL_MODULE := enn_public_api_ndk_v1\nLOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so\ninclude $(PREBUILT_SHARED_LIBRARY)\ninclude $(CLEAR_VARS)\nLOCAL_MODULE := enn_nnc_model_tester\nLOCAL_C_INCLUDES += \\\n\t${LOCAL_PATH} \\\n\t${LOCAL_PATH}/include\nLOCAL_LDLIBS := -llog\nLOCAL_CFLAGS += -Wall -std=c++14 -O3\nLOCAL_CPPFLAGS += -fexceptions -frtti\nLOCAL_SRC_FILES := enn_nnc_model_tester.cpp\nLOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1\ninclude $(BUILD_EXECUTABLE)\n```\n### Application.mk:\nThe `Application.mk` file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used.\n(example):\n```cmake\nAPP_ABI := arm64-v8a\nAPP_STL := c++_static\n```"
            ]
        ],
        "answer": "In the SDK Service, you are allowed to create a maximum of 5 projects. If you attempt to create more than 5 tabs, a warning popup will appear to notify you of the limit. Please adjust your projects accordingly within this constraint.",
        "ground_truth": "In the SDK Service, you are allowed to create a maximum of 5 tabs. If you attempt to create more than 5 tabs, a warning popup will appear to notify you of the limit. Please adjust your projects accordingly within this constraint."
    },
    {
        "question": "In the SDK Service, I'm unable to download the converted log and NNC file.",
        "contexts": [
            [
                "Question: In the SDK Service, I'm unable to download the converted log and NNC file.\nAnswer: You can download the converted log and NNC file for up to 7 days after their creation. After this period, they are deleted, and the download button is disabled. If you need to download them again, you'll have to re-upload the same model file and repeat the conversion under the same conditions.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Converting NN Models with ENN SDK Service\nContent:\nIn this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.\nTo get started, you must be a member of the Exynos Developer Society.\n- If you are not a member of the Exynos Developer Society, sign up from here.\n- If you already have an account, log in to the Exynos Developer Society.\nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from here.\n2. Navigate to the ENN SDK service page and provide a title for your project.\n3. Then, upload the downloaded TFLite model\n4. Next, select hardware preferences.\n\t- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n\t- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n5. After confirming your selections, click **Convert** to convert the model.\n6. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n7. Click **NNC Download** to download the converted NNC model.\n\tYou can now integrate the NNC model into the desired application.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## The ENN SDK\nThe ENN SDK tool facilitates the conversion of TensorFlow Lite neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the Samsung Exynos hardware to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of the Sample Native Program\nThe sample native program accepts a model file and raw input data files as parameters.\nIf golden output data files are provided, the program compares these files and displays the result. If no file is provided, the program generates a raw output file.\n### Model and Data Used\nIn this sample native program, a converted NNC file and raw input/output file available in the Github Repository are used.\n### Using Native Program\nTo execute the sample native program, refer to README file.",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n>adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n>```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Section: Function Implementation\\Implementing Function\\Preparing NNC Model\nContent:\nUse `copyNNCFromAssetsToInternalStorage` function to copy the NNC model file from the asset directory of app to its internal storage.\nIt is necessary to copy the NNC model file because the model file needs to be accessed from the internal storage when used by the ENN Framework.\n(example):\n```kotlin\nprivate fun copyNNCFromAssetsToInternalStorage(filename: String) {\n\ttry {\n\t\tval inputStream = context.assets.open(filename)\n\t\tval outputFile = File(context.filesDir, filename)\n\t\tval outputStream = FileOutputStream(outputFile)\n\t\tval buffer = ByteArray(2048)\n\t\tvar bytesRead: Int\n\t\twhile (inputStream.read(buffer).also { bytesRead = it } != -1) {\n\t\t\toutputStream.write(buffer, 0, bytesRead)\n\t\t}\n\t\tinputStream.close()\n\t\toutputStream.close()\n\t} catch (e: IOException) {\n\t\te.printStackTrace()\n\t}\n}\n```",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Copying Data to ERD Board\nContent:\nThe following commands copy the necessary files to the ERD board:\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/model.nnc /data/local/tmp/\nadb push example/input.bin /data/local/tmp/\nadb push example/golden.bin /data/local/tmp/\n```\nThese commands perform the following actions:\n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
                "Section: Quick Start Guide\\Quick Start Guide\\Basic Workflow\nContent:\nFollowing figure illustrates the three steps for converting and executing an NN model:\n```mermaid\nflowchart LR\n\tsubgraph \"ENN SDK Service\"\n\t\tdirection LR\n\t\tconvert(\"Convert The Model\")\n\tend\n\tsubgraph \"ENN Framework\"\n\t\tdirection LR\n\t\texecute(\"Execute The Model\")\n\tend\n\tmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```\n### Model Conversion\nTo convert TensorFlow Lite models, ENN SDK provides an online conversion tool through the Samsung Exynos Developer Society.\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.\nFor more information on the process of converting NN models, refer to Converting NN Models with ENN SDK Service.\n### Model Execution\nTo execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe ENN SDK provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.\nFor more information on the process of executing NN models, refer to Executing Models Using Native Program and Executing Models Using Android Application.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Native Samples\nContent:\nThis section provides an overview of the native sample program.\nEach sample program entry provides the functionality of the sample program, its location, and instructions for running it.\nFor more information on implementing the applications, refer to Getting Started With Native Samples guide.\n### NNC Model Tester\nThis sample program illustrates the method to execute an NNC model using the ENN framework.\n#### Functionality\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to README file in the Github repository.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/nnc-model-tester` directory in the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. The sample is available in the Github repository.\n2. Build the program by following the instructions described in Compiling Using NDK guide.\n3. Execute the program by following the instructions described in the Using ADB to Execute Native Program guide.",
                "Section: Compiling Using NDK\\Compiling Using NDK\nContent:\n## Build Using NDK\nAfter the Makefile is set up, the build process with NDK can be initiated.\n- Windows\n\t```shell\n\t%ANDROID_NDK_HOME%\ndk-build.cmd -C .\\jni\n\t```\n- Linux\n\t```shell\n\t$ANDROID_NDK_HOME/ndk-build -C ./jni\n\t```\nThis command instructs NDK to start the build process in the current directory.\n## Verifying the Build\nAfter the build process is complete, the compiled program can be verified by checking the `libs` directory:\n- Windows\n\t```shell\n\tdir libs\\arm64-v8a\\\n\t```\n- Linux\n\t```shell\n\tls libs/arm64-v8a/\n\t```\n\tThe compiled program (`enn_nnc_model_tester`) is visible in the output.\n## Troubleshooting\nIf you encounter any issues during the build process, ensure the following:\n- The `NDK_PROJECT_PATH` environment variable is correctly set.\n- The `ANDROID_NDK_HOME` environment variable points to the correct location of the NDK installation.\n- The paths in the `Android.mk` file are correct.",
                "Section: Starting Android Project\\Starting the Android Project\\Creating Android Project\nContent:\n1. To start a new project, click **New Project** or **+**.\n2. To select a template for project, select **Empty Views Activity** and click **Next**.\n3. To configure the project, input your preferred project name and choose a location on your computer to save it.\n4. Click **Finish** to create the project.\n### Version\nFollowing is the SDK version information for the sample application.\n| SDK Component | Version Number |\n| ---- | ---- |\n| Android minSdk | 31 |\n| Android targetSdk | 34 |\n| Android compileSdk | 34 |\n| CMake Minimum Version | 3.4.1 |\n| `com.android.application` Plugin | 8.0.2 |\n| `com.android.library` Plugin | 8.0.2 |\n| `org.jetbrains.kotlin.android` Plugin | 1.8.0 |\nFor more information, refer the following:\n- Module build gradle file\n- Project build gradle file",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Section: Changing the Model\\Changing the Model in an Android Sample Application\\Example: Switching to MobileNet V3\\Updating the Android Project-1\nContent:\n1. Copy the `mobilenet-v3.nnc` model file to `assets` directory of your Andriod project.\n2. Update `ModelConstants.kt` with the new input/output data types and sizes:\n\t```kotlin\n\tobject ModelConstants {\n\t\tconst val MODEL_NAME = \"mobilenet-v3.nnc\"\n\t\tval INPUT_DATA_TYPE = DataType.FLOAT32\n\t\tval INPUT_DATA_LAYER = LayerType.HWC\n\t\tconst val INPUT_SIZE_W = 224\n\t\tconst val INPUT_SIZE_H = 224\n\t\tconst val INPUT_SIZE_C = 3\n\t\tconst val INPUT_CONVERSION_SCALE = 256F\n\t\tconst val INPUT_CONVERSION_OFFSET = 0F\n\t\tval OUTPUT_DATA_TYPE = DataType.FLOAT32\n\t\tconst val OUTPUT_CONVERSION_SCALE = 1F\n\t\tconst val OUTPUT_CONVERSION_OFFSET = 0F\n\t\tconst val LABEL_FILE = \"labels1001.txt\"\n\t}\n\t```",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\nContent:\nThis section provides an overview of Android (Kotlin) sample applications.\nEach sample application entry provides the details of the functionality of the sample application, its location, and instructions for running it.\nFor more information on implementing the sample applications, refer to Getting Started With Android Samples guide.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n```kotlin\npackage com.samsung.depthestimation.data\n\nobject ModelConstants {\n const val MODEL_NAME = \\\"MiDaS_v2.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 256\n const val INPUT_SIZE_H = 256\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 256\n const val OUTPUT_SIZE_H = 256\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n}\n```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Implementing Sample Application-1\nContent:\nImplementing sample application involves the following five steps:\n1. Starting Android project: Covers the basics of starting an Android project, from installing Android studio to connecting the ERD board to Android studio.\n2. Setting necessary UI: Provides essential information on Android UI view components.\n3. Function implementation: Explains the functions available in the sample application.\n4. ENN SDK service: Provides the step-by-step process for converting the TFLite model to NNC model.\n5. ENN framework: Explains the implementation of the ENN framework in the sample application.\nThe general workflow of writing and executing an Android application using the ENN SDK is discribed in the following flowchart.",
                "Section: Changing the Model\\Changing the Model in an Android Sample Application\\Example: Switching to MobileNet V3\\Preparing the Model\nContent:\nTo integrate a new model, do the following:\n1. Download MobileNet V3 `.tflite` model from here.\n2. Upload the `.tflite` model downloaded in Step 1 to ENN SDK service.\n3. Then, follow the process described in the ENN SDK service guide, and select **Default** as the hardware type to convert the `.tflite` model to `.nnc` format.\n4. After the conversion is completed, download the model from ENN SDK service and then rename it as `mobilenet-v3.nnc`.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Classification In Android\nContent:\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Accelerate** hardware type option.\n#### Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input.\nThe classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: Compiling Using NDK\\Compiling Using NDK\\Creating the Makefile\nContent:\nThe Makefile is a crucial component in the build process.\nIt instructs the compiler on how to build the program.\nThe Makefile for this project is divided into two parts such as `Android.mk` and `Application.mk`.\n### Android.mk\nThe `Android.mk` file defines the module and its properties.\n(example):\n```cmake\nLOCAL_PATH := $(call my-dir)\ninclude $(CLEAR_VARS)\nLOCAL_MODULE := enn_public_api_ndk_v1\nLOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so\ninclude $(PREBUILT_SHARED_LIBRARY)\ninclude $(CLEAR_VARS)\nLOCAL_MODULE := enn_nnc_model_tester\nLOCAL_C_INCLUDES += \\\n\t${LOCAL_PATH} \\\n\t${LOCAL_PATH}/include\nLOCAL_LDLIBS := -llog\nLOCAL_CFLAGS += -Wall -std=c++14 -O3\nLOCAL_CPPFLAGS += -fexceptions -frtti\nLOCAL_SRC_FILES := enn_nnc_model_tester.cpp\nLOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1\ninclude $(BUILD_EXECUTABLE)\n```\n### Application.mk:\nThe `Application.mk` file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used.\n(example):\n```cmake\nAPP_ABI := arm64-v8a\nAPP_STL := c++_static\n```",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Performance Comparison\nContent:\nThis sample application provides a side-by-side analysis of the execution time differences between the ENN SDK and TFLite.\n#### Functionality\nThis application builds upon the Image Classification in Android sample application.<br>\nIn addition to executing the NNC model using the ENN framework, it runs the corresponding TFLite model.\nThe results and inference time of the ENN framework and TFLite are displayed at the bottom of the application interface.\n#### Location\nThis sample is available in the `enn-sdk-samples-9925/perf-compare` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.",
                "Section: Writing Native Program\\Writing Native Program\\Appendix\\A. File Structure\nContent:\n- `example`: This directory contains an example model and its corresponding input files.\n- `jni`: This directory includes source code, build files, and libraries.\n\t- `include`: A subdirectory that contain the required header files.\n\t- `lib64`: A subdirectory that contain the shared library.\n\t- `*.mk`: Various build files.\n\t- `enn_nnc_model_tester.cpp`: Source code for the sample program.\n- `libs`: This directory contains the built program and required libraries.",
                "Section: Quick Start Guide\\Quick Start Guide\nContent:\n## Abstract\nThis guide provides basic instructions for using Exynos Neural Network Software Development Kit (ENN SDK).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices.\n## Introduction\nENN SDK allows users to convert the trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nENN SDK contains ENN SDK service to convert trained NN models and ENN framework for executing converted models on Exynos platforms.\nThis guide covers the basics of using ENN SDK service and executing NN models with ENN framework.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK usage guide\nContent:\n### [Quick start guide](quick-start-guide)\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### [ENN SDK samples](enn-sdk-samples)\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### [Getting started with android samples](getting-started-with-android-samples)\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### [Getting started with native samples](getting-started-with-native-samples)\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\nContent:\n## ENN SDK\nThe ENN SDK provides a tool for converting TensorFlow Lite neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of Sample Application\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the ENN SDK to efficiently execute the NN model on the Exynos platform.\n### Used NN Model\nThis example uses the quantized `Inception v4` TFLite model from Kaggle Models for image classification.\n### Using the Sample Application\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.\nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Android (Kotlin) Samples\\Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Section: Compiling Using NDK\\Compiling Using NDK\nContent:\nThis section describes the method to use NDK to compile the native program.\nThe process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK."
            ]
        ],
        "answer": "You can download the converted log and NNC file for up to 7 days after their creation. After this period, they are deleted, and the download button is disabled. If you need to download them again, you'll have to re-upload the same model file and repeat the conversion under the same conditions.",
        "ground_truth": "You can download the converted log and NNC file for up to 7 days after their creation. After this period, they are deleted, and the download button is disabled. If you need to download them again, you'll have to re-upload the same model file and repeat the conversion under the same conditions."
    },
    {
        "question": "I entered the wrong password multiple times, and now my account is locked. What should I do?",
        "contexts": [
            [
                "Question: I entered the wrong password multiple times, and now my account is locked. What should I do?\nAnswer: In the Eco System, an account gets locked after 5 consecutive incorrect password attempts. If your account is locked, you can unlock it by going through the verification process sent to your linked email address. Please check your email for the necessary steps to regain access to your account.",
                "Question: In the SDK Service, I'm unable to download the converted log and NNC file.\nAnswer: You can download the converted log and NNC file for up to 7 days after their creation. After this period, they are deleted, and the download button is disabled. If you need to download them again, you'll have to re-upload the same model file and repeat the conversion under the same conditions.",
                "Section: Starting Android Project\\Starting the Android Project\\Installing Android Studio\nContent:\n1. **To Download Android studio**, visit the official website and click **Download Android Studio**.\n2. **To install and set up Android Studio**, execute the downloaded file and follow the instructions of installation wizard.\n\t> For more information, such as requirement for each OS, refer to Install Android Studio from Android Developers.\n3. Add the path to the Platform Tools directory in environment variables\n\t- Windows\n\t\t1. Navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables.\n\t\t2. Select `Path` User variable, then select **Edit**.\n\t\t3. Select New and add `%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools` directory.\n\t\t4. Select **Ok** to close all the settings windows.\n\t\t\t\n\t- Linux (Ubuntu)\n\t\t1. Install Platform Tools by:\n\t\t\t```shell\n\t\t\tsudo apt-get install android-tools-adb android-tools-fastboot\n\t\t\t```",
                "Question: How many projects can I create maximum in the SDK Service?\nAnswer: In the SDK Service, you are allowed to create a maximum of 5 tabs. If you attempt to create more than 5 tabs, a warning popup will appear to notify you of the limit. Please adjust your projects accordingly within this constraint.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.segmentation.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"deeplabv3.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_H = INPUT_SIZE_H\n const val OUTPUT_SIZE_W = INPUT_SIZE_W\n const val OUTPUT_SIZE_C = 21\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: Setting Necessary UI\\Setting the UI Components\\Programming UI Components\\Button\nContent:\nThe Button triggers the image loading and inference processes.\nIn the layout XML file, define a Button with a unique ID and set its text.\nIn the Kotlin file, use this ID to reference the Button and set its `onClickListener` to the appropriate function.\n- Layout `app/res/layout/*.xml` (example):\n\t```xml\n\t<Button\n\t\tandroid:id=\"@+id/sample_button\"\n\t\tandroid:text=\"Sample\" />\n\t```\n- Kotlin `app/java/package/fragments/Fragment.kt` (example):\n\t```kotlin\n\tbinding.sampleButton.setOnClickListener {\n\t\t// ... do something ... //\n\t}\n\t```",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n```kotlin\npackage com.samsung.depthestimation.data\n\nobject ModelConstants {\n const val MODEL_NAME = \\\"MiDaS_v2.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 256\n const val INPUT_SIZE_H = 256\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 256\n const val OUTPUT_SIZE_H = 256\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n}\n```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: Function Implementation\\Implementing Function\\Preparing NNC Model\nContent:\nUse `copyNNCFromAssetsToInternalStorage` function to copy the NNC model file from the asset directory of app to its internal storage.\nIt is necessary to copy the NNC model file because the model file needs to be accessed from the internal storage when used by the ENN Framework.\n(example):\n```kotlin\nprivate fun copyNNCFromAssetsToInternalStorage(filename: String) {\n\ttry {\n\t\tval inputStream = context.assets.open(filename)\n\t\tval outputFile = File(context.filesDir, filename)\n\t\tval outputStream = FileOutputStream(outputFile)\n\t\tval buffer = ByteArray(2048)\n\t\tvar bytesRead: Int\n\t\twhile (inputStream.read(buffer).also { bytesRead = it } != -1) {\n\t\t\toutputStream.write(buffer, 0, bytesRead)\n\t\t}\n\t\tinputStream.close()\n\t\toutputStream.close()\n\t} catch (e: IOException) {\n\t\te.printStackTrace()\n\t}\n}\n```",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Golden Data and Comparing it with Output (Optional)\nContent:\nAfter executing the model, load the golden binary data into memory and compare it with the data in the output memory location of model.\nThis optional step validates whether the output of model matches the expected results.\nUse the EnnAllocateAllBuffer function to obtain the output memory location.\n(example):\n```cpp\ntemplate <typename T>\nint compare_data(T *data1, T *data2, int size, T threshold) {\n\tint diff = 0;\n\tfor (int idx = 0; idx < size; idx++) {\n\t\tif (std::abs(data1[idx] - data2[idx]) > threshold) {\n\t\t\tdiff++;\n\t\t}\n\t}\n\treturn diff;\n}\n```",
                "Section: Function Implementation\\Implementing Function\\Processing Data\\Converting Input Data to Bitmap\\Image Data\nContent:\nThe `ActivityResult` object `getContent` retrieves an image from the device media.\nThe selected image is displayed in an ImageView and converted to a bitmap.\n(example):\n```kotlin\nprivate val getContent =\n\tregisterForActivityResult(ActivityResultContracts.GetContent()) { uri: Uri? ->\n\t\turi?.let {\n\t\t\tbinding.imageView.setImageURI(it)\n\t\t\tbinding.buttonProcess.isEnabled = true\n\t\t\tbitmapBuffer = ImageDecoder.decodeBitmap(\n\t\t\t\tImageDecoder.createSource(\n\t\t\t\t\trequireContext().contentResolver,\n\t\t\t\t\tit\n\t\t\t\t)\n\t\t\t) { decoder, _, _ ->\n\t\t\t\tdecoder.setTargetColorSpace(ColorSpace.get(ColorSpace.Named.SRGB))\n\t\t\t\tdecoder.allocator = ImageDecoder.ALLOCATOR_SOFTWARE\n\t\t\t\tdecoder.setTargetSampleSize(1)\n\t\t\t}\n\t\t}\n\t}\n```\nClick **Load** to launch the `ActivityResult`.\nIt allows the user to select an image from their device.\n(example):\n```kotlin\nbinding.buttonLoad.setOnClickListener {\n\tgetContent.launch(\"image/*\")\n}\n```",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n package com.samsung.imageclassification.data\n\n object ModelConstants {\n const val MODEL_NAME = \\\"inception_v4_quant.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.UINT8\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 299\n const val INPUT_SIZE_H = 299\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 1F\n const val INPUT_CONVERSION_OFFSET = 0F\n\n val OUTPUT_DATA_TYPE = DataType.UINT8\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n\n const val LABEL_FILE = \\\"labels1001.txt\\\"\n }\n ```\n3. If the new model uses a different set of labels, add the corresponding label text file to the assets directory.\n4. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Section: Starting Android Project\\Starting the Android Project\\Using ERD Board\\Linking Up the ERD Board\nContent:\n#### Configuring the ERD Board\nEnabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.\nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\n\tThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**.\n#### Connecting the ERD Board\n1. Connect the ERD board to your computer using a USB cable.\n2. In the pop-up that appears, select **Allow** to enable the USB debugging.\n3. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device.",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail#changing-the-model-in-an-android-sample-application\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: To update the model in your Android project, do the following:\n1. Place the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2. In the `ModelConstants.kt` file, update the specifications to match the new model.\n\t```kotlin\n\tpackage com.samsung.imageenhance.data\n\tobject ModelConstants {\n\t\tconst val MODEL_NAME = \"zero-dce.nnc\"\n\t\tval INPUT_DATA_TYPE = DataType.FLOAT32\n\t\tval INPUT_DATA_LAYER = LayerType.HWC\n\t\tconst val INPUT_SIZE_W = 600\n\t\tconst val INPUT_SIZE_H = 400\n\t\tconst val INPUT_SIZE_C = 3",
                "Section: Compiling Using NDK\\Compiling Using NDK\nContent:\nThis section describes the method to use NDK to compile the native program.\nThe process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK.",
                "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
                "Section: Changing the Model\\Changing the Model in an Android Sample Application\\Example: Switching to MobileNet V3\\Preparing the Model\nContent:\nTo integrate a new model, do the following:\n1. Download MobileNet V3 `.tflite` model from here.\n2. Upload the `.tflite` model downloaded in Step 1 to ENN SDK service.\n3. Then, follow the process described in the ENN SDK service guide, and select **Default** as the hardware type to convert the `.tflite` model to `.nnc` format.\n4. After the conversion is completed, download the model from ENN SDK service and then rename it as `mobilenet-v3.nnc`.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Section: Changing the Model\\Changing the Model in an Android Sample Application\nContent:\nThis guide provides a step-by-step process to replace the model in an Android sample application.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\nThis guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (ENN SDK).\nIt describes the steps for developing a native program that leverages the ENN SDK to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Execute Models\\Functions Documentation\\function EnnExecuteModelWait\nContent:\n```cpp\nEnnReturn EnnExecuteModelWait(\n\tconst EnnModelId model_id,\n\tconst int session_id =0\n)\n```\nWait result of calling EnnExecuteModelAsync() If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.\n**Parameters**:\n- **model_id** [IN] model ID from load_model\n- **session_id** [IN] session ID\n**Return**: EnnReturn result, 0 is success ",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n>adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n>```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Adding C++ to Module\nContent:\n1. Right click **Project** panel with **Android** option being selected.\n\t\n2. Select the **Add C++ to Module** option and click OK.\n\t",
                "Section: Changing the Model\\Changing the Model in an Android Sample Application\\Updating the Model in Android Studio\nContent:\nTo update the model in your Android project, do the following:\n1. Place the new model file in the `assets` directory of your Android Studio project.\n2. In the `ModelConstants.kt` file, update the specifications to match the new model.\n3. If the new model uses a different set of labels, add the corresponding label text file to the `assets` directory.\n4. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code.\n> **Notes:**\n>- The `assets` directory is located at `${APP_ROOT}/app/src/main`.\n>- The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Section: Getting Started with Native Samples\\Getting Started with Native Samples\nContent:\n## The ENN SDK\nThe ENN SDK tool facilitates the conversion of TensorFlow Lite neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the Samsung Exynos hardware to ensure optimal performance.\nThe ENN SDK provides a framework that facilitates the execution of NNC models on the Exynos platform.\n## Overview of the Sample Native Program\nThe sample native program accepts a model file and raw input data files as parameters.\nIf golden output data files are provided, the program compares these files and displays the result. If no file is provided, the program generates a raw output file.\n### Model and Data Used\nIn this sample native program, a converted NNC file and raw input/output file available in the Github Repository are used.\n### Using Native Program\nTo execute the sample native program, refer to README file.",
                "Section: Writing Native Program\\Writing Native Program\\Using ENN Framework on C++\\Copying Libraries\nContent:\nDownload the ENN framework library (ENN Public API NDK) from resources.\nNext, copy the required libraries by performing the following:\n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add the following code in `en_nnc_model_tester.cpp` to load ENN framework.\n\t(Example):\n\t```cpp\n\t#include \"include/enn_api-public_ndk_v1.hpp\"\n\t```\nFor more information on the overall structure of the source, refer to the File Structure section.",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Input Files\nContent:\nTo provide the data to the model, copy input data to the specified memory location.\nUse the EnnAllocateAllBuffer function to obtain the input memory location.\n(example):\n```cpp\nFILE *f = fopen(filename, \"rb\");\nchar *dst = reinterpret_cast<char *>(buffer_set[idx]->va);\nsize_t size;\nfseek(f, 0, SEEK_END);\nsize = ftell(f);\nfseek(f, 0, SEEK_SET);\nfread(dst, 1, size, f);\nfclose(f);\n```",
                "URL: news/bdb28629-c6e5-4fef-8800-4dfaaf3ce2d1/detail\nCategory: News\nTitle: [\uc2dc\uc2a4\ud15cLSI \ud14c\ud06c \ub370\uc774 2023] \ucd08\uc9c0\ub2a5\ud654: AI \uc2dc\ub300\ub97c \uc704\ud55c \ud558\uc774\ud37c \uc778\ud154\ub9ac\uc804\uc2a4\nContent: \uc548\uc804\uc5d0 \ub300\ud55c \uc0bc\uc131\uc758 \ud655\uace0\ud55c \uc758\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ucc28\uc138\ub300 \uc624\ud1a0 SoC\uc758 \uc138\uc774\ud504\ud2f0 \uc544\uc77c\ub79c\ub4dc(safety island)\ub294 ASIL-D\ub97c \uc900\uc218\ud558\uba70, \ud638\uc2a4\ud2b8 CPU\uc640 \ubcc4\ub3c4\ub85c \uc791\ub3d9\ud558\uc5ec \ub2e4\ub978 SoC\uc758 \uc0c1\ud0dc\ub97c \ubaa8\ub2c8\ud130\ub9c1\ud560 \uc608\uc815\uc774\ub2e4. \uc774\uc678\uc758 \ubd80\ubd84\uc740 ASPICE, ISO 26262 \ubc0f FMEA\uc640 \uac19\uc740 \uc790\ub3d9\ucc28 \ud45c\uc900\uc744 \ud1b5\ud574 ASIL-B\ub97c \uc900\uc218\ud558\ub3c4\ub85d \uc81c\uc791\ub418\uc5c8\ub2e4.\n\uc0ac\uc774\ubc84 \ubcf4\uc548 \ub610\ud55c \uc810\uc810 \ub354 \uc911\uc694\ud55c \uc694\uc18c\ub85c \ub5a0\uc624\ub974\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0, \uc0bc\uc131\uc758 \uc624\ud1a0 SoC\ub294 \uc554\ud638\ud654 \uc5d4\uc9c4\uc774 \ub0b4\uc7a5\ub41c \uae30\ubcf8 \ubcf4\uc548 \ud504\ub85c\uc138\uc11c\uc640 StrongBOX \ud558\ub4dc\uc6e8\uc5b4 \ube14\ub85d\uc744 \uac16\ucd94\uace0 \uc788\ub2e4. \ub610\ud55c \uc0bc\uc131\uc740 \uc0ac\uc6a9\uc790 \uc815\ubcf4\ub97c \ubcf4\ud638\ud558\ub294 \uc790\uccb4 \uc6b4\uc601\uccb4\uc81c\uc778 \uc5d1\uc2dc\ub178\uc2a4TEE\ub97c \uac1c\ubc1c\ud588\uc73c\uba70 \uc774\ubbf8 EAL2 \uc778\uc99d\uc744 \ubc1b\uc558",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\nContent:\nThe data such as the model file and input data are required for executing the program.\nThe model can be directly loaded by the framework using its path.\nHowever, the input data must be copied to memory.",
                "Section: Writing Native Program\\Writing Native Program\\Introduction\\Program Flow\nContent:\nThis program receives the locations of the model file, input file, and output file as parameters.\nInitially, the program initializes the ENN SDK framework.\nSubsequently, it opens the model and allocates the required buffers.\nThen, it loads the input files into the specified buffer locations.\nAfter this, the model is executed.\nPost-execution, the program loads and compares the output with the output files that are provided.\nIf the output files are not available, it dumps the buffer data into a file.\nBefore the program terminates, it releases all the buffers, closes the model, and de-initializes the ENN SDK framework.",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Section: ENN Framwork API Functions\\ENN Framwork API Functions\\Security, preference, get meta information..\\Functions Documentation\nContent:\n#### function EnnSetPreferencePerfMode\n```cpp\nEnnReturn EnnSetPreferencePerfMode(\n\tconst uint32_t val\n)\n```\nSetting Performance Mode.\n**Parameters**:\n- **val** [IN] value to set Performance Mode\n**Return**: EnnReturn result, 0 is success\n#### function EnnSetPreferenceTimeOut\n```cpp\nEnnReturn EnnSetPreferenceTimeOut(\n\tconst uint32_t val\n)\n```\nSetting Preset ID for time out.\n**Parameters**:\n- **val** [IN] value to set time out\n**Return**: EnnReturn result, 0 is success\n**Note**: in second ",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "Section: Function Implementation\\Implementing Function\\Processing Data\\Converting Input Data to Bitmap\\Process Image\nContent:\nClick **Process** to crop and process the image.\n(example):\n```kotlin\nprivate fun processImage(image: Bitmap): Bitmap {\n\tval rotatedCroppedImage = createCroppedBitmap(image)\n\treturn Bitmap.createScaledBitmap(\n\t\trotatedCroppedImage, INPUT_SIZE_W, INPUT_SIZE_H, true\n\t)\n}\nprivate fun createCroppedBitmap(image: Bitmap): Bitmap {\n\tval matrix = Matrix().apply { postRotate(90F) }\n\tval cropDim = calculateCropDimensions(image)\n\treturn Bitmap.createBitmap(\n\t\timage, cropDim[0], cropDim[1], cropDim[2], cropDim[2], matrix, true\n\t)\n}\n```",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n4. Modify `CMakeLists.txt` that is created here.\n\t```cmake\n\t# Include the directory where the header files are located\n\tinclude_directories(include)\n\t# Declare the imported shared library\n\tadd_library(\n\t\tenn_service_so\n\t\tSHARED\n\t\tIMPORTED\n\t)\n\t# Set the location of the imported library\n\tset_target_properties(\n\t\tenn_service_so\n\t\tPROPERTIES IMPORTED_LOCATION\n\t\t${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n\t)\n\t# Link the imported library to the target library\n\ttarget_link_libraries(\n\t\t...\n\t\tenn_service_so\n\t\t...\n\t)\n\t```",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "Section: Using Adb to Execute\\Using Adb to Execute\nContent:\nThis section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board.",
                "Section: Function Implementation\\Implementing Function\\Processing Data\\Converting Input Data to Bitmap\\Converting Image to Input Data\nContent:\nThe `preProcess` function converts the processed image to input data.\nThis data is casted as byte array and copied to the buffer of model.\n(example):\n```kotlin\nprivate fun preProcess(image: Bitmap): ByteArray {\n\tval byteArray = when (INPUT_DATA_TYPE) {\n\t\tDataType.UINT8 -> {\n\t\t\tconvertBitmapToUByteArray(image, INPUT_DATA_LAYER).asByteArray()\n\t\t}\n\t\tDataType.FLOAT32 -> {\n\t\t\tval data = convertBitmapToFloatArray(image, INPUT_DATA_LAYER)\n\t\t\tval byteBuffer = ByteBuffer.allocate(data.size * Float.SIZE_BYTES)\n\t\t\tbyteBuffer.order(ByteOrder.nativeOrder())\n\t\t\tbyteBuffer.asFloatBuffer().put(data)\n\t\t\tbyteBuffer.array()\n\t\t}\n\t\telse -> {\n\t\t\tthrow IllegalArgumentException(\"Unsupported input data type: ${INPUT_DATA_TYPE}\")\n\t\t}\n\t}\n\treturn byteArray\n}\n```",
                "Section: Using Adb to Execute\\Using Adb to Execute\\Copying Data to ERD Board\nContent:\nThe following commands copy the necessary files to the ERD board:\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/model.nnc /data/local/tmp/\nadb push example/input.bin /data/local/tmp/\nadb push example/golden.bin /data/local/tmp/\n```\nThese commands perform the following actions:\n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Using JNI Library from Kotlin\nContent:\nFollowing is an example for using JNI function in Kotlin.\n1. Load the JNI library\n\t(example):\n\t```kotlin\n\tinit {\n\t\tSystem.loadLibrary(\"enn_jni\")\n\t\t...\n\t}\n\t```\n2. Declare the external function\n\t(example):\n\t```kotlin\n\tprivate external fun ennOpenModel(filename: String): Long\n\t...\n\t```\n3. Use the JNI function (example):\n\t```kotlin\n\t// to open a model\n\tprivate fun setupENN() {\n\t\t...\n\t\tmodelId = ennOpenModel(fileAbsoluteDirectory)\n\t\t...\n\t}\n\t```",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Section: ENN Framework\\ENN Framework\\Using ENN Framework on Android\\Creating a Java Native Interface Library\\Writing JNI Library\nContent:\nFollowing function is an example of an implemented JNI wrapper.<br>\nFor more information, refer to the Android Developer Documentation.\n(example):\n```cpp\nextern \"C\"\nJNIEXPORT jlong JNICALL\nJava_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel(\n\t\tJNIEnv *env,\n\t\tjobject thiz,\n\t\tjstring j_filename\n) {\n\tEnnModelId model_id;\n\tconst char *filename = env->GetStringUTFChars(j_filename, 0);\n\tif (enn::api::EnnOpenModel(filename, &model_id)) {\n\t\t__android_log_print(ANDROID_LOG_ERROR, LOG_TAG, \"EnnOpenModel of [%s] Failed\", filename);\n\t}\n\treturn static_cast<jlong>(model_id);\n}\n```",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\nContent:\nThis section describes the steps for executing NN models on the ENN framework.\nFor more information on the ENN framework, refer to the documentation.",
                "Section: Changing the Model\\Changing the Model in an Android Sample Application\\Example: Switching to MobileNet V3\\Updating the Android Project-2\nContent:\n3. Adjust the `postProcess()` function for the output of the new model, which is a logits-vector instead of $[0,1]$ scores:\n\t```kotlin\n\tprivate fun postProcess(modelOutput: ByteArray): Map<String, Float> {\n\t\tval output = when (OUTPUT_DATA_TYPE) {\n\t\t\tDataType.FLOAT32 -> {\n\t\t\t\tval byteBuffer = ByteBuffer.wrap(modelOutput).order(ByteOrder.nativeOrder())\n\t\t\t\tval floatBuffer = byteBuffer.asFloatBuffer()\n\t\t\t\tval data = FloatArray(floatBuffer.remaining())\n\t\t\t\tfloatBuffer.get(data)\n\t\t\t\tdata.mapIndexed { index, value ->\n\t\t\t\t\tlabelList[index] to (1.0 / (1.0 + kotlin.math.exp(-value.toDouble()))).toFloat()\n\t\t\t\t}.filter { it.second >= threshold }.sortedByDescending { it.second }.toMap()\n\t\t\t}\n\t\t\telse -> {\n\t\t\t\tthrow IllegalArgumentException(\"Unsupported output data type: ${OUTPUT_DATA_TYPE}\")\n\t\t\t}\n\t\t}\n\t\treturn output\n\t}\n\t```",
                "Section: Getting Started with Android Samples\\Getting Started With Android Samples\\Glossary\nContent:\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|"
            ]
        ],
        "answer": "In the Eco System, an account gets locked after 5 consecutive incorrect password attempts. If your account is locked, you can unlock it by going through the verification process sent to your linked email address. Please check your email for the necessary steps to regain access to your account.",
        "ground_truth": "In the Eco System, an account gets locked after 5 consecutive incorrect password attempts. If your account is locked, you can unlock it by going through the verification process sent to your linked email address. Please check your email for the necessary steps to regain access to your account."
    },
    {
        "question": "Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?",
        "contexts": [
            [
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Section: ENN SDK Samples\\ENN SDK Samples\\Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
                "Section: Documentation Home\\ENN SDK Documentation\\ENN SDK references\nContent:\n### [ENN SDK developer guide](developer-guide)\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### [API references](api-reference)\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### [Support matrix](support-matrix)\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\3. Tools\\ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: Depth Estimation In Andriod:\n\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
                "URL: solution/AI/lab/136ee574-e585-46be-a368-1cd6767b1b55/detail\nCategory: Lab - Support Models\nTitle: DeeplabV3\nContent: Segmentation In Android:\n\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-2\nContent:\n3. When nnc_model_tester is built from Windows, execute permission must be provided.\n\t```shell\n\tadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n\t```\n4. Execute native binary on ERD board using adb shell.\n\t```shell\n\tadb shell\n\tcd /data/local/tmp/\n\texport LD_LIBRARY_PATH=/data/local/tmp\n\t./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n\t```\n5. When the command is successful, the following message is displayed:\n\t```shell\n\tLoaded Model:\n\t\tmodel.nnc(00000B7E01000000)\n\tModel Execution Time (1): 5413 microseconds\n\tAvg. Model Execution Time: 5413 microseconds\n\tOutput Layer(0): Golden Match\n\t- snr value:104.802\n\tENN Framework Execute Model Sucess\n\t```\nFor more information on this sample program, refer to ENN SDK Samples.\nFor more information on writing native programs using ENN framework, refer to Getting Started With Native Samples.",
                "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
                "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Required Parameters\nContent:\nTo execute NNC models using the ENN framework, the following parameters are required:\n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector<string>|List of input file paths|\n|`goldens` (optional)|vector<string>|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Inception v4\nContent: Image Classification In Android:\n\nThis sample application demonstrates the execution of a converted Inception v4 model using the ENN framework. The model is converted using ENN SDK service with the **Accelerate** hardware type option.\n\n## Functionality\nThe sample application accepts input from a camera feed or an image file and classifies the object within the input. The classified items, their corresponding scores, and the inference time are displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Section: Quick Start Guide\\Quick Start Guide\\Example Using ENN SDK\\Executing Models Using Native Program-1\nContent:\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the Github repository.\n1. Download the samples and navigate to the directory that contain the sample program.\n\t```shell\n\tgit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n\tcd enn-sdk-samples-9925/nnc-model-tester\n\t```\n2. Push the necessary files (native program and test files) using adb push.\n\t```shell\n\tadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\n\tadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\n\tadb push example/model.nnc /data/local/tmp/\n\tadb push example/input.bin /data/local/tmp/\n\tadb push example/golden.bin /data/local/tmp/\n\t```",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: Pose Estimation In Android:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n\n## Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n\n## Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n\n## Getting Started\nTo utilize the sample application:\n1.\\tDownload or clone the sample application from this repository.\n2.\\tOpen the sample application project in Android Studio.\n3.\\tConnect the ERD board to the computer.\n4.\\tRun the application (using Shift + F10).\n5.\\tSelect Camera or Image mode and provide the data for inference.\n\n## Changing the Model in an Android Sample Application\nTo update the model in your Android project, do the following:",
                "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability."
            ]
        ],
        "answer": "In the 1st AI Challenger, the ENN SDK primarily supports TensorFlow Lite models. These models are required to be converted with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher, and they should be up to four dimensions in size and not exceed 1GB in total size. For Neural Processing Unit (NPU) utilization, model quantization is mandatory. Additionally, model distillation is highly recommended for optimal performance. It's important to ensure that the models meet these criteria for compatibility with the ENN SDK.",
        "ground_truth": "In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK."
    },
    {
        "question": "Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?",
        "contexts": [
            [
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-4\nContent:\n| 43 | MAXPOOL | MAX_POOL_2D | O | O | O | O |\n| 44 | MEAN | MEAN | | O | O | O |\n| 45 | MINIMUM | MINIMUM | | O | O | O |\n| 46 | MIRROR_PAD | MIRROR_PAD | O | O | O | O |\n| 47 | MUL | MUL | | | O | O |\n| 48 | NEG | NEG | O | O | O | O |\n| 49 | PACK | PACK | | | O | O |\n| 50 | PAD | PAD | O | O | O | O |\n| 51 | PADV2 | PADV2 | O | | O | O |\n| 52 | PERMUTE | TRANSPOSE | O | | O | O |\n| 53 | POW | POW | | | O | O |\n| 54 | PRELU | PRELU | O | O | O | O |\n| 55 | QUANTIZE | QUANTIZE | O | O | O | O |\n| 56 | REDUCE_MAX | REDUCE_MAX | O | O | O | O |\n| 57 | REDUCE_MEAN | REDUCE_MEAN | O | O | O | O |\n| 58 | REDUCE_MIN | REDUCE_MIN | O | O | O | O |\n| 59 | REDUCE_SUM | SUM | O | | O | O |",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-1\nContent:\n| Operator_Name | Kernel Size | Padding | Strides | Dilation | Boost Mode |\n| ------------------------------ | ----------- | -------- | -------- | --------- | ---------- |\n| CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | {1 ~ 27} |\n| DECONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DECONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Golden Data and Comparing it with Output (Optional)\nContent:\nAfter executing the model, load the golden binary data into memory and compare it with the data in the output memory location of model.\nThis optional step validates whether the output of model matches the expected results.\nUse the EnnAllocateAllBuffer function to obtain the output memory location.\n(example):\n```cpp\ntemplate <typename T>\nint compare_data(T *data1, T *data2, int size, T threshold) {\n\tint diff = 0;\n\tfor (int idx = 0; idx < size; idx++) {\n\t\tif (std::abs(data1[idx] - data2[idx]) > threshold) {\n\t\t\tdiff++;\n\t\t}\n\t}\n\treturn diff;\n}\n```",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-3\nContent:\n| 27 | EXP | EXP | | O | O | O |\n| 28 | UNSQUEEZE | EXPAND_DIMS | | | | O |\n| 29 | FLATTEN | - | O | O | O | O |\n| 30 | FLOOR | FLOOR | | O | O | O |\n| 31 | FLOOR_DIV | FLOOR_DIV | | O | | O |\n| 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O |\n| 33 | GATHER | GATHER | | | O | O |\n| 34 | GLOBAL_AVGPOOL | - | O | | O | O |\n| 35 | GLOBAL_MAXPOOL | - | O | O | O | O |\n| 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O |\n| 37 | HARD_SWISH | HARD_SWISH | O | O | O | O |\n| 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O |\n| 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O |\n| 40 | LOG | LOG | | | O | O |\n| 41 | LOGISTIC | LOGISTIC | O | O | O | O |\n| 42 | MAXIMUM | MAXIMUM | | O | O | O |",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-2\nContent:\n| DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| AVGPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\n| MAXPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\nFor `AVGPOOL` and `MAXPOOL`, when the kernel size is larger than the supported size, it is split into multiple pooling layers as the origin.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Channel Alignment\nMaintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources.\n| Architecture | Channel Alignment |\n| -- | -- |\n| Gen-4 | 32 |\n#### Bilinear Resize Parameters\nTo ensure optimal image resizing using bilinear interpolation, configure the following settings:\n- **Option A**:\n\t- Aligned corner: `False`\n\t- Half pixel centers: `False`\n\t- Performance: High speed\n- **Option B**:\n\t- Aligned corner: `True`\n\t- Half pixel centers: `False`\n\t- Compatibility: Gen-4 and later NPUs\n\t- Performance: Medium speed\n- **Option C**:\n\t- Aligned corner: `False`\n\t- Half pixel centers: `True`\n\t- Note: Requires workaround\n\t- Performance: Reduced speed",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-7\nContent:\n| 92 | UNPACK | UNPACK | | | O | O |\n| 93 | SHAPE | SHAPE | | | | O |\n| 94 | CLIP | - | O | O | | |\n| 95 | CAST | CAST | O | | O | O |\n| 96 | BATCH_TO_SPACE_ND | BATCH_TO_SPACE_ND | | | | O |\n| 97 | EQUAL | EQUAL | | | O | O |\n| 98 | NEQUAL | NOT_EQUAL | | | O | |\n| 99 | LESS | LESS | | | O | O |\n| 100 | GREATER | GREATER | | | O | O |\n| 101 | GREATER_EQUAL | GREATER_EQUAL | | | O | O |\n| 102 | LESS_EQUAL | LESS_EQUAL | | | O | O |\n| 103 | ADD_N | ADD_N | | | O | O |\n| 104 | TOPK_V2 | TOPK_V2 | | | | O |\n| 105 | LOG_SOFTMAX | LOG_SOFTMAX | | | O | O |\n| 106 | FLOOR_MOD | FLOOR_MOD | | | O | O |\n| 107 | SEGMENT_SUM | SEGMENT_SUM | | | O | O |\n| 108 | GELU | GELU | | | O | O |",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Required Parameters\nContent:\nTo execute NNC models using the ENN framework, the following parameters are required:\n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector<string>|List of input file paths|\n|`goldens` (optional)|vector<string>|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-6\nContent:\n| 75 | SELECT | SELECT | | | | O |\n| 76 | SHIFT_CLAMP | - | O | O | | |\n| 77 | SIN | SIN | | | O | O |\n| 78 | SLICE | SLICE | O | O | O | O |\n| 79 | SOFTMAX | SOFTMAX | | O | O | O |\n| 80 | SPACE_TO_BATCH_ND | SPACE_TO_BATCH_ND | | | | O |\n| 81 | SPACE_TO_DEPTH | SPACE_TO_DEPTH | O | O | O | O |\n| 82 | SPLIT | SPLIT | O | | O | O |\n| 83 | SPLIT_V | SPLIT_V | O | | | O |\n| 84 | SQRT | SQRT | | O | O | O |\n| 85 | SQUARED_DIFFERENCE | SQUARED_DIFFERENCE | | | O | O |\n| 86 | SQUEEZE | SQUEEZE | | | O | O |\n| 87 | STRIDED_SLICE | STRIDED_SLICE | O | O | O | O |\n| 88 | SUB | SUB | | | O | O |\n| 89 | TANH | TANH | O | O | O | O |\n| 90 | TILE | TILE | O | | O | O |\n| 91 | TRANSPOSE | TRANSPOSE | O | O | O | O |",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Writing ENN Framework Code\nContent:\n#### Executing Model\nThe execution phase of model involves copying the input layer, executing the model, and copying the output layer.\nFor more information on copying data to the input layer and comparing data with the output layer, refer to Section Processing Input and Outputs section.\n1. EnnExecuteModel:\n\tThis function executes the model using the specified model ID.\n\t(example):\n\t```cpp\n\tenn::api::EnnExecuteModel(model_id)\n\t```\n#### Deinitializing Framework\nAfter executing the model, deinitialize the framework to release resources.\n1. EnnReleaseBuffers:\n\tThis function releases the allocated buffers.\n\t(example):\n\t```cpp\n\tenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n\t```\n2. EnnCloseModel:\n\tThis function closes the specified model.\n\t(example):\n\t```cpp\n\tenn::api::EnnCloseModel(model_id)\n\t```\n3. EnnDeinitialize:\n\tThis function deinitializes the ENN framework.\n\t(example):\n\t```cpp\n\tenn::api::EnnDeinitialize()\n\t```",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n object ModelConstants {\n const val MODEL_NAME = \\\"float32_pose.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val HEATMAP_DATA_TYPE = DataType.FLOAT32\n\n const val HEATMAP_SIZE_W = 9\n const val HEATMAP_SIZE_H = 9\n const val HEATMAP_SIZE_C = 17\n\n val OFFSET_DATA_TYPE = DataType.FLOAT32\n\n const val OFFSET_SIZE_W = 9\n const val OFFSET_SIZE_H = 9\n const val OFFSET_SIZE_C = 34\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n```kotlin\npackage com.samsung.depthestimation.data\n\nobject ModelConstants {\n const val MODEL_NAME = \\\"MiDaS_v2.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 256\n const val INPUT_SIZE_H = 256\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 256\n const val OUTPUT_SIZE_H = 256\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n}\n```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image."
            ]
        ],
        "answer": "In the 1st AI Challenger, certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
        "ground_truth": "In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance."
    },
    {
        "question": "Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?",
        "contexts": [
            [
                "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
                "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
                "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
                "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
                "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
                "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
                "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
                "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
                "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
                "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
                "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
                "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
                "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
                "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-1\nContent:\nENN SDK supports the following operators<br>| **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** |\n| ----- | ------------------------------ | ----------------------- | --- | --- | --- | --- |\n| 1 | ABS | ABS | | O | O | O |\n| 2 | ADD | ADD | | | O | O |\n| 3 | ARGMAX | ARG_MAX | | O | O | O |\n| 4 | ARGMIN | ARG_MIN | | O | O | O |\n| 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O |\n| 6 | BATCH_NORMALIZATION | - | O | | | |\n| 7 | CEIL | CEIL | O | O | O | O |\n| 8 | CONCATENATION | CONCATENATION | O | O | O | O |\n| 9 | CONSTANT | - | O | | | |\n| 10 | CONVOLUTION | CONV_2D | O | O | O | O |\n| 11 | COS | COS | | | O | O |\n| 12 | CROP | STRIDED_SLICE | | O | O | O |",
                "Section: Support Matrix\\Support Matrix\\Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
                "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
                "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
                "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
                "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
                "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
                "Section: Support Matrix\\Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
                "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
                "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
                "URL: solution/AI/overview/33c6328a-d7ff-4a32-bea4-2ba4e43ec39c/detail\nCategory: Solution - AI Overview\nTitle: A New Lives with Exynos Edge Devices\nContent: Samsung System LSI's premium mobile application processor (AP), Exynos, is an incredible powerhouse for on-device AI, incorporating a neural processing unit (NPU) which can perform a variety of computations in real time without delay.\nAs it provides customized and personalized services by analyzing users' various behaviors, such as lifestyle patterns, speech habits, expressions, reactions and health conditions. In other words, by analyzing lifestyle patterns, we provide tailored traffic information and recommended locations. By examining speech habits, we enhance voice recognition accuracy and offer individualized voice services. Through facial expression analysis, we can deliver services like music and movie suggestions based on emotions. With reaction analysis, we can improve the service and provide services according to preferences. Lastly, through health status analysis, we can furnish essential information for health management.\nWe look forward to the many positive changes and new opportunities that user-centric AI innovation will bring to our daily lives.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
                "URL: solution/AI/overview/710bb616-1201-41f6-9621-dcbed37259c5/detail\nCategory: Solution - AI Overview\nTitle: AI Technology with Exynos in Your Hand\nContent: Artificial intelligence (AI) technology has traditionally involved transmitting information collected from smart devices, such as cell phones, to a central cloud server for analysis and then returning it back to the device.\nOn-device AI, on the other hand, gathers and analyzes the data for itself without the help of a distant cloud server.\nBy getting rid of the need for centralized server, on-device AI has many advantages including lower latency, enhanced security & privacy, increased reliability and reduced cost since the data is processed in the palm of your hand",
                "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
                "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-4\nContent:\n| 43 | MAXPOOL | MAX_POOL_2D | O | O | O | O |\n| 44 | MEAN | MEAN | | O | O | O |\n| 45 | MINIMUM | MINIMUM | | O | O | O |\n| 46 | MIRROR_PAD | MIRROR_PAD | O | O | O | O |\n| 47 | MUL | MUL | | | O | O |\n| 48 | NEG | NEG | O | O | O | O |\n| 49 | PACK | PACK | | | O | O |\n| 50 | PAD | PAD | O | O | O | O |\n| 51 | PADV2 | PADV2 | O | | O | O |\n| 52 | PERMUTE | TRANSPOSE | O | | O | O |\n| 53 | POW | POW | | | O | O |\n| 54 | PRELU | PRELU | O | O | O | O |\n| 55 | QUANTIZE | QUANTIZE | O | O | O | O |\n| 56 | REDUCE_MAX | REDUCE_MAX | O | O | O | O |\n| 57 | REDUCE_MEAN | REDUCE_MEAN | O | O | O | O |\n| 58 | REDUCE_MIN | REDUCE_MIN | O | O | O | O |\n| 59 | REDUCE_SUM | SUM | O | | O | O |",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-3\nContent:\n| 27 | EXP | EXP | | O | O | O |\n| 28 | UNSQUEEZE | EXPAND_DIMS | | | | O |\n| 29 | FLATTEN | - | O | O | O | O |\n| 30 | FLOOR | FLOOR | | O | O | O |\n| 31 | FLOOR_DIV | FLOOR_DIV | | O | | O |\n| 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O |\n| 33 | GATHER | GATHER | | | O | O |\n| 34 | GLOBAL_AVGPOOL | - | O | | O | O |\n| 35 | GLOBAL_MAXPOOL | - | O | O | O | O |\n| 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O |\n| 37 | HARD_SWISH | HARD_SWISH | O | O | O | O |\n| 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O |\n| 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O |\n| 40 | LOG | LOG | | | O | O |\n| 41 | LOGISTIC | LOGISTIC | O | O | O | O |\n| 42 | MAXIMUM | MAXIMUM | | O | O | O |",
                "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
                "Section: ENN SDK Service\\ENN SDK\\ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
                "URL: solution/AI/lab/7cdb56ba-9e0c-441d-9ae5-5201152026bd/detail\nCategory: Lab - Support Models\nTitle: PoseNet\nContent: 1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n ```kotlin\n object ModelConstants {\n const val MODEL_NAME = \\\"float32_pose.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 257\n const val INPUT_SIZE_H = 257\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val HEATMAP_DATA_TYPE = DataType.FLOAT32\n\n const val HEATMAP_SIZE_W = 9\n const val HEATMAP_SIZE_H = 9\n const val HEATMAP_SIZE_C = 17\n\n val OFFSET_DATA_TYPE = DataType.FLOAT32\n\n const val OFFSET_SIZE_W = 9\n const val OFFSET_SIZE_H = 9\n const val OFFSET_SIZE_C = 34\n }\n ```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: Writing Native Program\\Writing Native Program\\Processing Input and Outputs\\Loading Golden Data and Comparing it with Output (Optional)\nContent:\nAfter executing the model, load the golden binary data into memory and compare it with the data in the output memory location of model.\nThis optional step validates whether the output of model matches the expected results.\nUse the EnnAllocateAllBuffer function to obtain the output memory location.\n(example):\n```cpp\ntemplate <typename T>\nint compare_data(T *data1, T *data2, int size, T threshold) {\n\tint diff = 0;\n\tfor (int idx = 0; idx < size; idx++) {\n\t\tif (std::abs(data1[idx] - data2[idx]) > threshold) {\n\t\t\tdiff++;\n\t\t}\n\t}\n\treturn diff;\n}\n```",
                "URL: solution/AI/lab/b62e77ff-2ef3-4bce-b32f-a688b905e3d6/detail\nCategory: Lab - Support Models\nTitle: MiDaS v2\nContent: To update the model in your Android project, do the following:\n1.\\tPlace the new model file in the assets directory of your Android Studio project. The assets directory is located at `${APP_ROOT}/app/src/main`.\n2.\\tIn the `ModelConstants.kt` file, update the specifications to match the new model.\n```kotlin\npackage com.samsung.depthestimation.data\n\nobject ModelConstants {\n const val MODEL_NAME = \\\"MiDaS_v2.nnc\\\"\n\n val INPUT_DATA_TYPE = DataType.FLOAT32\n val INPUT_DATA_LAYER = LayerType.HWC\n\n const val INPUT_SIZE_W = 256\n const val INPUT_SIZE_H = 256\n const val INPUT_SIZE_C = 3\n\n const val INPUT_CONVERSION_SCALE = 127.5F\n const val INPUT_CONVERSION_OFFSET = 127.5F\n\n val OUTPUT_DATA_TYPE = DataType.FLOAT32\n\n const val OUTPUT_SIZE_W = 256\n const val OUTPUT_SIZE_H = 256\n\n const val OUTPUT_CONVERSION_SCALE = 1F\n const val OUTPUT_CONVERSION_OFFSET = 0F\n}\n```\n3. If the new model requires different input and output formats, modify the `preProcess()` and `postProcess()` functions in the code. The `preProcess()` and `postProcess()` functions are located in the `executor/ModelExecutor.kt` file.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Data Processing Procedures\n- **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation.\n\tFor efficient execution, it is recommended to use parallel processing on GPU or CPU.\n- **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment.\n#### Layer Adjustments\nTo enhance performance, it is recommended to exclude the dropout layer.\n#### PReLU\nUse the `PReLU` activation function for optimal performance.\nAlthough `LeakyReLU` is functional, it may not provide the same level of efficiency.\n#### Sharing IFM and OFM Recursively\nMerge successive concatenate layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM).",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-7\nContent:\n| 92 | UNPACK | UNPACK | | | O | O |\n| 93 | SHAPE | SHAPE | | | | O |\n| 94 | CLIP | - | O | O | | |\n| 95 | CAST | CAST | O | | O | O |\n| 96 | BATCH_TO_SPACE_ND | BATCH_TO_SPACE_ND | | | | O |\n| 97 | EQUAL | EQUAL | | | O | O |\n| 98 | NEQUAL | NOT_EQUAL | | | O | |\n| 99 | LESS | LESS | | | O | O |\n| 100 | GREATER | GREATER | | | O | O |\n| 101 | GREATER_EQUAL | GREATER_EQUAL | | | O | O |\n| 102 | LESS_EQUAL | LESS_EQUAL | | | O | O |\n| 103 | ADD_N | ADD_N | | | O | O |\n| 104 | TOPK_V2 | TOPK_V2 | | | | O |\n| 105 | LOG_SOFTMAX | LOG_SOFTMAX | | | O | O |\n| 106 | FLOOR_MOD | FLOOR_MOD | | | O | O |\n| 107 | SEGMENT_SUM | SEGMENT_SUM | | | O | O |\n| 108 | GELU | GELU | | | O | O |",
                "Section: Support Matrix\\Support Matrix\\Supported Operators-6\nContent:\n| 75 | SELECT | SELECT | | | | O |\n| 76 | SHIFT_CLAMP | - | O | O | | |\n| 77 | SIN | SIN | | | O | O |\n| 78 | SLICE | SLICE | O | O | O | O |\n| 79 | SOFTMAX | SOFTMAX | | O | O | O |\n| 80 | SPACE_TO_BATCH_ND | SPACE_TO_BATCH_ND | | | | O |\n| 81 | SPACE_TO_DEPTH | SPACE_TO_DEPTH | O | O | O | O |\n| 82 | SPLIT | SPLIT | O | | O | O |\n| 83 | SPLIT_V | SPLIT_V | O | | | O |\n| 84 | SQRT | SQRT | | O | O | O |\n| 85 | SQUARED_DIFFERENCE | SQUARED_DIFFERENCE | | | O | O |\n| 86 | SQUEEZE | SQUEEZE | | | O | O |\n| 87 | STRIDED_SLICE | STRIDED_SLICE | O | O | O | O |\n| 88 | SUB | SUB | | | O | O |\n| 89 | TANH | TANH | O | O | O | O |\n| 90 | TILE | TILE | O | | O | O |\n| 91 | TRANSPOSE | TRANSPOSE | O | O | O | O |",
                "Section: Support Matrix\\Support Matrix\\Hardware Features-2\nContent:\n| DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| DEPTHWISE_DILATION_CONVOLUTION | {1 ~ 4095} | {1 ~ 7} | {1 ~ 7} | {1 ~ 63} | - |\n| AVGPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\n| MAXPOOL | {1 ~ 31} | {1 ~ 15} | {1 ~ 15} | {1 ~ 127} | - |\nFor `AVGPOOL` and `MAXPOOL`, when the kernel size is larger than the supported size, it is split into multiple pooling layers as the origin.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
                "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
                "Section: ENN SDK Developer Guide\\ENN SDK Developer Guide\\5. Advanced Topics\\Model Design Tips\nContent:\n#### Channel Alignment\nMaintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources.\n| Architecture | Channel Alignment |\n| -- | -- |\n| Gen-4 | 32 |\n#### Bilinear Resize Parameters\nTo ensure optimal image resizing using bilinear interpolation, configure the following settings:\n- **Option A**:\n\t- Aligned corner: `False`\n\t- Half pixel centers: `False`\n\t- Performance: High speed\n- **Option B**:\n\t- Aligned corner: `True`\n\t- Half pixel centers: `False`\n\t- Compatibility: Gen-4 and later NPUs\n\t- Performance: Medium speed\n- **Option C**:\n\t- Aligned corner: `False`\n\t- Half pixel centers: `True`\n\t- Note: Requires workaround\n\t- Performance: Reduced speed",
                "Section: Writing Native Program\\Writing Native Program\\Executing Model\\Required Parameters\nContent:\nTo execute NNC models using the ENN framework, the following parameters are required:\n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector<string>|List of input file paths|\n|`goldens` (optional)|vector<string>|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
                "URL: solution/AI/lab/7aa740ec-5449-4247-9f24-babeb4f6d240/detail\nCategory: Lab - Support Models\nTitle: Zero-DCE\nContent: Image Enhance In Android:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.",
                "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
                "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION."
            ]
        ],
        "answer": "The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
        "ground_truth": "The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon."
    }
]