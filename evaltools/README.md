# Evaltools
Python package and Docker container for running RAGAS on AWS Lambda

## Usage
### Deploying
1. Build Docker image at `perf_metric/evaltools/`
    ```bash
    docker build -t {tag-name} .
    ```
1. Tag and push to AWS ECR
    ```bash
    docker tag {tag-name}:latest {ecr-uri}:latest
    docker push {ecr-uri}:latest
    ```
1. Deploy the image to AWS Lambda
1. Set environment variables
    - `CHAT_FUNCTION_NAME`: lambda fuction name for response generation
    - `OPENAI_API_KEY`: OpenAI API key

> For more details refer to [AWS Documents](https://docs.aws.amazon.com/lambda/latest/dg/python-image.html#python-image-instructions)

## Components
### Python Package (evaltools)
- `utils`: Main script
- `lambda_manager`: Script for invoking chat Lambda function
- `testset_generator`: Script for generating testset with RAGAS
- `chatbot_evaluator`: Script for evaluating chatbot's responce with RAGAS

### Lambda Function
#### Event Structure
```json
{
    "action": "{action-name}",
    "body": ["body-data"]
}
```
#### Actions
| Action Name | Details | Event body | Return body |
| ------ | ------- | ---------- | ----------- |
| `create_testset` | Creates test sets using RAGAS based on provided contexts | Required:<br>`List[Dictionary]`: list of contexts<br>`Content`: `String` - Content of context<br>`Section`: `String` - Section name for context<br><br>Optional:<br>`n`: `Int` - Number of testset to create | `List[Dictionary]`: list of test sets<br>`question`: `String` - Question for evaluation<br>`contexts`: `List[String]` - List of used context for creating question<br>`ground_truth`: `String` - Answer created by RAGAS<br>`evolution_type`: `String` - Type of question<br>`metadata`: `List[Dictionary]` - List of metadata from context |
| `create_evalset` | Creates evaluation set by adding chatbot response based on provided test sets | `List[Dictionary]`: list of test sets created at `create_testset` | `List[Dictionary]`: list of test sets<br>`question`: `String` - Question for evaluation<br>`contexts`: `List[String]` - List of used context when generating answer<br>`ground_truth`: `String` - Answer created by RAGAS<br>`evolution_type`: `String` - Type of question<br>`metadata`: `List[Dictionary]` - List of metadata from context<br>`answer`: `String` - Answer generated by chatbot |
| `evaluate` | Evaluate evaluation set using RAGAS | `List[Dictionary]`: list of evaluation sets created at `create_evalset` | `List[Dictionary]`: Result for evaluation with `score` added<br>`score`: `Dictionary` - Dictionary of evaluation metric and its score |


